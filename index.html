<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="我菜故我在">
<meta property="og:type" content="website">
<meta property="og:title" content="小菜鸡">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="小菜鸡">
<meta property="og:description" content="我菜故我在">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小菜鸡">
<meta name="twitter:description" content="我菜故我在">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>小菜鸡</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-106410509-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小菜鸡</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/21/NLP算法工程师技能加点路线/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/21/NLP算法工程师技能加点路线/" itemprop="url">NLP算法工程师技能加点路线</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-21T11:12:09+08:00">
                2020-04-21
              </time>
            

            
              <i class="fa fa-thumb-tack"></i>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>下图为2018年LOL段位的统计数据：</p>
<p><img width="500" src="/images/lol.jpeg"></p>
<p>参考LOL的段位体系，先制定一个小目标，在NLP算法工程师中达到黄金段位。下图整理了自己准备进入游戏时带的技能包，数字1，2，3表示学技能的优先级，绿色饼状图表示掌握该技能的经验值。</p>
<p>技能掌握包含三个方面：</p>
<ol>
<li>理论知识。需要全面总结并包含自己的理解，以博客文章形式输出；</li>
<li>工程实践。需要动手实验并记录总结，以博客文章形式输出；</li>
<li>代码编写。需要整理源码或自己编码， 并push到Github上。</li>
</ol>
<p>同时要关注NLP行业的最新发展趋势，与时俱进，不断扩展技能包。这次的段位提升计划时间为一到两年，冲！</p>
<p><img src="/images/NLPer.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/27/LambdaMART从放弃到入门/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/27/LambdaMART从放弃到入门/" itemprop="url">LambdaMART从放弃到入门</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-27T11:53:17+08:00">
                2020-03-27
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p> <img src="/images/letfly.png" alt=""></p>
<p>这篇文章希望站着把三件事办了：</p>
<ol>
<li>理解Lambda；</li>
<li>理解MART；</li>
<li>理解LambdaMART。</li>
</ol>
<p><strong>【待进一步整理】</strong></p>
<h1 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h1><p>关于Lambda梯度要从RankNet说起，RankNet提出了一种概率损失函数来学习Ranking Function，并应用Ranking Function对文档进行排序。LambdaRank在RankNet的基础上引入评价指标Z （如NDCG、ERR等），其损失函数的梯度代表了文档下一次迭代优化的方向和强度，由于引入了IR评价指标，Lambda梯度更关注位置靠前的优质文档的排序位置的提升，有效的避免了下调位置靠前优质文档的位置这种情况的发生。LambdaRank相比RankNet的优势在于分解因式后训练速度变快，同时考虑了评价指标，直接对问题求解，效果更明显。 详细内容可以参考<a href="https://octopuscoder.github.io/2020/01/19/搜索排序算法/" target="_blank" rel="external">搜索排序算法</a>。</p>
<h1 id="MART"><a href="#MART" class="headerlink" title="MART"></a>MART</h1><p>MART(Multiple Additive Regression Tree)的另一个名字叫GBDT(Gradient Boosting Decision Tree)，理解GBDT要从BT开始。</p>
<h2 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h2><p>BT(Boosting Tree)提升树是以<strong>决策树</strong>为基本学习器的提升方法，它被认为是统计学习中性能最好的方法之一。对于分类问题，提升树的决策树是二叉决策树，对于回归问题，提升树中的决策是二叉回归树。</p>
<p>提升树模型可以表示为决策树为基学习器的加法模型：</p>
<span>$$\begin{gather*}
f(\overrightarrow{\mathbf{x}})=f_{M}(\overrightarrow{\mathbf{x}})=\sum_{m=1}^{M} h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)
\end{gather*}$$</span><!-- Has MathJax -->
<p>其中<span>$h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)$</span><!-- Has MathJax -->表示第m个决策树，<span>$\Theta_{m}$</span><!-- Has MathJax -->为第m个决策树的参数，M为决策树的数量。</p>
<p><strong>提升树</strong>采用<strong>前向分步算法</strong>:</p>
<ul>
<li>首先确定初始提升树<span>$f_{0}(\overrightarrow{\mathbf{x}})=0$</span><!-- Has MathJax -->。</li>
<li>第m步模型为：<span>$f_{m}(\overrightarrow{\mathbf{x}})=f_{m-1}(\overrightarrow{\mathbf{x}})+h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)$</span><!-- Has MathJax -->，其中<span>$h_{m}(\cdot)$</span><!-- Has MathJax -->为待求的第m棵决策树。</li>
<li>通过经验风险极小化确定第m棵决策树的参数<span>$\Theta_{m}$: $\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left(\bar{y}_{i}, f_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right)$</span><!-- Has MathJax -->。这里没有引入正则项，而XGBoost中引入了正则项。</li>
</ul>
<p>不同问题的提升树学习算法主要区别在于使用的损失函数不同，（设预测值为<span>$\tilde{y}$</span><!-- Has MathJax --> ，真实值为<span>$\hat{y}$</span><!-- Has MathJax -->):</p>
<ul>
<li>回归问题：通常使用平方误差损失函数<span>$L(\tilde{y}, \hat{y})=(\tilde{y}-\hat{y})^{2}$</span><!-- Has MathJax -->。</li>
<li>分类问题：通常使用指数损失函数：<span>$L(\tilde{y}, \hat{y})=e^{-\tilde{y} \hat{y}}$</span><!-- Has MathJax --></li>
</ul>
<p><strong>提升树</strong>的学习<strong>思想</strong>有点类似一打高尔夫球，先粗略的打一杆，然后在之前的基础上逐步靠近球洞，也就是说<strong>每一棵树学习的是之前所有树预测值和的残差</strong>，这个<strong>残差</strong>就是一个加预测值后得到真实值的累加量。</p>
<p>例如在回归问题中，提升树采用平方误差损失函数，此时：</p>
<span>$$\begin{gather*}
\begin{aligned}
L\left(\tilde{y}, f_{m}(\overrightarrow{\mathbf{x}})\right) &amp;=L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathbf{x}})+h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)\right)
=\left(\tilde{y}-f_{m-1}(\overrightarrow{\mathbf{x}})-h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)\right)^{2} &amp;=\left(r-h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)\right)^{2}
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax -->
<p>其中<span>$r=\tilde{y}-f_{m-1}(\overrightarrow{\mathbf{x}})$</span><!-- Has MathJax -->为当前模型拟合数据的残差。所以对回归问题的提升树算法，第m个决策树 <span>$h_{m}(\cdot)$</span><!-- Has MathJax -->只需要简单拟合当前模型的残差。</p>
<p>回归提升树算法如下：</p>
<p> <img src="/images/boostingTree.png" alt=""></p>
<h2 id="梯度提升树-GBT"><a href="#梯度提升树-GBT" class="headerlink" title="梯度提升树(GBT)"></a>梯度提升树(GBT)</h2><p>上面所讲的提升树中，当损失函数是<strong>平方损失函数</strong>和<strong>指数损失函数</strong>时，每一步优化都很简单。因为平方损失函数和指数损失函数的求导非常简单。当损失函数是一般函数时，往往每一步优化不是很容易。针对这个问题，<code>Freidman</code>提出了<strong>梯度提升树算法(GBT)</strong>。</p>
<blockquote>
<p> <strong>梯度提升树(GBT)</strong>的一个核心思想是<strong>利用损失函数的负梯度在当前模型的值作为残差的近似值</strong>，本质上是<strong>对损失函数进行一阶泰勒展开</strong>，从而拟合一个回归树。</p>
</blockquote>
<p><strong>如何理解用负梯度近似残差</strong></p>
<ol>
<li><p>在对目标函数进行优化时，负梯度往往是函数下降最快的方向，自然也是GBDT目标函数下降最快的方向，所以用梯度去拟合首先是没什么问题的（并不是拟合梯度，只是用梯度去拟合）；GBDT本来中的g代表gradient，本来就是用梯度拟合；</p>
</li>
<li><p>用残差去拟合，只是目标函数是均方误差的一种特殊情况，CART中采用均方误差，符合这种情况。</p>
</li>
<li><p>为什么不直接使用残差拟合？目标函数除了loss可能还有正则项，正则中有参数和变量，很多情况下只拟合残差loss变小但是正则变大，目标函数不一定就小，这时候就要用梯度了，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向；</p>
</li>
</ol>
<p><strong>泰勒展开公式：</strong><br><span>$$\begin{gather*}
TaylorExpansion:f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}
\end{gather*}$$</span><!-- Has MathJax --><br>将损失函数使用<strong>一阶泰勒展开公式</strong>(<span>$\Delta x$</span><!-- Has MathJax -->相当于这里的<span>$h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right))$</span><!-- Has MathJax -->：</p>
<span>$$\begin{gather*}
L\left(\tilde{y}, f_{m}(\overrightarrow{\mathrm{x}})\right)=L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathrm{x}})+h_{m}\left(\overrightarrow{\mathrm{x}} ; \Theta_{m}\right)\right)=L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathrm{x}})\right)+\frac{\partial L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathrm{x}})\right)}{\partial f_{m-1}(\overrightarrow{\mathrm{x}})} h_{m}\left(\overrightarrow{\mathrm{x}} ; \Theta_{m}\right)
\end{gather*}$$</span><!-- Has MathJax -->
<p>则有：</p>
<span>$$\begin{gather*}
\Delta L=L\left(\tilde{y}, f_{m}(\overrightarrow{\mathbf{x}})\right)-L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathbf{x}})\right)=\frac{\partial L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathbf{x}})\right)}{\partial f_{m-1}(\overrightarrow{\mathbf{x}})} h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)
\end{gather*}$$</span><!-- Has MathJax -->
<p>要使得损失函数降低，则根据梯度下降的思想让损失函数对<span>$h_{m}\left(\overrightarrow{\mathbf{x}} ; \mathbf{\Theta}_{m}\right)$</span><!-- Has MathJax -->进行求导，按照负梯度更新该值，则损失函数是下降的，即：<span>$h_{m}\left(\overrightarrow{\mathbf{x}} ; \Theta_{m}\right)=-\frac{\partial L\left(\tilde{y}, f_{m-1}(\overrightarrow{\mathbf{x}})\right)}{\partial f_{m-1}(\overrightarrow{\mathbf{x}})}$</span><!-- Has MathJax --></p>
<p>这里进一步解释一下，对于函数f：<br><span>$$\begin{gather*}
f\left(\theta_{k+1}\right) \approx f\left(\theta_{k}\right)+\frac{\partial f\left(\theta_{k}\right)}{\partial \theta_{k}}\left(\theta_{k+1}-\theta_{k}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>使用梯度下降算法时，<span>$\theta_{k+1}=\theta_{k+1}-\eta \frac{\partial f\left(\theta_{k}\right)}{\partial \theta_{k}}$</span><!-- Has MathJax -->。</p>
<p>而在GBDT中，我们对损失函数进行展开：<span>$L\left(y, f_{m}(x)\right) \approx L\left(y, f_{m-1}(x)\right)+\frac{\partial L\left(y, f_{m-1}(x)\right)}{\partial f_{m-1}(x)}\left(f_{m}(x)-f_{m-1}(x)\right)$</span><!-- Has MathJax --></p>
<p>即，<span>$L\left(y, f_{m}(x)\right) \approx L\left(y, f{m-1}(x)\right)+\frac{\partial L\left(y, f_{m-1}(x)\right)}{\partial f_{m-1}(x)} T_{m}(x)$</span><!-- Has MathJax --></p>
<p>在优化<span>$L(y, f(x))$</span><!-- Has MathJax -->时，应用梯度下降算法时，有：<span>$f_{m}(x)=f_{m-1}(x)-\eta \frac{\partial L\left(y, f_{m-1}(x)\right)}{\partial f_{m-1}(x)}$</span><!-- Has MathJax --></p>
<p>则有<span>$T_{m}(x)=-\eta \frac{\partial L\left(y, f{m-1}(x)\right)}{\partial f_{m-1}(x)}$</span><!-- Has MathJax --> ，即负梯度可以近似认为是残差，区别在于$\eta$。</p>
<ul>
<li>对于平方损失函数，它就是通常意义的残差。</li>
<li>对于一般损失函数，它就是残差的近似。</li>
</ul>
<p>这里我们相当于获得了样本的标签，接下来就是用这个标签来训练决策树。</p>
<p>另外，梯度提升树用于分类模型时，是<strong>梯度提升决策树<code>GBDT</code></strong>；用于回归模型时，是<strong>梯度提升回归树<code>GBRT</code>，</strong>二者的区别主要是损失函数不同。</p>
<p><strong>GBRT</strong>算法的伪代码如下：</p>
<p> <img src="/images/GBRT.png" alt=""></p>
<p>另外，Freidman从bagging策略受到启发，采用<strong>随机梯度提升</strong>来修改了原始的梯度提升算法，即每一轮迭代中，新的决策树拟合的是原始训练集的一个子集（而并不是原始训练集）的残差，这个子集是通过对原始训练集的无放回随机采样而来，类似于自助采样法。这种方法<strong>引入了随机性，有助于改善过拟合</strong>，另一个好处是<strong>未被采样的另一部分字集可以用来计算包外估计误差</strong>。</p>
<p>这时我们再看一下LambdaMART算法的流程：</p>
<p><img src="/images/LambdaMART2.png" alt=""></p>
<p>对比GBRT和LambdaMART算法流程可以发现两者非常相似，主要区别是LambdaMART将GBRT中要拟合的负梯度替换为Lambda梯度，而LambdaMART对排序的最核心的改进正是这个Lambda梯度，具体介绍可以参考<a href="[http://octopuscoder.github.io/2020/01/19/%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/](http://octopuscoder.github.io/2020/01/19/搜索排序算法/">搜索排序算法</a>)中关于RankNet和LambdaRank的介绍。</p>
<h2 id="XBGoost"><a href="#XBGoost" class="headerlink" title="XBGoost"></a>XBGoost</h2><p>这里既然提到了提升树和梯度提升树，那么顺便也介绍一下大名鼎鼎的XGBoost，这部分介绍完后我们再探LambdaMART的一些细节。</p>
<p><strong>Xgboost</strong>也使用与提升树相同的<strong>前向分步算法，</strong>其区别在于：Xgboost通过<strong>结构风险最小化</strong>来确定下一个决策树的参数$\Theta_{m}$：</p>
<span>$\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left(\tilde{y}_{i}, f_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right)+\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right)$</span><!-- Has MathJax -->
<p>其中：</p>
<ul>
<li><span>$\Omega\left(h_{m}\right)$</span><!-- Has MathJax -->为第m个决策树的正则化项，这是XGBoost和GBT的一个重要区别。</li>
<li><span>$\mathcal{L}=\sum_{i=1}^{N} L\left(\tilde{y}_{i}, f_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right)+\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right)$</span><!-- Has MathJax -->为目标函数。</li>
</ul>
<p>与提升树不同的是<strong>，Xgboost</strong>还使用了<strong>二阶泰勒展开</strong>，定义：</p>
<span>$\hat{y}_{i}^{&lt;m-1&gt;}=f_{m-1}\left(\overrightarrow{\mathbf{x}}_{i}\right), \quad g_{i}=\frac{\partial L\left(\tilde{y}_{i}, \hat{y}_{i}^{&lt;m-1&gt;}\right)}{\partial \hat{y}_{i}^{&lt;m-1&gt;}}, \quad h_{i}=\frac{\partial^{2} L\left(\tilde{y}_{i}, \hat{y}_{i}^{&lt;m-1&gt;}\right)}{\partial^{2} \hat{y}_{i}^{&lt;m-1&gt;}}$</span><!-- Has MathJax -->
<p>其中，<span>$g_{i}, h_{i}$</span><!-- Has MathJax -->分别是损失函数<span>$L\left(\tilde{y}_{i}, \hat{y}_{i}^{&lt;m-1&gt;}\right)$</span><!-- Has MathJax -->对<span>$\hat{y}_{i}^{&lt;m-1&gt;}$</span><!-- Has MathJax -->的一阶导数和二阶导数。再看<strong>泰勒展开式</strong>：</p>
<p>Taylor expansion <span>$f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$</span><!-- Has MathJax --></p>
<p>因此我们对损失函数二阶泰勒展开有（<span>$\Delta x$</span><!-- Has MathJax -->相当于这里的<span>$h_{m}(\overrightarrow{\mathbf{x}})$</span><!-- Has MathJax -->）</p>
<span>$$\begin{gather*}
\begin{aligned} \mathcal{L} &amp;=\sum_{i=1}^{N} L\left(\tilde{y}_{i}, f_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right)+\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right)=\sum_{i=1}^{N} L\left(\tilde{y}_{i}, \hat{y}_{i}^{&lt;m-1&gt;}+h_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right)+\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right) \\ &amp; \simeq \sum_{i=1}^{N}\left[L\left(\tilde{y}_{i}, \hat{y}_{i}^{&lt;m-1&gt;}\right)+g_{i} h_{m}\left(\overrightarrow{\mathbf{x}}_{i}\right)+\frac{1}{2} h_{i} h_{m}^{2}\left(\overrightarrow{\mathbf{x}}_{i}\right)\right]+\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right)+\text { constant } \end{aligned}
\end{gather*}$$</span><!-- Has MathJax -->
<p><strong>提升树(GBT)</strong>只使用了一阶泰勒展开。而XGBoost的正则化项由两部分组成：<span>$\Omega\left(h_{m}(\overrightarrow{\mathbf{x}})\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$</span><!-- Has MathJax --></p>
<p>该部分表示<strong>决策树的复杂度，</strong>其中T为叶节点的个数，$w_{j}$为每个叶节点的输出值，$\gamma, \lambda \geq 0$为系数，控制这两个部分的比重。</p>
<ul>
<li>叶结点越多，则决策树越复杂。</li>
<li>每个叶结点输出值的绝对值越大，则决策树越复杂。</li>
</ul>
<blockquote>
<p>该复杂度是一个经验公式。事实上还有很多其他的定义复杂度的方式，只是这个公式效果还不错。</p>
</blockquote>
<p><strong>Xgboost</strong>相比与<strong>GBDT</strong>：</p>
<ol>
<li><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了<strong>二阶泰勒展开，同时用到了一阶和二阶导数</strong>。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。例如，xgboost支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）</p>
</li>
<li><p>xgboost在<strong>代价函数里加入了正则项，用于控制模型的复杂度</strong>。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</p>
</li>
<li><p><strong>列抽样（column subsampling）</strong>。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</p>
</li>
<li><p><strong>并行化处理</strong>：在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。</p>
</li>
</ol>
<p><strong>相比于GBDT，Xgboost最重要的优点还是用到了二阶泰勒展开信息和加入正则项</strong> 。</p>
<h1 id="LambdaMART"><a href="#LambdaMART" class="headerlink" title="LambdaMART"></a>LambdaMART</h1><p>LambdaMART算法的流程：</p>
<p><img src="/images/LambdaMART2.png" alt=""></p>
<p>叶子结点输出值的计算采用牛顿迭代法，对于一个函数<span>$g(\gamma)$</span><!-- Has MathJax -->，为了找到$\gamma$使得函数$g$取得极小（大）值采用牛顿迭代法的迭代步骤为：</p>
<span>$$\begin{gather*}

\gamma_{n+1}=\gamma_{n}-\frac{g^{\prime}\left(\gamma_{n}\right)}{g^{\prime \prime}\left(\gamma_{n}\right)}

\end{gather*}$$</span><!-- Has MathJax -->
<p>相应地，LambdaMART第m棵树中第k个叶子结点的输出值计算公式为：</p>
<span>$$\begin{gather*}

\gamma_{k m}=\frac{\sum_{x_{i} \in R_{k m}} \frac{\partial C}{\partial s_{i}}}{\sum_{x_{i} \in R_{k m}} \frac{\partial^{2} C}{\partial s_{i}^{2}}}=\frac{-\sum_{x_{i} \in R_{k m}} \sum_{\{i, j\} \rightleftharpoons I}\left|\Delta Z_{i j}\right| \rho_{i j}}{\sum_{x_{i} \in R_{k m}} \sum_{\{i, j\} \rightleftharpoons I}\left|\Delta Z_{i j}\right| \sigma \rho_{i j}\left(1-\rho_{i j}\right)}

\end{gather*}$$</span><!-- Has MathJax -->
<p>具体原理可以参考<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf" target="_blank" rel="external">From RankNet to LambdaRank to LambdaMART: An Overview</a></p>
<p>下面我们基于Ranklib源码和一个具体的例子来进一步理解LambdaMART算法的过程。Ranklib是Learning to Rank领域的一个优秀的开源算法库，实现了MART,RankNet,RankBoost,LambdaMart,Random Forest等模型，代码为Java。我们这里基于微软发布的LambdaMART来进行介绍，LambdaMART.java中的LambdaMART.learn()是学习流程的管控函数，学习过程主要有下面四步构成：</p>
<ol>
<li><p>计算deltaNDCG以及lambda;</p>
</li>
<li><p>以lambda作为label训练一棵regression tree;</p>
</li>
<li><p>在tree的每个叶子节点通过预测的regression lambda值还原出gamma，即最终输出得分；</p>
</li>
<li><p>用3的模型预测所有训练集合上的得分（+learningRate*gamma）,然后用这个得分对每个query的结果排序，计算新的每个query的base ndcg，以此为基础回到第1步，组成森林。</p>
</li>
</ol>
<p>重复这个步骤，直到满足下列两个收敛条件之一：</p>
<ol>
<li><p>树的个数达到训练参数设置；</p>
</li>
<li><p>Random Forest在validation集合上没有变好。</p>
</li>
</ol>
<p>下面用一组实际的数据来说明整个计算过程，假设我们有10个query的训练数据，每个query下有10个doc，每个query-doc对有10个feature，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.002736</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.002736</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.025992</span> <span class="number">2</span>:<span class="number">0.125000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.027360</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.001368</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.001368</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.188782</span> <span class="number">2</span>:<span class="number">0.375000</span> <span class="number">3</span>:<span class="number">0.333333</span> <span class="number">4</span>:<span class="number">1.000000</span> <span class="number">5</span>:<span class="number">0.195622</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.077975</span> <span class="number">2</span>:<span class="number">0.500000</span> <span class="number">3</span>:<span class="number">0.666667</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.086183</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.075239</span> <span class="number">2</span>:<span class="number">0.125000</span> <span class="number">3</span>:<span class="number">0.333333</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.077975</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.079343</span> <span class="number">2</span>:<span class="number">0.250000</span> <span class="number">3</span>:<span class="number">0.666667</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.084815</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.147743</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.147743</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.058824</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.058824</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">0</span> qid:<span class="number">1830</span> <span class="number">1</span>:<span class="number">0.071135</span> <span class="number">2</span>:<span class="number">0.125000</span> <span class="number">3</span>:<span class="number">0.333333</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.073871</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1840</span> <span class="number">1</span>:<span class="number">0.007364</span> <span class="number">2</span>:<span class="number">0.200000</span> <span class="number">3</span>:<span class="number">1.000000</span> <span class="number">4</span>:<span class="number">0.500000</span> <span class="number">5</span>:<span class="number">0.013158</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">1</span> qid:<span class="number">1840</span> <span class="number">1</span>:<span class="number">0.097202</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.000000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.096491</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line"><span class="number">2</span> qid:<span class="number">1840</span> <span class="number">1</span>:<span class="number">0.169367</span> <span class="number">2</span>:<span class="number">0.000000</span> <span class="number">3</span>:<span class="number">0.500000</span> <span class="number">4</span>:<span class="number">0.000000</span> <span class="number">5</span>:<span class="number">0.169591</span> <span class="number">6</span>:<span class="number">0.000000</span> <span class="number">7</span>:<span class="number">0.000000</span> <span class="number">8</span>:<span class="number">0.000000</span> <span class="number">9</span>:<span class="number">0.000000</span> <span class="number">10</span>:<span class="number">0.000000</span></div><div class="line">......</div></pre></td></tr></table></figure>
<p>为了简便，省略了余下的数据。上面的数据格式是按照Ranklib readme中要求的格式组织，除了行号之外，第一列是query-doc对的实际label（人工标注数据），第二列是qid，后面10列都是feature。</p>
<p>这份数据每组qid中的doc初始顺序可以是随机的，也可以是从实际的系统中获得的顺序，总之这个是计算ndcg的初始状态。对于qid=1830，它的10个doc的初始顺序的label序列是：0, 0, 0, 1, 1, 0, 1, 1, 0, 0(实际中label可以扩展为1，2，3，4等，根据数据自行决定)。dcg的计算公式为：</p>
<span>$$\begin{gather*}
dcg(i)=\frac{2^{l a b e l(i)}-1}{\log _{2}(i+1)}
\end{gather*}$$</span><!-- Has MathJax -->
<p>i表示当前doc在这个qid下的位置（从1开始，避免分母为0），label(i)是doc(i)的标注值。而一个query的dcg则是其下所有doc的加和：</p>
<span>$$\begin{gather*}
dcg(query)=\sum_{i} \frac{2^{l a b e l(i)}-1}{\log _{2}(i+1)}
\end{gather*}$$</span><!-- Has MathJax -->
<p>根据上式可以计算初始状态下每个qid的dcg：</p>
<span>$$\begin{gather*}

\begin{aligned} d c g(q i d=1830) &amp;=\frac{2^{0}-1}{\log _{2}(1+1)}+\frac{2^{0}-1}{\log _{2}(2+1)}+\ldots+\frac{2^{0}-1}{\log _{2}(10+1)} \\ &amp;=0+0+0+0.431+0.387+0+0.333+0.315+0 \\ &amp;+0=1.466 \end{aligned}
\end{gather*}$$</span><!-- Has MathJax -->
<p>要计算ndcg，还需要计算理想集的dcg，将初始状态按照label排序，qid=1830得到的序列是1,1,1,1,0,0,0,0,0,0，计算dcg:</p>
<span>$$\begin{gather*}
ideal_dcg(qid =1830) =\frac{2^{1}-1}{\log _{2}(1+1)}+\frac{2^{1}-1}{\log _{2}(2+1)}+\ldots
\begin{aligned}+\frac{2^{0}-1}{\log _{2}(10+1)} &amp; \\ &amp;=1+0.631+0.5+0.431+0+0+0+0+0+0 \\ &amp;=2.562 \end{aligned}
\end{gather*}$$</span><!-- Has MathJax -->
<p>两者相除得到初始状态下qid=1830的ndcg:</p>
<span>$$\begin{gather*}

ndcg(qid=1830)=\frac{dcg(qid=1830)}{ideal_{-} ndcg(qid=1830)}=\frac{1.466}{2.562}=0.572

\end{gather*}$$</span><!-- Has MathJax -->
<p>下面要计算每一个doc的deltaNDCG，公式如下：</p>
<span>$$\begin{gather*}
\begin{array}{c}
\operatorname{deltaNDCG}(i, j) \\
=| \text {ndcg(original sequence) }-ndcg(\operatorname{swap}(i, j) \text { sequence }) |
\end{array}
\end{gather*}$$</span><!-- Has MathJax -->
<p>deltaNDCG(i,j)是将位置i和位置j的位置互换后产生的ndcg变化（其他位置均不变），显然有相同label的deltaNDCG(i,j)=0。在qid=1830的初始序列0, 0, 0, 1, 1, 0, 1, 1, 0, 0，由于前3的label都一样，所以deltaNDCG(1,2)=deltaNDCG(1,3)=0，不为0的是deltaNDCG(1,4), deltaNDCG(1,5), deltaNDCG(1,7), deltaNDCG(1,8)。将1，4位置互换，序列变为1, 0, 0, 0, 1, 0, 1, 1, 0, 0，计算得到dcg=2.036，整个deltaNDCG(1,4)的计算过程如下：</p>
<span>$$\begin{gather*}
\begin{array}{l}
\qquad \begin{array}{l}
dcg(q i d=1830, \operatorname{swap}(1,4))=\frac{2^{1}-1}{\log _{2}(1+1)}+\frac{2^{0}-1}{\log _{2}(2+1)}+\ldots \\
+\frac{2^{0}-1}{\log _{2}(10+1)}
\end{array} \\
=1+0+0+0+0.387+0+0.333+0.315+0+0 \\
=2.036
\end{array}
\end{gather*}$$</span><!-- Has MathJax -->
<span>$$\begin{gather*}
\begin{array}{l}
n d c g(swap(1,4))=\frac{d c g(swap(1,4))}{ideal_{-} dcg}=\frac{2.036}{2.562}=0.795 \\
\text {deltaNDCG}(1,4)=\operatorname{detal} N D C G(4,1) \\
=| \text {ndcg}(\text {original sequence})-\text {ndcg}(\operatorname{swap}(1,4)) | \\
=|0.572-0.795|=0.222
\end{array}
\end{gather*}$$</span><!-- Has MathJax -->
<p>同样过程可以计算出deltaNDCG(1,5)=0.239, deltaNDCG(1,7)=0.260, deltaNDCG(1,8)=0.267等。</p>
<p>进一步，要计算lambda(i)，根据paper，还需要ρ值，ρ可以理解为$doc_i$比$doc_j$差的概率，其计算公式为：</p>
<span>$\rho_{i j}=\frac{1}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}$</span><!-- Has MathJax -->
<p>参考<a href="[http://octopuscoder.github.io/2020/01/19/%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/](http://octopuscoder.github.io/2020/01/19/搜索排序算法/">搜索排序算法</a>)中关于LambdaRank的介绍：</p>
<span>$$\begin{gather*}

\lambda_{i j}=\frac{\partial C\left(s_{i}-s_{j}\right)}{\partial s_{i}}=\frac{-\sigma}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\left|\Delta_{N D C G}\right|

\end{gather*}$$</span><!-- Has MathJax -->
<p>Ranklib中直接取σ=1，如下图，蓝，红，绿三种颜色分别对应σ=1，2，4时ρ函数的曲线情形（横坐标是<span>$s_i-s_j$</span><!-- Has MathJax -->）:</p>
<p>初始时，模型为空，所有模型预测得分都是0，所以<span>$s_i=s_j=0，&rho;_{ij}=1/2$</span><!-- Has MathJax -->，lambda(i,j)的计算公式为：</p>
<span>$$\begin{gather*}

\lambda_{i j}=\rho_{i j} *|\operatorname{deltaNDCG}(i, j)|

\end{gather*}$$</span><!-- Has MathJax -->
<p>上式为Ranklib中实际使用的公式，而在paper中，还需要再乘以-σ，在σ=1时，就是符号正好相反，这两种方式是等价的，符号并不影响模型训练结果（其实大可以把代码中lambda的值前面加一个负号，只是注意在每轮计算train, valid和最后计算test的ndcg的时候，模型预测的得分modelScores要按升序排列——越负的doc越好，而不是源代码中按降序。最后训练出的模型是一样的，这说明这两种方式完全对称，所以符号的问题可以省略。甚至不乘以-σ，更符合人的习惯——分数越大越好，降序排列结果。）：</p>
<span>$$\begin{gather*}

\lambda_{i}=\sum_{j(l a b e l(i)&gt;l a b e l(j))} \lambda_{i j}-\sum_{j(label(i)&lt;label(j))} \lambda_{i j}

\end{gather*}$$</span><!-- Has MathJax -->
<p>计算<span>$\lambda_{1}$</span><!-- Has MathJax -->，由于label(1)=0，qid=1830中的其他doc的label都大于或者等于0，所以<span>$\lambda_{1}$</span><!-- Has MathJax -->的计算中所有的<span>$\lambda_{1,j}$</span><!-- Has MathJax -->都为负项。将之前计算的各<span>$deltaNDCG(1,j)$</span><!-- Has MathJax -->代入，且初始状态下<span>$&rho;_{ij}=1/2$</span><!-- Has MathJax -->，所以:</p>
<span>$$\begin{gather*}*

\begin{aligned}
\lambda_{1} &amp;=-0.5 *(\text {deltaNDCG}(1,3)+\text {deltaNDCG}(1,4)+\text {deltaNDCG}(1,6)+\text {deltaNDCG}(1,7)) \\
&amp;=-0.5 *(0.222+0.239+0.260+0.267)=-0.495
\end{aligned}

\end{gather*}$$</span><!-- Has MathJax -->
<p>可以计算出初始状态下qid=1830各个doc的lambda值，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.111</span>  <span class="number">-0.120</span>  <span class="number">0.000</span>   <span class="number">-0.130</span>  <span class="number">-0.134</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">1</span>): <span class="number">-0.495</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.039</span>  <span class="number">-0.048</span>  <span class="number">0.000</span>   <span class="number">-0.058</span>  <span class="number">-0.062</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">2</span>): <span class="number">-0.206</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.014</span>  <span class="number">-0.022</span>  <span class="number">0.000</span>   <span class="number">-0.033</span>  <span class="number">-0.036</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">3</span>): <span class="number">-0.104</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.111</span>   <span class="number">0.039</span>   <span class="number">0.014</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.015</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.025</span>   <span class="number">0.028</span>   <span class="keyword">lambda</span>(<span class="number">4</span>): <span class="number">0.231</span> </div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.120</span>   <span class="number">0.048</span>   <span class="number">0.022</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.006</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.017</span>   <span class="number">0.019</span>   <span class="keyword">lambda</span>(<span class="number">5</span>): <span class="number">0.231</span> </div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.015</span>  <span class="number">-0.006</span>  <span class="number">0.000</span>   <span class="number">-0.004</span>  <span class="number">-0.008</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">6</span>): <span class="number">-0.033</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.130</span>   <span class="number">0.058</span>   <span class="number">0.033</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.004</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.006</span>   <span class="number">0.009</span>   <span class="keyword">lambda</span>(<span class="number">7</span>): <span class="number">0.240</span> </div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.134</span>   <span class="number">0.062</span>   <span class="number">0.036</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.008</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.003</span>   <span class="number">0.005</span>   <span class="keyword">lambda</span>(<span class="number">8</span>): <span class="number">0.247</span> </div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.025</span>  <span class="number">-0.017</span>  <span class="number">0.000</span>   <span class="number">-0.006</span>  <span class="number">-0.003</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">9</span>): <span class="number">-0.051</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="number">-0.028</span>  <span class="number">-0.019</span>  <span class="number">0.000</span>   <span class="number">-0.009</span>  <span class="number">-0.005</span>  <span class="number">0.000</span>   <span class="number">0.000</span>   <span class="keyword">lambda</span>(<span class="number">10</span>): <span class="number">-0.061</span></div></pre></td></tr></table></figure>
<p>上表中每一列都是考虑了符号的<span>$\lambda_{i,j}$</span><!-- Has MathJax -->，即如果label(i)<label(j)，则为负值，反之为正值，每行结尾的<span>$\lambda_i$<!-- Has MathJax -->是前面的加和，即为最终的<span>$\lambda_i$</span><!-- Has MathJax -->。可以看到，<span>$\lambda_i$</span><!-- Has MathJax -->在系统中表达了<span>$doc_i$</span><!-- Has MathJax -->上升或者下降的强度，label越高，位置越后，<span>$\lambda_i$</span><!-- Has MathJax -->为正值，越大，表示趋向上升的方向，力度也越大；label越小，位置越靠前，<span>$\lambda_i$</span><!-- Has MathJax -->为负值，越小，表示趋向下降的方向，力度也大（<span>$\lambda_i$</span><!-- Has MathJax -->的绝对值表达了上升、下降的强度）。</label(j)，则为负值，反之为正值，每行结尾的<span></p>
<p>完成各doc的<span>$\lambda$</span><!-- Has MathJax -->值计算后，Regression Tree便开始以每个doc的<span>$\lambda$</span><!-- Has MathJax -->值为目标，训练模型。并在最后对叶子节点上的样本 <span>$\lambda$</span><!-- Has MathJax -->均值还原成 𝛾 ，乘以learningRate加到此前的Regression Trees上，更新score，重新对query下的doc按score排序，再次计算deltaNDCG以及 λ ，如此迭代下去直至树的数目达到参数设定或者在validation集上不再持续变好（一般实践来说不在模型训练时设置validation集合，因为validation集合一般比训练集合小很多，很容易收敛，达不到效果，不如训练时一步到位，然后另起test集合做结果评估）。</p>
<p>Regression Tree的训练最主要的就是决定如何分裂节点。lambdaMART采用最朴素的最小二乘法，也就是最小化平方误差和来分裂节点：即对于某个选定的feature，选定一个值val，所有&lt;=val的样本分到左子节点，&gt;val的分到右子节点。然后分别对左右两个节点计算平方误差和，并加在一起作为这次分裂的代价。遍历所有feature以及所有可能的分裂点val(每个feature按值排序，每个不同的值都是可能的分裂点)，在这些分裂中找到代价最小的。</p>
<p>继续上面的例子，假设样本只有上面计算出 λ 的10个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">qId=<span class="number">1830</span> features <span class="keyword">and</span> lambdas</div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.003</span> <span class="number">2</span>:<span class="number">0.000</span> <span class="number">3</span>:<span class="number">0.000</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.003</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">1</span>):<span class="number">-0.495</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.026</span> <span class="number">2</span>:<span class="number">0.125</span> <span class="number">3</span>:<span class="number">0.000</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.027</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">2</span>):<span class="number">-0.206</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.001</span> <span class="number">2</span>:<span class="number">0.000</span> <span class="number">3</span>:<span class="number">0.000</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.001</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">3</span>):<span class="number">-0.104</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.189</span> <span class="number">2</span>:<span class="number">0.375</span> <span class="number">3</span>:<span class="number">0.333</span> <span class="number">4</span>:<span class="number">1.000</span> <span class="number">5</span>:<span class="number">0.196</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">4</span>):<span class="number">0.231</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.078</span> <span class="number">2</span>:<span class="number">0.500</span> <span class="number">3</span>:<span class="number">0.667</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.086</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">5</span>):<span class="number">0.231</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.075</span> <span class="number">2</span>:<span class="number">0.125</span> <span class="number">3</span>:<span class="number">0.333</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.078</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">6</span>):<span class="number">-0.033</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.079</span> <span class="number">2</span>:<span class="number">0.250</span> <span class="number">3</span>:<span class="number">0.667</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.085</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">7</span>):<span class="number">0.240</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.148</span> <span class="number">2</span>:<span class="number">0.000</span> <span class="number">3</span>:<span class="number">0.000</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.148</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">8</span>):<span class="number">0.247</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.059</span> <span class="number">2</span>:<span class="number">0.000</span> <span class="number">3</span>:<span class="number">0.000</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.059</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">9</span>):<span class="number">-0.051</span></div><div class="line">qId=<span class="number">1830</span>    <span class="number">1</span>:<span class="number">0.071</span> <span class="number">2</span>:<span class="number">0.125</span> <span class="number">3</span>:<span class="number">0.333</span> <span class="number">4</span>:<span class="number">0.000</span> <span class="number">5</span>:<span class="number">0.074</span> <span class="number">6</span>:<span class="number">0.000</span> <span class="number">7</span>:<span class="number">0.000</span> <span class="number">8</span>:<span class="number">0.000</span> <span class="number">9</span>:<span class="number">0.000</span> <span class="number">10</span>:<span class="number">0.000</span>    <span class="keyword">lambda</span>(<span class="number">10</span>):<span class="number">-0.061</span></div></pre></td></tr></table></figure>
<p>上表中除了第一列是qid，最后一列是lambda外，其余都是feature，比如我们选择feature(1)的0.059做分裂点，则左子节点&lt;=0.059的doc有: 1, 2, 3, 9；而&gt;0.059的被安排到右子节点，doc有4, 5, 6, 7, 8, 10。由此左右两个子节点的lambda均值分别为：</p>
<span>$$\begin{gather*}

\bar{\lambda}_{L}=\frac{\lambda_{1}+\lambda_{2}+\lambda_{3}+\lambda 9}{4}=\frac{-0.495-0.206-0.104-0.051}{4}=-0.214

\end{gather*}$$</span><!-- Has MathJax -->
<span>$$\begin{gather*}
\begin{array}{l}
\lambda_{R}^{-}=\frac{\lambda_{4}+\lambda_{5}+\lambda_{6}+\lambda 7+\lambda_{8}+\lambda_{10}}{6} \\
=\frac{0.231+0.231-0.033+0.240+0.247-0.061}{6}=0.143
\end{array}
\end{gather*}$$</span><!-- Has MathJax -->
<p>继续计算左右子节点的平方误差和：</p>
<span>$$\begin{gather*}
\begin{aligned}
&amp;s_{L}=\sum_{i \in L}\left(\lambda_{i}-\bar{\lambda}_{L}\right)^{2}=(-0.495+0.214)^{2}\\
&amp;+(-0.206+0.214)^{2}+(-0.104+0.214)^{2}\\
&amp;+(-0.051+0.214)^{2}=0.118\\
&amp;s_{R}=\sum_{i \in R}\left(\lambda_{i}-\bar{\lambda}_{R}\right)^{2}=(0.231-0.143)^{2}\\
&amp;+(0.231-0.143)^{2}+(-0.033-0.143)^{2}\\
&amp;+(0.240-0.143)^{2}+(0.247-0.143)^{2}\\
&amp;+(0.016-0.143)^{2}=0.083
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax -->
<p>因此将feature(1)的0.059作为分裂点时的均方差（分裂代价）是：</p>
<span>$$\begin{gather*}
\cos t_{0.059 @ \text { feature}(1)}=s_{L}+s_{R}=0.118+0.083=0.201
\end{gather*}$$</span><!-- Has MathJax -->
<p>我们可以像上面那样遍历所有feature的不同值，尝试分裂，计算Cost，最终选择所有可能分裂中最小Cost的那一个作为分裂点。然后将𝑠𝐿和𝑠𝑅 分别作为左右子节点的属性存储起来，并把分裂的样本也分别存储到左右子节点中，然后维护一个队列，始终按平方误差和s降序插入新分裂出的节点，每次从该队列头部拿出一个节点（并基于这个节点上的样本）进行分裂（即最大均方差优先分裂），直到树的分裂次数达到参数设定（训练时传入的leaf值，叶子节点的个数与分裂次数等价）。这样就完成了一棵Regression Tree的训练。</p>
<p>上面讲述了一棵树的标准分裂过程，此外，树的分裂还包含了其他参数设定，例如叶子节点上的最少样本数，比如我们设定为3，则在feature(1)处，0.001和0.003两个值都不能作为分裂点，因为用它们做分裂点，左子树的样本数分别是1和2，均&lt;3。叶子节点的最少样本数越小，模型则拟合得越好，当然也容易过拟合（over-fitting）；反之如果设置得越大，模型则可能欠拟合（under-fitting），实践中可以使用cross validation的办法来寻找最佳的参数设定。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://zhuanlan.zhihu.com/p/57814935" target="_blank" rel="external">GBT、GBDT、GBRT与Xgboost</a></p>
<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf" target="_blank" rel="external">From RankNet to LambdaRank to LambdaMART: An Overview</a></p>
<p><a href="https://www.cnblogs.com/wowarsenal/p/3900359.html" target="_blank" rel="external">LambdaMART简介——基于Ranklib源码</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/19/搜索排序算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/19/搜索排序算法/" itemprop="url">搜索排序算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-19T15:56:07+08:00">
                2020-01-19
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<p><strong><em>[待进一步整理]</em></strong></p>
<h1 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h1><p>典型的搜索排序算法框架如下图所示，分为线下训练和线上排序两个部分。模型包括相关性模型、时效性模型、个性化模型和点击模型等。特征包括Query特征、Doc特征、User特征和Query-Doc匹配特征等。日志包括展现日志、点击日志和Query日志。<br><img width="700" src="/images/search_structure.png"></p>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><h2 id="泛特征"><a href="#泛特征" class="headerlink" title="泛特征"></a>泛特征</h2><p>Query特征：意图分类、关键词、词权重等。<br>Doc特征：文章分类、长度、点赞数等。<br>User特征：年龄、性别等。<br>Query-Doc匹配特征：类别匹配、BM25。<br>点击特征：CTR、首次点击等。</p>
<h1 id="日志设计"><a href="#日志设计" class="headerlink" title="日志设计"></a>日志设计</h1><p>展现日志：理论上可根据经验进行人工标注打分，并且作为模型的启动训练数据。<br>点击日志：用户的点击行为日志，可以用于Query日志挖掘，进行查询扩展等，例如多个query搜索结果用户都点击了同一篇文档，则可认为这些query相似。<br>Query日志：用于和点击／转化数据做联合分析。</p>
<h1 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h1><p>相关性模型：Learning to Rank模型</p>
<ol>
<li>按照样本生成方法和Loss Function不同分为point wise、pair wise和list wise三种，参考<a href="https://wesure.lexiangla.com/teams/dm/docs/1665fb8a09d111ea81c20a58ac131706?company_from=wesure" target="_blank" rel="external">L2R算法介绍</a>。2019年Google提出的group wise模型 TF Ranking。</li>
<li>按照模型结构划分，可分为线性排序模型、树模型、深度学习模型，以及组合模型（GBDT+LR，Deep&amp;Wide）。</li>
</ol>
<p>时效性模型：[待补充]。<br>个性化模型：逻辑回归(Logistic Regression）。<br>点击模型：深度置信网络(DeepBeliefNetworks)。</p>
<h2 id="相关性模型"><a href="#相关性模型" class="headerlink" title="相关性模型"></a>相关性模型</h2><h3 id="模型分类"><a href="#模型分类" class="headerlink" title="模型分类"></a>模型分类</h3><p><img width="700" src="/images/search_kinds.png"></p>
<h3 id="迭代过程"><a href="#迭代过程" class="headerlink" title="迭代过程"></a>迭代过程</h3><p><img width="700" src="/images/search_process.png"></p>
<h3 id="主流模型及评测指标"><a href="#主流模型及评测指标" class="headerlink" title="主流模型及评测指标"></a>主流模型及评测指标</h3><p><img width="700" src="/images/search_evalute.png"></p>
<h3 id="主流模型对比"><a href="#主流模型对比" class="headerlink" title="主流模型对比"></a>主流模型对比</h3><p><img height="500" src="/images/serach_model_compare.png"></p>
<p>图中Rank Creation指给定查询Query和文档Docs，得到Docs排序结果；Rank Aggregation指综合多个不同的Ranking System的排序结果，得出最终排序结果。</p>
<h3 id="模型详情"><a href="#模型详情" class="headerlink" title="模型详情"></a>模型详情</h3><h4 id="RankSVM"><a href="#RankSVM" class="headerlink" title="RankSVM"></a>RankSVM</h4><p>RankSVM(2003)，将排序问题转化为pairwise的分类问题，然后使用SVM分类模型进行学习并求解，举例说明，两组query及其召回的documents，其中query-doc的相关程度等级分为三档，如下图所示：<br><img width="700" src="/images/search_ranksvm1.png"><br>同一个query下的不同相关度的doc的feature vector可以进行组合，形成新的feature vector：x1-x2，x1-x3，x2-x3。同时label也会被重新赋值，例如L1-L2，L1-L3，L2-L3。这几个feature vector的label被赋值成分类问题中的positive label。进一步，为了形成一个标准的分类问题，我们还需要有negative samples，这里我们就使用前述的几个新的positive feature vector的反方向向量作为相应的negative samples：x2-x1，x3-x1，x3-x2。另外，需要注意的是，我们在组合形成新的feature vector的时候，不能使用在原始排序问题中处于相同相似度等级的两个feature vector，也不能使用处于不同query下的两个feature vector。<br><img width="700" src="/images/search_ranksvm2.png"></p>
<h4 id="RankNet"><a href="#RankNet" class="headerlink" title="RankNet"></a>RankNet</h4><p>微软(2005)提出，属于pairwise算法，从概率的角度来解决排序问题。RankNet的核心是提出了一种概率损失函数来学习Ranking Function，并应用Ranking Function对文档进行排序。这里的Ranking Function可以是任意对参数可微的模型，也就是说，该概率损失函数并不依赖于特定的机器学习模型，在论文中，RankNet是基于神经网络实现的。除此之外，GBDT等模型也可以应用于该框架。<br>对于任意一个doc对(Ui,Uj)，模型输出的score分别为si和sj，那么根据模型的预测，Ui比Uj与Query更相关的概率为Pij。由于RankNet使用的模型一般为神经网络，根据经验sigmoid函数能提供一个比较好的概率评估。<br>那么根据模型的预测，Ui比Uj与Query更相关的概率为：<br><span>$$\begin{gather*}
P_{i j}=P\left(U_{i}&gt;U_{j}\right)=\frac{1}{1+e^{-\sigma\left(s_{i}-s_{j}\right)}}
\end{gather*}$$</span><!-- Has MathJax --><br><strong><em>RankNet证明了如果知道一个待排序文档的排列中相邻两个文档之间的排序概率，则通过推导可以算出每两个文档之间的排序概率。因此对于一个待排序文档序列，只需计算相邻文档之间的排序概率，不需要计算所有pair，减少计算量。</em></strong><br>对于训练数据中的Ui和Uj，它们都包含有一个与Query相关性的真实label，比如Ui与Query的相关性label为good，Uj与Query的相关性label为bad，那么显然Ui比Uj更相关。我们定义Ui比Uj更相关的真实概率为Sij：<br><span>$$\begin{gather*}
\bar{P}_{i j}=\frac{1}{2}\left(1+S_{i j}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>如果$U_i$比$U_j$更相关，那么Sij=1；如果Ui不如Uj相关，那么Sij=−1；如果$U_i$、$U_j$与Query的相关程度相同，那么Sij=0。通常，两个doc的相关度可由人工标注或者从搜索日志中得到。<br>对于一个排序，RankNet从各个doc的相对关系来评价排序结果的好坏，排序的效果越好，那么有错误相对关系的doc pair就越少。所谓错误的相对关系即如果根据模型输出Ui排在Uj前面，但真实label为Ui的相关性小于Uj，那么就记一个错误pair，RankNet本质上就是以错误的pair最少为优化目标。而在抽象成cost function时，RankNet实际上是引入了概率的思想：不是直接判断Ui排在Uj前面，而是说Ui以一定的概率P排在Uj前面，即是以预测概率与真实概率的差距最小作为优化目标。最后，RankNet使用Cross Entropy作为cost function，来衡量预测相关性概率对真实相关性概率的拟合程度：<br><span>$$\begin{gather*}
C=-\bar{P}_{i j} \log P_{i j}-\left(1-\bar{P}_{i j}\right) \log \left(1-P_{i j}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>该损失函数有以下两个特点：<br>进一步化简后有：<br><span>$$\begin{gather*}
\begin{aligned}
C_{i j} &amp;=-\frac{1}{2}\left(1+S_{i j}\right) \log \frac{1}{1+e^{-\sigma\left(s_{i}-s_{j}\right)}}-\frac{1}{2}\left(1-S_{i j}\right) \log \frac{e^{-\sigma\left(s_{i}-s_{j}\right)}}{1+e^{-\sigma\left(s_{i}-s_{j}\right)}} \\
&amp;=-\frac{1}{2}\left(1+S_{i j}\right) \log \frac{1}{1+e^{-\sigma\left(s_{i}-s_{j}\right)}}-\frac{1}{2}\left(1-S_{i j}\right)\left[-\sigma\left(s_{i}-s_{j}\right)+\log \frac{1}{1+e^{-\sigma\left(s_{i}-s_{j}\right)}}\right] \\
&amp;=\frac{1}{2}\left(1-S_{i j}\right) \sigma\left(s_{i}-s_{j}\right)+\log \left(1+e^{-\sigma\left(s_{i}-s_{j}\right)}\right)
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax --><br>当Sij=1，有：<br><span>$$\begin{gather*}
C=\log \left(1+e^{-\sigma\left(s_{i}-s_{j}\right)}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>当Sij=-1，有：<br><span>$$\begin{gather*}
C=\log \left(1+e^{-\sigma\left(s_{j}-s_{i}\right)}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>下图展示了当Sij分别取1，0，-1的时候cost function以si-sj为变量的函数图像：<br><img width="700" src="/images/search_fun_pic.png"><br>可以看到当Sij=1时，模型预测的si比sj越大，其代价越小；Sij=−1时，si比sj越小，代价越小；Sij=0时，代价的最小值在si与sj相等处取得。<br>该损失函数有下面两个特点：</p>
<ol>
<li>当两个相关性不同的文档算出来的模型分数相同时，损失函数的值大于0，仍会对这对pair做惩罚，使他们的排序位置区分开。</li>
<li>损失函数是一个类线性函数，可以有效减少异常样本数据对模型的影响，因此具有鲁棒性。</li>
</ol>
<p>代价函数C可微，下面就可以用随机梯度下降法来迭代更新模型参数wk了，即：<br><span>$$\begin{gather*}
w_{k} \rightarrow w_{k}-\eta \frac{\partial C}{\partial w_{k}}
\end{gather*}$$</span><!-- Has MathJax --><br>η为步长，代价C沿负梯度方向变化：<br><span>$$\begin{gather*}
\Delta C=\sum_{k} \frac{\partial C}{\partial w_{k}} \Delta w_{k}=\sum_{k} \frac{\partial C}{\partial w_{k}}\left(\eta \frac{\partial C}{\partial w_{k}}\right)=-\eta \sum_{k}\left(\frac{\partial C}{\partial w_{k}}\right)^{2}&lt;0
\end{gather*}$$</span><!-- Has MathJax --><br>这表明沿负梯度方向更新参数确实可以降低总代价。<br>随机梯度下降法时，有：<br><span>$$\begin{gather*}
\begin{aligned}
\frac{\partial C}{\partial w_{k}} &amp;=\frac{\partial C}{\partial s_{i}} \frac{\partial s_{i}}{\partial w_{k}}+\frac{\partial C}{\partial s_{j}} \frac{\partial s_{j}}{\partial w_{k}}=\sigma\left(\frac{1}{2}\left(1-S_{i j}\right)-\frac{1}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\right)\left(\frac{\partial s_{i}}{\partial w_{k}}-\frac{\partial s_{j}}{\partial w_{k}}\right) \\
&amp;=\lambda_{i j}\left(\frac{\partial s_{i}}{\partial w_{k}}-\frac{\partial s_{j}}{\partial w_{k}}\right)
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax --><br>其中：<br><span>$$\begin{gather*}
\lambda_{i j} \equiv \frac{\partial C\left(s_{i}-s_{j}\right)}{\partial s_{i}}=\sigma\left(\frac{1}{2}\left(1-S_{i j}\right)-\frac{1}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>上面的是对于每一对pair都会进行一次权重的更新，其实是可以对同一个query下的所有文档pair全部带入神经网络进行前向预测，然后计算总差分并进行误差后向反馈，这样将大大减少误差反向传播的次数，加速RankNet训练过程，即利用批处理的梯度下降法：<br><span>$$\begin{gather*}
\frac{\partial C}{\partial w_{k}}=\sum_{(i, j) \in I}\left(\frac{\partial C_{i j}}{\partial s_{i}} \frac{\partial s_{i}}{\partial w_{k}}+\frac{\partial C_{i j}}{\partial s_{j}} \frac{\partial s_{j}}{\partial w_{k}}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>其中：<br><span>$$\begin{gather*}
\frac{\partial C_{i j}}{\partial s_{i}}=\sigma\left(\frac{1}{2}\left(1-S_{i j}\right)-\frac{1}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\right)=-\frac{\partial C_{i j}}{\partial s_{j}}
\end{gather*}$$</span><!-- Has MathJax --><br>令：<br><span>$$\begin{gather*}
\lambda_{i j}=\frac{\partial C_{i j}}{\partial s_{i}}=\sigma\left(\frac{1}{2}\left(1-S_{i j}\right)-\frac{1}{1+e^{o\left(s_{i}-s_{j}\right)}}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>有：<br><span>$$\begin{gather*}
\begin{aligned}
\frac{\partial C}{\partial w_{k}} &amp;=\sum_{(i, j) \in I} \sigma\left(\frac{1}{2}\left(1-S_{i j}\right)-\frac{1}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\right)\left(\frac{\partial s_{i}}{\partial w_{k}}-\frac{\partial s_{j}}{\partial w_{k}}\right) \\
&amp;=\sum_{(i, j) \in I} \lambda_{i j}\left(\frac{\partial s_{i}}{\partial w_{k}}-\frac{\partial s_{j}}{\partial w_{k}}\right) \\
&amp;=\sum_{i} \lambda_{i} \frac{\partial s_{i}}{\partial w_{k}}
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax --></p>
<p>如何理解上式最后一步的化简及λi的意义呢？前面讲过集合I中只包含label不同的doc的集合，且每个pair仅包含一次，即(Ui,Uj)与(Uj,Ui)等价。为方便起见，我们假设I中只包含Ui相关性大于Uj的pair(Ui,Uj)，即I中的pair均满足Sij=1，那么：<br><span>$$\begin{gather*}
\lambda_{i}=\sum_{j:(i, j) \in I} \lambda_{i j}-\sum_{k:(k, i) \in I} \lambda_{k i}
\end{gather*}$$</span><!-- Has MathJax --><br>举例说明，假设有三个doc，其真实相关性满足U1&gt;U2&gt;U3，那么集合I中就包含{(1,2), (1,3), (2,3)}共三个pair，则：<br><span>$$\begin{gather*}
\frac{\partial C}{\partial w_{k}}=\left(\lambda_{12} \frac{\partial s_{1}}{\partial w_{k}}-\lambda_{12} \frac{\partial s_{2}}{\partial w_{k}}\right)+\left(\lambda_{13} \frac{\partial s_{1}}{\partial w_{k}}-\lambda_{13} \frac{\partial s_{3}}{\partial w_{k}}\right)+\left(\lambda_{23} \frac{\partial s_{2}}{\partial w_{k}}-\lambda_{23} \frac{\partial s_{3}}{\partial w_{k}}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>显然λ1=λ12+λ13，λ2=λ23−λ12，λ3=−λ13−λ23。<br><strong><em>λi决定着第i个doc在迭代中的移动方向和幅度，真实的排在Ui前面的doc越少，排在Ui后面的doc越多，那么文档Ui向前移动的幅度就越大。</em></strong><br>如何理解呢？我们可以结合损失函数C的图像来看，第i个doc与query越相关，λi越大，则wk变化越大，损失函数C减少越多，而损失函数C的图像在Sij=1时越小，si-sj越大，表明模型输出的文档i的分数与文档j分数相差越大，即文档Ui向前移动的幅度就越大。</p>
<h4 id="GBrank"><a href="#GBrank" class="headerlink" title="GBrank"></a>GBrank</h4><p><strong><em>基本思想：对两个具有relative relevance judgment的Documents，利用pairwise的方式构造一个特殊的 loss function，再使用GBDT的方法来对此loss function进行优化，求解其极小值。</em></strong><br>GBRank的创新点之一就在于构造一个特殊的loss function。首先，我们需要构造pair，即在同一个query下有两个doc，可以通过人工标注或者搜索日志来对这两个doc与该query的相关程度进行判断，得到一个pair关系，即其中一个doc的相关程度要比另一个doc的相关程度更高，这就是relative relevance judgment。一旦我们有了这个pairwise的相对关系，问题就成了如何利用这些doc pair学习出一个排序模型。<br>假设我们有以下的preference pairs 作为training data：<br><span>$$\begin{gather*}
\left\{\left(x_{i}^{(1)}, x_{i}^{(2)}\right), x_{i}^{(1)}&gt;x_{i}^{(2)}\right\}_{i=1}^{N}
\end{gather*}$$</span><!-- Has MathJax --><br>则可构造损失函数L(f)，与SVM中hinge loss类似:<br><span>$$\begin{gather*}
L(f)=\frac{1}{2} \sum_{i=1}^{N}\left(\max \left\{0, \tau-\left(f\left(x_{i}^{(1)}\right)-f\left(x_{i}^{(2)}\right)\right\}\right)^{2}\right.
\end{gather*}$$</span><!-- Has MathJax --><br>在hinge loss的基础上，将原来为1的参数改成了τ。即当两个doc相关度差距达到τ以上的时候，loss才为0。若f(x)输出固定，那么损失函数最少，但不是训练目标。<br>然后问题就变成了怎样对这个loss function进行优化求解极小值。这里使用了GBDT的思想，即Functional Gradient Descent的方法。</p>
<blockquote>
<p>在GBDT中，Functional Gradient Descent的使用为：将需要求解的f(x)表示成一个additive model，即将一个函数分解为若干个小函数的加和形式，而这每个小函数的产生过程是串行生成的，即每个小函数都是在拟合loss function在已有的f(x)上的梯度方向（由于训练数据是有限个数的，所以f(x)是离散值的向量，而此梯度方向也表示成一个离散值的向量），然后将拟合的结果函数进一步更新到f(x)中，形成一个新的f(x)。</p>
</blockquote>
<p>对于loss function，利用Functional Gradient Descent的方法优化为极小值的过程为：将f(x)表示成additive model，每次迭代的时候，用一个regression tree来拟合loss function在当前f(x)上的梯度方向。此时由于训练数据是有限个数的，f(x)同样只是一系列离散值，梯度向量也是一系列离散值，则可使用regression tree来拟合这一系列离散值。<br>但不同之处在于，由于使用pairwise方法，这里的loss function中，有两个不一样的f(x)的离散值，所以每次我们需要对f(x)在这两个点上的值都进行更新，即需要对一个training instance计算两个梯度方向。首先将以下两个变量看作未知变量：<br><span>$$\begin{gather*}
f\left(x_{i}^{(1)}\right), \quad f\left(x_{i}^{(2)}\right), \quad i=1, \cdots, N
\end{gather*}$$</span><!-- Has MathJax --><br>然后求解loss function对这两个未知变量的梯度（分别对两个未知变量求导），如下：<br><span>$$\begin{gather*}
-\max \left\{0, f\left(x_{i}^{(2)}\right)-f\left(x_{i}^{(1)}\right)+\tau\right\}, \quad \max \left\{0, f\left(x_{i}^{(2)}\right)-f\left(x_{i}^{(1)}\right)+\tau\right\}, \quad i=1, \cdots, N
\end{gather*}$$</span><!-- Has MathJax --><br>如果两个变量之差大于0，则此时对应的loss为0，我们无需对f(x)进行迭代更新；反之loss不为0，我们需要对f(x)进行迭代更新，即使得新的f(x)在这个instance上的两个点的预测值能够更接近真实值。f(x)更新类似于梯度下降方法中参数的更新：<br><span>$$\begin{gather*}
f_{k}(x)=f_{k-1}(x)-\eta \nabla L\left(f_{k}(x)\right)
\end{gather*}$$</span><!-- Has MathJax --><br>pairwise方法中，具体为：<br><span>$$\begin{gather*}
\begin{aligned}
&amp;f_{k}\left(x_{i}^{(1)}\right)=f_{k-1}\left(x_{i}^{(1)}\right)+\eta\left(f_{k-1}\left(x_{i}^{(2)}\right)-f_{k-1}\left(x_{i}^{(1)}\right)+\tau\right)\\
&amp;f_{k}\left(x_{i}^{(2)}\right)=f_{k-1}\left(x_{i}^{(2)}\right)-\eta\left(f_{k-1}\left(x_{i}^{(2)}\right)-f_{k-1}\left(x_{i}^{(1)}\right)+\tau\right)
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax --><br>当学习速率η等于1的时候，更新公式即为：<br><span>$$\begin{gather*}
\begin{aligned}
&amp;f_{k}\left(x_{i}^{(1)}\right)=f_{k-1}\left(x_{i}^{(2)}\right)+\tau\\
&amp;f_{k}\left(x_{i}^{(2)}\right)=f_{k-1}\left(x_{i}^{(1)}\right)-\tau
\end{aligned}
\end{gather*}$$</span><!-- Has MathJax --><br>当我们收集到所有loss值不为0的training instance后，我们便得到了其对应的更新值：<br><span>$$\begin{gather*}
\left\{\left(x_{i}^{(1)}, f_{k-1}\left(x_{i}^{(2)}\right)+\tau\right),\left(x_{i}^{(2)}, f_{k-1}\left(x_{i}^{(1)}\right)-\tau\right)\right\}
\end{gather*}$$</span><!-- Has MathJax --><br>接着，使用一棵regression tree对这些数据进行拟合，生成一个拟合函数$g_{k}(x)$，然后将这次迭代更新的拟合函数更新到f(x)中，此处采用线性叠加的方式：<br><span>$$\begin{gather*}
f_{k}(x)=\frac{k f_{k-1}(x)+\beta g_{k}(x)}{k+1}
\end{gather*}$$</span><!-- Has MathJax --><br>其中，ß为shrinking系数。<br>为什么在每次迭代更新的时候，新的regression tree不像GBDT中那样，纯粹地去拟合梯度方向（一个离散值的向量），而是去拟合：<br><span>$$\begin{gather*}
f_{k}(x)=f_{k-1}(x)-\eta \nabla L\left(f_{k}(x)\right)
\end{gather*}$$</span><!-- Has MathJax --><br>这样一个 原始预测值+梯度更新值 后的新预测值向量呢？<br>因为在每次迭代更新的时候，只是取了部分训练数据（即所有loss值不为0的training instance中的doc pair），所以每次拟合的时候，都只是对这部分数据进行训练，得到一个regression tree，然后把这个新的拟合函数（即regression tree）添加到总的预测函数f(x)中去，即这个regression tree在预测时候是需要对所有训练数据，而不是部分数据，进行预测的。所以如果每次迭代是去拟合梯度的话（梯度方向完全有可能与当前的f(x)向量方向相差很大），在预测的时候，这个regression tree对其余数据（并没有参与这个regression tree训练的数据）的预测值会偏离它们原始值较多，而且这个偏离是不在期望之中的，因为这些数据的当前预测值已经相对靠谱了（不会对loss function有贡献）。所以，当每次拟合的目标是 原始f(x)向量 + 梯度向量 的时候，这个新的向量不会跑的太偏（即跟原始向量相差较小），这时候拟合出来的结果regression tree在对整体数据进行预测的时候，也不会跑的太偏，只是会根据梯度方向稍微有所改变，对其它并不需要更新的数据的影响也相对较小。但同时也在逐渐朝着整体的优化方向上去尝试，所以才会这么去做。（以一个队伍在山峰间移动，分别寻找各自合适位置为例）。</p>
<h4 id="LambdaRank"><a href="#LambdaRank" class="headerlink" title="LambdaRank"></a>LambdaRank</h4><p>RankNet以错误pair最少为优化目标的RankNet算法，然而许多时候仅以错误pair数来评价排序的好坏是不够的，像NDCG或者ERR等评价指标就只关注top k个结果的排序，当我们采用RankNet算法时，往往无法以这些指标为优化目标进行迭代，所以RankNet的优化目标和IR评价指标之间还是存在gap的，如下图所示：<br><img width="500" src="/images/search_lambdaRank.png"><br>如上图所示，每个线条表示文档，蓝色表示相关文档，灰色表示不相关文档，RankNet以pairwise error的方式计算cost，左图的cost为13，右图通过把第一个相关文档下调3个位置，第二个文档上条5个位置，将cost降为11，但是像NDCG或者ERR等评价指标只关注top k个结果的排序，在优化过程中下调前面相关文档的位置不是我们想要得到的结果。图 1右图左边黑色的箭头表示RankNet下一轮的调序方向和强度，但我们真正需要的是右边红色箭头代表的方向和强度，即更关注靠前位置的相关文档的排序位置的提升。LambdaRank正是基于这个思想演化而来，其中Lambda指的就是红色箭头，代表下一次迭代优化的方向和强度，也就是梯度。<br>LambdaRank在RankNet的加速算法形式的基础上引入评价指标Z （如NDCG、ERR等），把交换两个文档的位置引起的评价指标的变化|ΔNDCG|，作为其中一个因子，实验表明对模型效果有显著的提升：<br><span>$$\begin{gather*}
\lambda_{i j}=\frac{\partial C\left(s_{i}-s_{j}\right)}{\partial s_{i}}=\frac{-\sigma}{1+e^{\sigma\left(s_{i}-s_{j}\right)}}\left|\Delta_{N D C G}\right|
\end{gather*}$$</span><!-- Has MathJax --><br>ΔNDCG表示将Ui和Uj进行交换，交换后排序的NDCG与交换前排序的NDCG的差值。<br>损失函数的梯度代表了文档下一次迭代优化的方向和强度，由于引入了IR评价指标，Lambda梯度更关注位置靠前的优质文档的排序位置的提升。有效的避免了下调位置靠前优质文档的位置这种情况的发生。LambdaRank相比RankNet的优势在于分解因式后训练速度变快，同时考虑了评价指标，直接对问题求解，效果更明显。</p>
<h4 id="LambdaMART"><a href="#LambdaMART" class="headerlink" title="LambdaMART"></a>LambdaMART</h4><ol>
<li>Mart（Multiple Additive Regression Tree，Mart就是GBDT），定义了一个框架，缺少一个梯度。</li>
<li>LambdaRank重新定义了梯度，赋予了梯度新的物理意义。</li>
</ol>
<p>因此，所有可以使用梯度下降法求解的模型都可以使用这个梯度，MART就是其中一种，将梯度Lambda和MART结合就是大名鼎鼎的LambdaMART。<br>MART的原理是直接在函数空间对函数进行求解，模型结果由许多棵树组成，每棵树的拟合目标是损失函数的梯度，在LambdaMART中就是Lambda。LambdaMART的具体算法过程如下：<br><img width="700" src="/images/search_lambdaMart.png"><br>可以看出LambdaMART的框架其实就是MART，主要的创新在于中间计算的梯度使用的是Lambda，是pairwise的。MART需要设置的参数包括：树的数量M、叶子节点数L和学习率v，这3个参数可以通过验证集调节获取最优参数。</p>
<p>MART输出值的计算：</p>
<ol>
<li><p>首先设置每棵树的最大叶子数，基分类器通过最小平方损失进行分裂，达到最大叶子数量时停止分裂</p>
</li>
<li><p>使用牛顿法得到叶子的输出，计算效用函数对模型得分的二阶导<span>$\frac{\partial \lambda_{i}}{\partial s_{i}}=\frac{\partial^{2} C}{\partial^{2} s_{i}}$</span><!-- Has MathJax --></p>
<span>$$\begin{gather*}

\frac{\partial^{2} C}{\partial^{2} s_{i}}=\sum_{\{i, j\} \rightleftharpoons I} \sigma^{2}\left|\triangle Z_{i j}\right| \rho_{i j}\left(1-\rho_{i j}\right)&NegativeMediumSpace;

\end{gather*}$$</span><!-- Has MathJax -->
</li>
<li><p>得到第m颗树的第k个叶子的输出值:</p>
<span>$$\begin{gather*}

\gamma_{k m}=\frac{\sum_{x_{i} \in R_{k m}} \frac{\partial C}{\partial s_{i}}}{\sum_{x_{i} \in R_{k m}} \frac{\partial^{2} C}{\partial^{2} s_{i}}}=\frac{-\sum_{x_{i} \in R_{k m}} \sum_{\{i, j\} \rightleftharpoons I}\left|\triangle Z_{i j}\right| \rho_{i j}}{\sum_{x_{i} \in R_{k m}} \sum_{\{i, j\} \rightleftharpoons I}\left|\Delta Z_{i j}\right| \sigma \rho_{i j}\left(1-\rho_{i j}\right)}&NegativeMediumSpace;

\end{gather*}$$</span><!-- Has MathJax -->
</li>
<li><p><span>$x_i$</span><!-- Has MathJax -->为第i个样本，<span>$x_{i} \in R_{k m}$</span><!-- Has MathJax -->意味着落入该叶子的样本，这些样本共同决定了该叶子的输出值。</p>
</li>
</ol>
<p>LambdaMART具有很多优势：</p>
<ol>
<li>适用于排序场景：不是传统的通过分类或者回归的方法求解排序问题，而是直接求解;</li>
<li>损失函数可导：通过损失函数的转换，将类似于NDCG这种无法求导的IR评价指标转换成可以求导的函数，并且赋予了梯度的实际物理意义，数学解释非常漂亮;</li>
<li>增量学习：由于每次训练可以在已有的模型上继续训练，因此适合于增量学习;</li>
<li>组合特征：因为采用树模型，因此可以学到不同特征组合情况;</li>
<li>特征选择：因为是基于MART模型，因此也具有MART的优势，可以学到每个特征的重要性，可以做特征选择;</li>
<li>适用于正负样本比例失衡的数据：因为模型的训练对象具有不同label的文档pair，而不是预测每个文档的label，因此对正负样本比例失衡不敏感。</li>
</ol>
<p><strong>LambdaMART是不是很好懂？</strong></p>
<p><img width="300" src="/images/Emoticon1.png"></p>
<p>有较多博客、资料在此戛然而止，好像上述资料已经足够理解LambdaMART了，这也让我一度怀疑自己的IQ，基础和我一样不太好的兄弟们可以进入另一篇博客，我们从提升树、梯度提升树和梯度提升决策树说起，并结合Ranklib源码和具体的例子，以及微软的相关技术报告一起来看看LambdaMART的<a href="">真相</a>。</p>
<blockquote>
<p>爱奇艺实践：<br>在没有加入稀疏类特征之前，我们的模型是LambdaMART模型，在IR领域是最先进的模型，该模型是一个gbdt模型，基于boosting思想，不断增加决策树，来减小残差。该模型在很多竞赛中表现良好，因为不用过多的特征处理，树模型会考虑特征本身的数据分布，同时有很好的学习泛化能力，树结构很难兼容高维稀疏特征，比方说我们的document是上亿级的特征，很难每个节点走一次树的分割，所以对于加入稀疏特征的时候，树模型会遇到瓶颈。但是在处理高维稀疏特征的时候，像LR、FM、FFM可以认为是线性模型，特征的增加并不会对此类模型造成压力，上亿维也没关系。LR模型弱点在于特征组合能力不足，很多情况下特征组合方式比较重要，树模型从根节点到叶子节点的路径其实是一种组合方式。如下图所示：</p>
</blockquote>
<p><img width="700" src="/images/search_iqiyi_lm.png"></p>
<h4 id="深度模型"><a href="#深度模型" class="headerlink" title="深度模型"></a>深度模型</h4><h5 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h5><p>爱奇艺实现：<br><img width="700" src="/images/search_iqiyi_dnn.jpg"><br>底层是query和document的描述文本做多粒度切词，之后做embedding后加权平均，得到document和query的向量表达，拼接这两组向量，同时再做点积，（两个向量越来越相近，拼接的时候希望上层网络学到两个向量的相似性，需要有足够的样本和正负样例，所以我们自己做了点积）。除了向量特征，模型也适用了稠密特征，即利用gbdt抽取特征，与embedding特征做拼接，最后经过三个全连接层，接sigmoid函数，就可以得到样本的score，并在此基础上用ndcg的衡量标准去计算损失，从而反向优化网络结构。而在online服务侧，则直接用样本去predict得分。这个模型上线之后，效果非常明显。其中，二次搜索率降低（二次搜索率越低越好，说明用户一次搜中）。</p>
<h5 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h5><p>美团点评模型框架：<br><img width="700" src="/images/search_wd_structure.png"><br>美团点评模型具体实现：<br><img width="700" src="/images/search_wd_model.png"><br>在训练时，分别对样本数据进行清洗和提权。在特征方面，对于连续特征，用Min-Max方法做归一化。在交叉特征方面，结合业务需求，提炼出多个在业务场景意义比较重大的交叉特征。用Adam做为优化器，用Cross Entropy做为损失函数。在训练期间，与Wide &amp; Deep Learning论文中不同之处在于，将组合特征作为输入层分别输入到对应的Deep组件和Wide组件中。然后在Deep部分将全部输入数据送到3个ReLU层，在最后通过Sigmoid层进行打分。训练数据7000万+，并用超过3000万的测试数据进行线下模型预估。Batch－Size设为50000，Epoch设为20。<br>线上AB实验结果：<br><img width="700" src="/images/search_wd_abtest.png"></p>
<h5 id="LambdaDNN"><a href="#LambdaDNN" class="headerlink" title="LambdaDNN"></a>LambdaDNN</h5><p>大众点评搜索排序模型，基于TensorFlow分布式框架实现。<br><img width="700" src="/images/search_lambdaDnn.png"><br>NDCG的计算公式中，折损的权重是随着位置呈指数变化的。然而实际曝光点击率随位置变化的曲线与NDCG的理论折损值存在着较大的差异。<br>对于移动端的场景来说，用户在下拉滑动列表进行浏览时，视觉的焦点会随着滑屏、翻页而发生变动。例如用户翻到第二页时，往往会重新聚焦，因此，会发现第二页头部的曝光点击率实际上是高于第一页尾部位置的。我们尝试了两种方案去微调NDCG中的指数位置折损：<br>根据实际曝光点击率拟合折损曲线：根据实际统计到的曝光点击率数据，拟合公式替代NDCG中的指数折损公式，绘制的曲线如图12所示。<br>计算Position Bias作为位置折损：Position Bias在业界有较多的讨论，用户点击商户的过程可以分为观察和点击两个步骤：a.用户需要首先看到该商户，而看到商户的概率取决于所在的位置；b.看到商户后点击商户的概率只与商户的相关性有关。步骤a计算的概率即为Position Bias，这块内容可以讨论的东西很多，这里不再详述。<br><img width="700" src="/images/search_ndcg.png"><br>经过上述对NDCG计算改造训练出的LambdaDNN模型，相较Base树模型和Pointwise DNN模型，在业务指标上有了非常显著的提升。<br><img width="700" src="/images/search_lm_dnn_lmDnn.png"></p>
<h5 id="LambdaDeepFM"><a href="#LambdaDeepFM" class="headerlink" title="LambdaDeepFM"></a>LambdaDeepFM</h5><p>[待补充]</p>
<h5 id="LambdaDCN"><a href="#LambdaDCN" class="headerlink" title="LambdaDCN"></a>LambdaDCN</h5><p>Lambda梯度除了与DNN网络相结合外，也可以与绝大部分常见的网络结构相结合。为了进一步学习到更多交叉特征，美团点评团队在LambdaDNN的基础上分别尝试了LambdaDeepFM和LambdaDCN网络；其中DCN网络是一种加入Cross的并行网络结构，交叉的网络每一层的输出特征与第一层的原始输入特征进行显性的两两交叉，相当于每一层学习特征交叉的映射去拟合层之间的残差。<br><img width="700" src="/images/search_lambdaDcn.png"><br>离线的对比实验表明，Lambda梯度与DCN网络结合之后充分发挥了DCN网络的特点，简洁的多项式交叉设计有效地提升模型的训练效果。NDCG指标对比效果如下图所示：<br><img width="500" src="/images/search_dcn_compare.png"></p>
<h4 id="补充模型"><a href="#补充模型" class="headerlink" title="补充模型"></a>补充模型</h4><h5 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h5><p><img width="700" src="/images/search_fm.png"></p>
<h5 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h5><p><img width="700" src="/images/search_dssm.png"></p>
<h5 id="点击模型"><a href="#点击模型" class="headerlink" title="点击模型"></a>点击模型</h5><p><img width="700" src="/images/search_click.png"></p>
<h5 id="DBN模型"><a href="#DBN模型" class="headerlink" title="DBN模型"></a>DBN模型</h5><p><img width="700" src="/images/search_dbn.png"></p>
<h5 id="IRGAN"><a href="#IRGAN" class="headerlink" title="IRGAN"></a>IRGAN</h5><p><img width="700" src="/images/search_irgan.png"></p>
<h1 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h1><ol>
<li>LambdaRank</li>
<li>LambdaMART</li>
<li>Wide&amp;Deep</li>
</ol>
<h1 id="模型组合"><a href="#模型组合" class="headerlink" title="模型组合"></a>模型组合</h1><p>爱奇艺实践：<br><img width="700" src="/images/search_model_compare.png"></p>
<blockquote>
<p>针对GBDT和LR模型的优缺点，做了进一步的模型组合的尝试：<br>第一种方式，用 LR 模型把高维稠密特征进行学习，学习出高维特征，把该特征和原始特征做拼接，学习 gbdt 模型。<br>该办法效果不好，提升很弱。<br><strong> 剖析缘由：</strong> 把高维特征刚在一个特征去表达，丢掉了原始的特征。<br>第二种方式，用 gbdt 去学，学习后把样本落入叶子节点信息来进来与高维稠密特征拼接，在此根底上用 LR 学习。<br>该模型效果变差。<br><strong> 剖析缘由：</strong> 点击类和穿插类特征是对排序影响最大的特征，这类特征和大量的稠密类特征做拼接时，重要性被稀释了，导致模型的学习能力变弱。</p>
</blockquote>
<h1 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h1><p><strong>Python训练+Golang部署</strong></p>
<ol>
<li>LambdaRank，基于xgboost实现，参考：<a href="https://github.com/dmlc/xgboost/tree/master/demo/rank" target="_blank" rel="external">https://github.com/dmlc/xgboost/tree/master/demo/rank</a></li>
<li>LambdaMART，基于LightGBM实现，参考：<a href="https://github.com/jiangnanboy/learning_to_rank" target="_blank" rel="external">https://github.com/jiangnanboy/learning_to_rank</a></li>
<li>Wide&amp;Deep基于TensorFlow实现，参考：<a href="https://github.com/tensorflow/ranking" target="_blank" rel="external">https://github.com/tensorflow/ranking</a></li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://mp.weixin.qq.com/s/NqVP0ksfLiRLSGkuWxiz5A" target="_blank" rel="external">浅谈微视推荐系统中的特征工程</a><br><a href="https://cloud.tencent.com/developer/news/184638" target="_blank" rel="external">回顾·搜索引擎算法体系简介——排序和意图篇</a><br><a href="https://tech.meituan.com/2017/07/28/dl.html" target="_blank" rel="external">深度学习在美团推荐平台排序中的运用</a><br><a href="https://www.cnblogs.com/bentuwuying/p/6684585.html" target="_blank" rel="external">Learning to Rank算法介绍：GBRank</a><br><a href="https://www.cnblogs.com/bentuwuying/p/6683832.html" target="_blank" rel="external">Learning to Rank算法介绍：RankSVM 和 IR SVM</a><br><a href="https://tech.meituan.com/2019/01/17/dianping-search-deeplearning.html" target="_blank" rel="external">大众点评搜索基于知识图谱的深度学习排序实践</a><br><a href="https://www.cnblogs.com/bentuwuying/p/6690836.html" target="_blank" rel="external">Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart</a><br><a href="https://cloud.tencent.com/developer/article/1500313" target="_blank" rel="external">「回顾」爱奇艺搜索排序模型迭代之路</a><br><a href="https://blog.csdn.net/v_JULY_v/article/details/81410574" target="_blank" rel="external">通俗理解kaggle比赛大杀器xgboost</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/18/Learning-to-rank算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/18/Learning-to-rank算法/" itemprop="url">Learning to rank算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-18T15:25:57+08:00">
                2019-11-18
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>互联网<strong>搜索</strong>诞生初期，检索模型使用的特征相对简单，这些特征主要基于query与doc之间的相关度来对文档进行排序。另一种传统排序模型是重要性排序模型，此时模型不考虑query ，而仅仅根据文档（网页）之间的图结构来判断doc的重要程度，例如PageRank排序模型。而随着互联网不断发展，海量数据的产生，更多复杂有效的特征被用于搜索排序，人工调参已不能满足需求，此时机器学习开始在搜索排序领域得到应用，learning to rank逐渐成为热门研究方向。</p>
<h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索的目标是选择与用户输入query最相关的一组文档，目前主要步骤如下：</p>
<ol>
<li>粗排：query-doc匹配，得到一组与query相关的文档，目前一般采用倒排索引实现；</li>
<li>精排：选取更多特征，按照用户点击该doc的可能性大小排序，Learning to rank即是实现该部分的机器学习模型。</li>
</ol>
<h2 id="机器学习排序系统"><a href="#机器学习排序系统" class="headerlink" title="机器学习排序系统"></a>机器学习排序系统</h2><p>典型的机器学习排序系统如下图所示：<br><img src="/images/rank-system.jpg" alt=""></p>
<h2 id="Learning-to-rank"><a href="#Learning-to-rank" class="headerlink" title="Learning to rank"></a>Learning to rank</h2><p>包含pointwise方法、pairwise方法和listwise方法三种类型。</p>
<p>(1) pointwise方法：对于某一个query， 判断每个doc这个query的相关度，由此将docs排序问题转化为了分类（比如相关、不相关）或回归问题（相关程度越大，回归函数的值越大）。其L2R框架具有以下特征：</p>
<ul>
<li>输入空间中样本是单个 doc（和对应 query）构成的特征向量；</li>
<li>输出空间中样本是单个 doc（和对应 query）的相关度；</li>
<li>假设样本满足打分函数；</li>
<li>损失函数评估单个 doc 的预测得分和真实得分之间差异。</li>
</ul>
<p>(2) pairwise方法：该方法并不关心某一个doc与query相关程度的具体数值，而是将排序问题转化为任意两个不同的docs di和dj谁与当前的query更相关的相对顺序的排序问题。一般分为di比dj更相关、更不相关和相关程度相等三个类别，分别记为{+1, -1, 0}，由此便又转化为了分类问题。其 L2R 框架具有以下特征：</p>
<ul>
<li>输入空间中样本是（同一 query 对应的）两个 doc（和对应 query）构成的两个特征向量；</li>
<li>输出空间中样本是 pairwise preference；</li>
<li>假设空间中样本满足二变量函数；</li>
<li>损失函数评估 doc pair 的预测 preference 和真实 preference 之间差异。</li>
</ul>
<p>(3) listwise方法：将一个query对应的所有相关文档看做一个整体，作为单个训练样本。其 L2R 框架具有以下特征：</p>
<ul>
<li>输入空间中样本是（同一 query 对应的）所有 doc构成的多个特征向量（列表）；</li>
<li>输出空间中样本是这些 doc与对应 query的相关度排序列表；</li>
<li>假设空间中样本满足多变量函数，对于 docs 得到其排列，实践中，通常是一个打分函数，根据打分函数对所有 docs 的打分进行排序得到 docs 相关度的排列；</li>
<li>损失函数分成两类，一类是直接和评价指标相关的，还有一类不是直接相关的。</li>
</ul>
<p>由于评价指标通常是离散不可微的，直接优化ranking评价指标并不简单，通常有以下处理方式：</p>
<ul>
<li>优化基于评价指标的 ranking error 的连续可微的近似，这种方法就可以直接应用已有的优化方法，如SoftRank，ApproximateRank，SmoothRank。</li>
<li>优化基于评价指标的 ranking error 的连续可微的上界，如 SVM-MAP，SVM-NDCG，PermuRank。</li>
<li>使用可以优化非平滑目标函数的优化技术，如 AdaRank，RankGP。非直接相关的损失函数则不再使用与评价相关的loss来优化模型，而是设计能衡量模型输出与真实排列之间差异的 loss，如此获得的模型在评价指标上也能获得不错的性能。例如 ：ListNet，ListMLE，StructRank和BoltzRank。</li>
</ul>
<h3 id="主要模型分类"><a href="#主要模型分类" class="headerlink" title="主要模型分类"></a>主要模型分类</h3><p>按照这三种类型，可以将主要模型做如下梳理：<br><img width="600" src="/images/search_kinds.png"><br>图中Rank Creation指给定查询Query和文档Docs，得到Docs排序结果；Rank Aggregation指综合多个不同的Ranking System的排序结果，得出最终排序结果。</p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p>下面以xgboost模型为例，说明如何完成排序。大致过程如下：<br><strong>训练</strong></p>
<ol>
<li>获取训练数据；</li>
<li>构造特征候选集，完成训练样本准备；</li>
<li>基于xgboost寻找划分点，重复该步至不能再分裂划分点；</li>
<li>通过最小化pairwise loss生成下一棵树；</li>
<li>生成设定数量的树后，训练完成；</li>
</ol>
<p><strong>测试</strong></p>
<ol>
<li>输入测试样本；</li>
<li>根据训练所得模型和打分函数进行打分；</li>
<li>获得每个<query,doc>对打分，并完成排序。</query,doc></li>
</ol>
<p>值得注意的是，目前使用比较多的XGBoost ranking模型是LambdaRank，但尚未完成，所以目前使用pairwise rank，参考<a href="https://github.com/dmlc/xgboost/tree/master/demo/rank" target="_blank" rel="external">dmlc/xgboost</a> .</p>
<p><strong>代码示例</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">import os,sys,json</div><div class="line">import math</div><div class="line">import random</div><div class="line">import numpy as np</div><div class="line">import pandas as pd</div><div class="line">import sklearn.utils</div><div class="line">import xgboost as xgb</div><div class="line"></div><div class="line">enableFeatures = [</div><div class="line">    &quot;fea1&quot;,</div><div class="line">    &quot;fea2&quot;</div><div class="line">]</div><div class="line"></div><div class="line">train_data = pd.read_csv(&quot;train.csv&quot;, sep=&quot;\t&quot;)</div><div class="line">test_data = pd.read_csv(&quot;test.csv&quot;, sep=&quot;\t&quot;)</div><div class="line">print(&quot;all data &#123;0&#125;&quot;.format(train_data.shape))</div><div class="line">print(&quot;all data &#123;0&#125;&quot;.format(test_data.shape))</div><div class="line"></div><div class="line"># 分组处理数据</div><div class="line">traingroups = &#123;&#125;</div><div class="line">validgroups = &#123;&#125;</div><div class="line"></div><div class="line">for qid, df in list(train_data.groupby(&quot;group_id&quot;)):</div><div class="line">    group = traingroups.setdefault(qid, &#123;&#125;)</div><div class="line">    group[&quot;length&quot;] = len(df)</div><div class="line">    group[&quot;weight&quot;] = 1</div><div class="line">    group[&quot;df&quot;] = df</div><div class="line"></div><div class="line">for qid, df in list(test_data.groupby(&quot;group_id&quot;)):</div><div class="line">    group = validgroups.setdefault(qid, &#123;&#125;)</div><div class="line">    group[&quot;length&quot;] = len(df)</div><div class="line">    group[&quot;weight&quot;] = 1</div><div class="line">    group[&quot;df&quot;] = df</div><div class="line"></div><div class="line">print(&quot;groups&quot;)</div><div class="line">traingroups = [group for qid, group in traingroups.items() ]</div><div class="line">validgroups = [group for qid, group in validgroups.items() ]</div><div class="line"></div><div class="line">print(&quot;concat&quot;)</div><div class="line">train_X = pd.concat(map(lambda x: sklearn.utils.shuffle(x[&quot;df&quot;]), traingroups), ignore_index=True)</div><div class="line">valid_X = pd.concat(map(lambda x: sklearn.utils.shuffle(x[&quot;df&quot;]), validgroups), ignore_index=True)</div><div class="line">train_y = train_X[&quot;label&quot;]</div><div class="line">valid_y = valid_X[&quot;label&quot;]</div><div class="line"></div><div class="line">train_X = train_X[enableFeatures]</div><div class="line">valid_X = valid_X[enableFeatures]</div><div class="line">train_X, valid_X = train_X.align(valid_X, join=&quot;left&quot;, axis=1)</div><div class="line"></div><div class="line">print(&quot;data length weight&quot;)</div><div class="line">train_q = np.array(list(map(lambda x: x[&quot;length&quot;], traingroups)))</div><div class="line">valid_q = np.array(list(map(lambda x: x[&quot;length&quot;], validgroups)))</div><div class="line">train_w = np.array(list(map(lambda x: x[&quot;weight&quot;], traingroups)))</div><div class="line">valid_w = np.array(list(map(lambda x: x[&quot;weight&quot;], validgroups)))</div><div class="line"></div><div class="line">print(&quot;model train&quot;)</div><div class="line">#&quot;rank:ndcg&quot;</div><div class="line">ranker = xgb.XGBRanker(objective=&quot;rank:pairwise&quot;)</div><div class="line">ranker.fit(train_X, train_y, train_q, sample_weight=train_w,</div><div class="line">    eval_set=[(valid_X, valid_y)], eval_group=[valid_q], sample_weight_eval_set=[valid_w],</div><div class="line">    eval_metric=[&quot;ndcg@5-&quot;, &quot;ndcg@10-&quot;], early_stopping_rounds=5, verbose=True,</div><div class="line">    callbacks=[xgb.callback.reset_learning_rate(lambda x,y: 0.95 ** x * 0.1)])</div><div class="line">ranker.feature_names = map(str, train_X.columns)</div><div class="line">with open(&quot;xgb_pair.model.features&quot;, &quot;w&quot;) as fw:</div><div class="line">    fw.write( &quot;,&quot;.join(ranker.feature_names))</div><div class="line">ranker.save_model(&apos;xgb_pair.model&apos;)</div><div class="line">model = xgb.Booster(model_file=&apos;xgb_pair.model&apos;)</div><div class="line">print(ranker.get_booster().get_score(importance_type=&apos;gain&apos;) )</div></pre></td></tr></table></figure></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/seasongirl/article/details/100178083" target="_blank" rel="external">用xgboost做排序任务——xgboost下的learning2rank</a><br><a href="https://blog.csdn.net/zuolixiangfisher/article/details/81072922" target="_blank" rel="external">Learning to Rank系列之概述</a><br><a href="http://www.bdpt.net/cn/2018/11/20/rank教程-06-learning-to-rank-概述/" target="_blank" rel="external">Learning to Rank 概述</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/31/BERT二阶段fine-tune代码分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/31/BERT二阶段fine-tune代码分析/" itemprop="url">BERT二阶段fine tune代码分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-31T16:56:20+08:00">
                2019-05-31
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>BERT除预训练代码run_pretraining.py外，还提供了run_classifier.py用于文本分类和run_squad.py用于阅读理解，下面通过对比三个代码总结出如何快速基于BERT做二阶段fine tune的方法。此外，如果TF Hub中有对应任务可使用的预训练模型，也可直接使用，例如同样用于分类的run_classifier_with_tfhub.py。</p>
<h2 id="代码整体结构"><a href="#代码整体结构" class="headerlink" title="代码整体结构"></a>代码整体结构</h2><p>代码的整体流程如下图所示：<br><img src="/images/BERT.png" alt=""></p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在bert_config_file文件中配置各个参数，例如attention_probs_dropout_prob和directionality等，config文件在BERT提供的预训练模型中。</p>
<h3 id="输入处理"><a href="#输入处理" class="headerlink" title="输入处理"></a>输入处理</h3><p>结构图中以get input files表示整个的输入数据处理部分，不同于早期版本的数据处理过程，当前的TF版本将数据转化为features用于训练。所以需要建立相应的结构体承接数据，并建立对应的数据处理方法，最后转化为features，下表中convert_exps_to_features为convert_examples_to_features简写。</p>
<table>
<thead>
<tr>
<th>代码/功能</th>
<th>承接数据</th>
<th>数据处理方法</th>
<th>转化为features</th>
</tr>
</thead>
<tbody>
<tr>
<td>run_classifier.py</td>
<td>InputExample, InputFeatures</td>
<td>DataProcessor</td>
<td>convert_single_example</td>
</tr>
<tr>
<td>run_squad.py</td>
<td>SquadExample, InputFeatures</td>
<td>read_squad_examples</td>
<td>convert_exps_to_features</td>
</tr>
</tbody>
</table>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>以run_classifier.py为例，使用model_fn_builder函数建立模型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">model_fn = model_fn_builder(</div><div class="line">      bert_config=bert_config,</div><div class="line">      num_labels=len(label_list),</div><div class="line">      init_checkpoint=FLAGS.init_checkpoint,</div><div class="line">      learning_rate=FLAGS.learning_rate,</div><div class="line">      num_train_steps=num_train_steps,</div><div class="line">      num_warmup_steps=num_warmup_steps,</div><div class="line">      use_tpu=FLAGS.use_tpu,</div><div class="line">      use_one_hot_embeddings=FLAGS.use_tpu)</div></pre></td></tr></table></figure></p>
<p>具体地，将配置、训练数据等信息传入create_model函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">(total_loss, per_example_loss, logits, probabilities) = create_model(</div><div class="line">        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,</div><div class="line">        num_labels, use_one_hot_embeddings)</div></pre></td></tr></table></figure></p>
<p>create_model函数首先基于BERT建立模型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">model = modeling.BertModel(</div><div class="line">      config=bert_config,</div><div class="line">      is_training=is_training,</div><div class="line">      input_ids=input_ids,</div><div class="line">      input_mask=input_mask,</div><div class="line">      token_type_ids=segment_ids,</div><div class="line">      use_one_hot_embeddings=use_one_hot_embeddings)</div></pre></td></tr></table></figure></p>
<p>然后根据需要取得BERT模型的输出，例如在run_classifier.py中:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">output_layer = model.get_pooled_output()</div></pre></td></tr></table></figure></p>
<p>run_squad.py中:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">final_hidden = model.get_sequence_output()</div></pre></td></tr></table></figure></p>
<p>此部分相当于利用BERT作为一个Encoder来编码输入信息，随后便可根据任务定义对应的可学习参数和loss。</p>
<h3 id="建立estimator"><a href="#建立estimator" class="headerlink" title="建立estimator"></a>建立estimator</h3><p>根据已建立的模型和配置信息新建estimator。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">estimator = tf.contrib.tpu.TPUEstimator(</div><div class="line">      use_tpu=FLAGS.use_tpu,</div><div class="line">      model_fn=model_fn,</div><div class="line">      config=run_config,</div><div class="line">      train_batch_size=FLAGS.train_batch_size,</div><div class="line">      predict_batch_size=FLAGS.predict_batch_size)</div></pre></td></tr></table></figure></p>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><p>使用input_fn_builder建立训练数据，输入estimator开始训练。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">train_input_fn = input_fn_builder(</div><div class="line">        input_file=train_writer.filename,</div><div class="line">        seq_length=FLAGS.max_seq_length,</div><div class="line">        is_training=True,</div><div class="line">        drop_remainder=True)</div><div class="line">    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)</div></pre></td></tr></table></figure></p>
<h2 id="使用TF-Hub"><a href="#使用TF-Hub" class="headerlink" title="使用TF Hub"></a>使用TF Hub</h2><p>run_classifier_with_tfhub.py与run_classifier.py整体流程非常类似，区别在于run_classifier_with_tfhub.py中获得BERT模型是通过TF Hub。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bert_module = hub.Module(bert_hub_module_handle, tags=tags, trainable=True)</div></pre></td></tr></table></figure></p>
<p>获得BERT模型输出时需要指定signature。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bert_outputs = bert_module(</div><div class="line">      inputs=bert_inputs,</div><div class="line">      signature=&quot;tokens&quot;,</div><div class="line">      as_dict=True)</div></pre></td></tr></table></figure></p>
<p>其他部分与run_classifier.py类似。</p>
<h2 id="新任务"><a href="#新任务" class="headerlink" title="新任务"></a>新任务</h2><p>对于一个新任务，可参考run_classifier.py代码进行修改，主要修改数据处理、模型建立等部分，具体地，InputExample，InputFeatures，convert_examples_to_features和create_model函数等。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/15/文本查重-SimHash和MinHash算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/15/文本查重-SimHash和MinHash算法/" itemprop="url">文本查重-SimHash和MinHash算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-15T15:16:45+08:00">
                2019-05-15
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>SimHash和MinHash算法主要应用于海量文本查重，两者都属于<a href="https://www.cnblogs.com/wt869054461/p/8148940.html" target="_blank" rel="external">局部敏感哈希</a>（Locality-Sensitive Hashing, LSH）算法，而LSH又是<a href="http://www.cnblogs.com/ljygoodgoodstudydaydayup/p/10519253.html" target="_blank" rel="external">近似最近邻查找</a>（Approximate Nearest  Neighbor, ANN）中的一类算法，其主要思想是利用降维和索引，加快查找过程。</p>
<h2 id="SimHash"><a href="#SimHash" class="headerlink" title="SimHash"></a>SimHash</h2><p>算法的过程如下图所示：<br><img src="http://dl.iteye.com/upload/attachment/437426/baf42378-e625-35d2-9a89-471524a355d8.jpg" alt=""><br>具体过程为：</p>
<ol>
<li>确定文本Doc中各个词语（中文需分词）的权重，方法可以有多种，例如TfIdf和TextRank算法，则文档可表示为(feature, weight)组成的向量；</li>
<li>初始化一个长度为64位（可综合空间和时间复杂度考虑具体数值）的Doc特征向量，各元素初始化为0；</li>
<li>对Doc中每个词语利用hash函数计算一个长度为64位的特征向量；</li>
<li>遍历词语特征向量，如果第i位为1，则Doc特征向量对应位+w，否则-w；</li>
<li>所有词语处理完毕后，判断Doc特征向量的每一位，如果大于0，则置1，否则置0；</li>
<li>获得各Doc的特征向量后，可利用海明距离判断两篇文档的相似性，一般地，距离&lt;=3时认为两者是相似的。</li>
</ol>
<p>上述过程可以理解为Doc的特征降维过程，我们将降维后的特征向量成为fingerprints。完成降维后，我们需要思考如何设计索引来加快查找过程。<br>第一种方案是对查询向量Q进行变化，查找64位所有3位以内的变化的组合，如下图所示：<br><img src="http://dl.iteye.com/upload/attachment/438114/ae5a47cb-068c-32b6-a4d5-6a5b808b0b26.jpg" alt=""><br>单一内容Q需要进行四万多次查询，时间复杂度太高。<br>第二种方案是对已生成Doc的fingerprints进行3位以内的组合变化，但需要多占据四万多倍的原始空间，如下图所示：<br><img src="http://dl.iteye.com/upload/attachment/437527/d31f3880-d43e-33ac-89c3-3e11a3f4f95e.jpg" alt=""><br>如何找到一种空间和时间的平衡呢？假设海明距离在3以内的两个文档是相似的，那么只要将64位的二进制串分为4块，根据鸽巢原理，两个文档的fingerprints中至少有一块是完全相同的，如下图所示：<br><img src="http://dl.iteye.com/upload/attachment/437559/689719df-54b7-318c-bc90-e289f84344b9.jpg" alt=""><br>由于查询时无法预知是哪一块区域相同，因此需要对四块中的每一块都建立索引，假设样本库中有<span>$2^32$</span><!-- Has MathJax -->，约43亿的fingerprints，那么每个table中包含<span>$2^(32-16)$</span><!-- Has MathJax -->=65536个候选结果，大大减少了海明距离的计算成本。上图所示的索引结构在MongoDB中实现非常简单。此外，索引块数k和table中候选结果数量m也是一个平衡抉择的过程，k越大，m越小，空间复杂度升高；k越小，m越大，时间复杂度升高。下图是k=4时索引示意图：<br><img src="http://dl.iteye.com/upload/attachment/437586/b72b8dc2-9139-3078-ad24-b689f64fd71a.jpg" alt=""></p>
<h2 id="MinHash"><a href="#MinHash" class="headerlink" title="MinHash"></a>MinHash</h2><p>与SimHash不同，MinHash要从衡量两个文档的相似度说起。Jaccard相似度用于描述两个集合的相似程度，假设有两个集合A和B，相似度=A与B交集的元素个数／A与B并集的元素个数，公式为：<br><span>$$\begin{gather*}
J(A, B)=\frac{|A \cap B|}{|A \cup B|}
\end{gather*}$$</span><!-- Has MathJax --><br>海量文本直接求Jaccard相似度复杂度太高，两个文档需要逐个词比较，为降低复杂度，我们使用两个文档的最小哈希值相等的概率来等价于两个文档的Jaccard相似度，并可以证明两者是相等的。首先说明一下如何求一个集合的最小哈希值，假设现在有4个集合，分别为S1，S2，S3，S4；其中，S1={a,d}, S2={c}, S3={b,d,e}, S4={a,c,d}，所以全集U={a,b,c,d,e}。我们可以构造如下0-1矩阵：</p>
<table>
<thead>
<tr>
<th>元素</th>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>S4</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>b</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>c</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>d</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>e</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>为得到各集合的最小哈希值，首先对矩阵进行随机行打乱，则某个集合（某一列）的最小哈希值就等于打乱后第一个值为1的行所在的行号。定义一个最小哈希函数h，用于模拟对矩阵进行随机打乱，假设打乱后的矩阵如下表所示：</p>
<table>
<thead>
<tr>
<th>元素</th>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>S4</th>
</tr>
</thead>
<tbody>
<tr>
<td>b</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>e</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>a</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>d</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>c</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>则h(S1)=2, h(S2)=4, h(S3)=0, h(S4)=2。经过随机打乱后，两个集合的最小哈希值相等的概率=两集合的Jaccard的相似度证明如下：</p>
<blockquote>
<p>仅考虑集合S1和S2，那么这两列所在的行有以下三种情况：</p>
<ol>
<li>S1和S2的值都为1，用X表示；</li>
<li>一个值为1，另一个为0，用Y表示；</li>
<li>S1和S2的值都为0，用Z表示。</li>
</ol>
</blockquote>
<p>S1与S2的交集元素个数为X，并集个数为X+Y，所以sim(S1,S2)=Jaccard(S1,S2)=X/(X+Y)。随机打乱后h(S1)=h(S2)的概率等于从上往下扫描，在遇到Y行之前遇到X行的概率（Z行没有影响），或者说把X个黑球和Y个白球放入一个袋子中，首次拿到黑球的概率，即h(S1)=h(S2)的概率为X/(X+Y)。假设特征矩阵很大时，对其进行打乱非常耗时，而且要进行多次打乱，其实可以通过多个随机哈希函数来模拟打乱的效果。具体地，定义n个随机哈希函数$h_1, h_2,…,h_n$，sig(i,j)表示签名矩阵中第i个哈希函数在第j列上的元素，将签名矩阵中各个元素sig(i,j)初始化为inf(无穷大)，对原矩阵中每一行r：</p>
<ol>
<li>计算<span>$h_1(r),h_2(r),...,h_n(r)$</span><!-- Has MathJax -->；</li>
<li>对于每一列j：<ul>
<li>如果j所在的第r行为0，什么都不做；</li>
<li>如果j所在的第r行为1，则对每个i=1,2,…,n，<span>$sig(i,j)=min(sig(i,j), h_i(r))$</span><!-- Has MathJax --></li>
</ul>
</li>
</ol>
<p>例如<span>$h_1(x)=(x+1)%5, h_2(x)=(3*x+1)%5$</span><!-- Has MathJax -->，则经过上述操作可获得如下的签名矩阵：</p>
<table>
<thead>
<tr>
<th>哈希函数</th>
<th>S1</th>
<th>S2</th>
<th>S3</th>
<th>S4</th>
</tr>
</thead>
<tbody>
<tr>
<td>$h_1$</td>
<td>1</td>
<td>3</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>$h_2$</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>上述获得各文档签名矩阵可以理解为一个降维过程，获得签名后，假如数据量十分庞大的话，两两比较的话计算复杂度仍然非常高，所以需要类似SimHash索引分块的方法来降低查询复杂度。当一个新文档到达时，希望仅比较与其相似性较高的文档，忽略相似性较低的集合。如下图所示，假设签名矩阵有12行，我们将3行为一组放进一个“桶”里：<br><img width="500" src="/images/bucket.png"><br>对于S2，仅需要查询与其具有相同桶的集合，如下图所示：<br><img width="500" src="/images/bucket5.png"><br>即只需要查询S4和S5。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.8001&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.8001&amp;rep=rep1&amp;type=pdf</a><br><a href="https://blog.csdn.net/heiyeshuwu/article/details/44117473" target="_blank" rel="external">https://blog.csdn.net/heiyeshuwu/article/details/44117473</a><br><a href="https://www.cnblogs.com/sddai/p/6110704.html" target="_blank" rel="external">https://www.cnblogs.com/sddai/p/6110704.html</a></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/14/TextRank算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/14/TextRank算法/" itemprop="url">TextRank算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-14T15:25:17+08:00">
                2019-05-14
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>TextRank算法可以用于提取文本关键词和生成摘要，其思想来源于PageRank算法。Google两位创始人，在斯坦福大学读研期间从事网页排序研究时，受到学术界对学术论文重要性的评估方法（论文引用次数），提出了PageRank算法。PageRank算法的核心思想比较直观：</p>
<ol>
<li>如果一个网页被很多其他网页链接到，说明这个网页很重要，对应的PR(PageRank)值也越高；</li>
<li>如果一个PR值较高的网页链接了某个网页，则该网页的PR值也会相应提高。</li>
</ol>
<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h3><p>将网页之间的链接抽象为一张有向图，如下图所示：<br><img width="500" src="/images/dgraph.png"></p>
<p><center><strong><em><a href="https://www.cnblogs.com/mcomco/p/10304383.html" target="_blank" rel="external">图片来源</a></em></strong></center><br>图结构构造完成后，可使用以下公式计算网页的PR值：<br><span>$$\begin{gather*}
S\left(V_{i}\right)=(1-d)+d * \sum_{j \in I n\left(V_{i}\right)} \frac{1}{\left|O u t\left(V_{j}\right)\right|} S\left(V_{j}\right)
\end{gather*}$$</span><!-- Has MathJax --><br><span>$S\left(V_{i}\right)$</span><!-- Has MathJax -->表示网页i的PR值，即网页的重要性指标。d是阻尼系数，一般取0.85。<span>$I n\left(V_{i}\right)$</span><!-- Has MathJax -->表示指向网页i的网页集合。<span>$|O u t\left(V_{j}\right)|$</span><!-- Has MathJax -->表示网页j指向的网页总数，<span>$S\left(V_{j}\right)$</span><!-- Has MathJax -->表示网页j的PR值。可将各网页PR值设置为1，经过多次迭代，满足收敛条件后获得各个网页的PR值。</p>
<h3 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank"></a>TextRank</h3><h4 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h4><p>以下面的文章为例，首先进行过滤停用词等预处理（中文需要分词），然后建立如图所示单词之间的连接图，此时PageRank算法中网页之间的链接关系体现为一定窗口大小内单词之间的相邻关系，例如以“systems”为中心，窗口大小为3时，”types”, “linear”和“compatibility”与其具有“链接关系“。<br><img src="/images/keywords.png" alt=""></p>
<p><center><strong><em><a href="https://upload-images.jianshu.io/upload_images/3579920-09c220dd15b3f13b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/996" target="_blank" rel="external">图片来源</a></em></strong></center><br>图构造完成后，单词的TR值计算公式为：<br><span>$$\begin{gather*}
W S\left(V_{i}\right)=(1-d)+d * \sum_{V_{j} \in I n\left(V_{i}\right)} \frac{w_{j i}}{\sum_{V_{k} \in O u t\left(V_{j}\right)} w_{j k}} W S\left(V_{j}\right)
\end{gather*}$$</span><!-- Has MathJax --><br>TR值计算公式与PR值十分类似，区别在于加入了一个参数<span>$w_{j i}$</span><!-- Has MathJax -->，一般来说<span>$w_{j i}$</span><!-- Has MathJax -->的值为文章中第j个单词和第i个单词在一定窗口大小的共现次数。后续的迭代过程与PR值计算类似。</p>
<h4 id="摘要生成"><a href="#摘要生成" class="headerlink" title="摘要生成"></a>摘要生成</h4><p>将文本中的每个句子看作图中的一个节点，句子之间的“链接关系”由句子间的相似性体现。句子相似性有多种计算方式，这里使用一种很简单的方法，计算两个句子共有词比例。句子相似性公式：<br><span>$$\begin{gather*}
\text {Similarity}\left(S_{i}, S_{j}\right)=\frac{\left|\left\{w_{k} | w_{k} \in S_{i} \&amp; w_{k} \in S_{j}\right\}\right|}{\log \left(\left|S_{i}\right|\right)+\log \left(\left|S_{j}\right|\right)}
\end{gather*}$$</span><!-- Has MathJax --><br><span>$S_{i}, S_{j}$</span><!-- Has MathJax -->分别表示第i个和第j个句子，$w_{k}$表示句子中的词语，公式中分子表示两个句子共有词的个数，分母表示两个句子词总数对数求和。后续TR值计算和迭代过程与关键词提取类似。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote>
<p><a href="http://www.cnblogs.com/xueyinzhe/p/7101295.html" target="_blank" rel="external">http://www.cnblogs.com/xueyinzhe/p/7101295.html</a><br><a href="https://blog.csdn.net/woshiliulei0/article/details/81479434" target="_blank" rel="external">https://blog.csdn.net/woshiliulei0/article/details/81479434</a></p>
</blockquote>
<p><strong><em>本文中图片均来自互联网</em></strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/07/使用TensorFlow-Serving快速部署模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/07/使用TensorFlow-Serving快速部署模型/" itemprop="url">使用TensorFlow Serving快速部署模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-07T11:41:42+08:00">
                2019-05-07
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="工业产品中TensorFlow使用方法"><a href="#工业产品中TensorFlow使用方法" class="headerlink" title="工业产品中TensorFlow使用方法"></a>工业产品中TensorFlow使用方法</h1><ol>
<li>用TensorFlow的C++/Java/Nodejs API直接使用保存的TensorFlow模型：类似Caffe，适合做桌面软件。</li>
<li>直接将使用TensorFlow的Python代码放到Flask等Web程序中，提供Restful接口：实现和调试方便，但效率不太高，不大适合高负荷场景，且没有版本管理、模型热更新等功能。</li>
<li>将TensorFlow模型托管到TensorFlow Serving中，提供RPC或Restful服务：实现方便，高效，自带版本管理、模型热更新等，很适合大规模线上业务。</li>
</ol>
<blockquote>
<p>参考链接：<a href="https://cloud.tencent.com/developer/article/1375668" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1375668</a> </p>
</blockquote>
<h1 id="TensorFlow-Serving简介"><a href="#TensorFlow-Serving简介" class="headerlink" title="TensorFlow Serving简介"></a>TensorFlow Serving简介</h1><p><a href="https://github.com/tensorflow/serving" target="_blank" rel="external">Tensorflow Serving</a>是Google官方提供的模型部署方式，正确导出模型后，可一分钟完成部署（官方广告）。TF1.8后，Tensorflow Serving支持RESTfull API和grpc的请求方式，模型部署完成后可很方便的利用post请求进行测试。</p>
<h1 id="TensorFlow-Serving服务框架"><a href="#TensorFlow-Serving服务框架" class="headerlink" title="TensorFlow Serving服务框架"></a>TensorFlow Serving服务框架</h1><p>框架分为模型训练、模型上线和服务使用三部分。模型训练与正常的训练过程一致，只是导出时需要按照TF Serving的标准定义输入、输出和签名。模型上线时指定端口号和模型路径后，通过tensorflow_model_server命令启动服务。服务使用可通过grpc和RESTfull方式请求。<br><img width="724" src="https://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_jpg/rFWVXwibLGtzxrqiba6BicbqCjDDQ313ohCZJQ5u0LTnK5okv89ibHbf2pI6YWMq05UNjjoiaxxibxd6pqk6l07T04rA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1"></p>
<h1 id="模型导出"><a href="#模型导出" class="headerlink" title="模型导出"></a>模型导出</h1><p>需指定模型的输入和输出，并在tags中包含”serve”，在实际使用中，TF Serving要求导出模型包含”serve”这个tag。此外，还需要指定默认签名，tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY = “serving_default”，此外tf.saved_model.signature_constants定义了三类签名，分别是：</p>
<ul>
<li>分类classify</li>
<li>回归regress</li>
<li>预测predict</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">CLASSIFY_METHOD_NAME = &quot;tensorflow/serving/classify&quot;</div><div class="line">PREDICT_METHOD_NAME = &quot;tensorflow/serving/predict&quot;</div><div class="line">REGRESS_METHOD_NAME = &quot;tensorflow/serving/regress&quot;</div></pre></td></tr></table></figure>
<p>一般而言，用predict就完事了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">with sess.graph.as_default() as graph:</div><div class="line">    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)</div><div class="line">    signature = tf.saved_model.signature_def_utils.predict_signature_def(inputs=&#123;&apos;image&apos;: in_image&#125;,</div><div class="line">                                      outputs=&#123;&apos;prediction&apos;: graph.get_tensor_by_name(&apos;final_result:0&apos;)&#125;,)</div><div class="line">    builder.add_meta_graph_and_variables(sess=sess,</div><div class="line">                                         tags=[&quot;serve&quot;],</div><div class="line">                                         signature_def_map=&#123;&apos;predict&apos;:signature, tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:signature&#125;)</div><div class="line">    builder.save()</div></pre></td></tr></table></figure></p>
<h1 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=模型名 --model_base_path=模型所在路径</div></pre></td></tr></table></figure>
<h1 id="请求服务"><a href="#请求服务" class="headerlink" title="请求服务"></a>请求服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -d &apos;&#123;&quot;inputs&quot;: [[1.1,1.2,0.8,1.3]]&#125;&apos; -X POST http://localhost:8501/v1/models/模型名:predict</div></pre></td></tr></table></figure>
<p>python可以通过post请求，golang可以通过grpc服务请求。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/07/基于Tensorflow-Hub进行迁移学习完成人脸BMI指数预测/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/07/基于Tensorflow-Hub进行迁移学习完成人脸BMI指数预测/" itemprop="url">基于Tensorflow Hub进行迁移学习完成人脸BMI指数预测</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-07T11:23:00+08:00">
                2019-05-07
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Tensorflow-Hub"><a href="#Tensorflow-Hub" class="headerlink" title="Tensorflow Hub"></a>Tensorflow Hub</h1><p>TF Hub是一个通过复用Tensorflow models来完成迁移学习的模型库，目前有自然语言、图像和视频三大类，具体可参考下面链接（部分页面需要翻墙，你懂得）：</p>
<blockquote>
<p><a href="https://www.tensorflow.org/hub" target="_blank" rel="external">https://www.tensorflow.org/hub</a></p>
</blockquote>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p>首先对人物图片进行人脸识别，然后利用tfhub中inception v3模型提取feature vector，最后使用SVR模型完成基于人脸的BMI指数预测。<br><img src="/images/face2bmimodel.png" alt=""></p>
<blockquote>
<p>参考论文链接：<a href="https://arxiv.org/abs/1703.03156" target="_blank" rel="external">https://arxiv.org/abs/1703.03156</a></p>
</blockquote>
<h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p>这里介绍golang版本解决方案，python的资源丰富，例如<a href="https://github.com/ageitgey/face_recognition" target="_blank" rel="external">face_recognition</a>等。<a href="https://github.com/Kagami/go-face" target="_blank" rel="external">go-face</a>提供了纯go版本的人脸识别功能，不需要安装opencv等复杂的环境依赖，相关的依赖也可以通过apt-get方式快速安装，值得注意的是其需要人脸识别的模型文件shape_predictor和dlib_face_recognition，具体介绍可以参考其github主页。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">//主要代码</div><div class="line">const dataDir = &quot;testdata&quot;</div><div class="line"></div><div class="line">func main() &#123;</div><div class="line">	// Init the recognizer.</div><div class="line">	rec, err := face.NewRecognizer(dataDir)</div><div class="line">	if err != nil &#123;</div><div class="line">		log.Fatalf(&quot;Can&apos;t init face recognizer: %v&quot;, err)</div><div class="line">	&#125;</div><div class="line">	// Free the resources when you&apos;re finished.</div><div class="line">	defer rec.Close()</div><div class="line"></div><div class="line">	// Test</div><div class="line">	testImage := filepath.Join(dataDir, &quot;face.jpg&quot;)</div><div class="line">	// Recognize faces on that image.</div><div class="line">	faces, err := rec.RecognizeFile(testImagePristin)</div><div class="line">	if err != nil &#123;</div><div class="line">		log.Fatalf(&quot;Can&apos;t recognize: %v&quot;, err)</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="tfhub"><a href="#tfhub" class="headerlink" title="tfhub"></a>tfhub</h2><p>这里使用google发布的inception_v3模型，由于网络原因，如果在代码中无法下载可以选择手动下载并指定路径，下载时url为：</p>
<blockquote>
<p><a href="https://storage.googleapis.com/tfhub-modules/google/imagenet/inception_v3/feature_vector/1.tar.gz" target="_blank" rel="external">https://storage.googleapis.com/tfhub-modules/google/imagenet/inception_v3/feature_vector/1.tar.gz</a></p>
</blockquote>
<p>模型下载完成并指定路径后可直接在hub中使用并获得输入图片的feature vector。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">height, width = hub.get_expected_image_size(module_spec)</div><div class="line">resized_input_tensor = tf.placeholder(tf.float32, [None, height, width, 3], name=&quot;input_tensor&quot;)</div><div class="line">m = hub.Module(module_spec)</div><div class="line">bottleneck_tensor = m(resized_input_tensor)</div></pre></td></tr></table></figure></p>
<h2 id="SVR模型"><a href="#SVR模型" class="headerlink" title="SVR模型"></a>SVR模型</h2><p>对于一般的回归问题，给定训练样本，模型希望学习得到一个f(x)与y尽可能的接近，只有f(x)和y完全相同时，损失才为零，而支持向量回归可以容忍f(x)与y之前最多有ε的偏差，当且仅当f(x)与y的差别绝对值大于ε时，才计算损失。此时相当于以f(x)为中心，构建一个宽度为2ε的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。如下图所示：<br><img src="/images/svr.png" alt="图片"><br>参考链接：</p>
<blockquote>
<p><a href="https://blog.csdn.net/zb123455445/article/details/78354489" target="_blank" rel="external">https://blog.csdn.net/zb123455445/article/details/78354489</a></p>
</blockquote>
<p>Tensorflow中实现SVR模型首先设置和初始化W, b和ε，通过W*x+b获得final_tensor，最后计算loss，公式为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.name_scope(&apos;loss&apos;):</div><div class="line">    loss = tf.reduce_mean(tf.maximum(0., tf.subtract(tf.abs(tf.subtract(final_tensor, ground_truth_input)), epsilon)))</div></pre></td></tr></table></figure></p>
<h1 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h1><p>训练代码参考了tensorflow提供的鲜花分类的retrain.py代码，主要对loss函数，数据处理和模型导出做了修改。</p>
<blockquote>
<p>参考连接：<a href="https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py" target="_blank" rel="external">https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py</a></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/Language-Model-based-on-BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/Language-Model-based-on-BERT/" itemprop="url">Language Model based on BERT</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-25T19:22:22+08:00">
                2019-03-25
              </time>
            

            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>2018年10月谷歌AI团队发布BERT模型，在11种NLP任务测试中刷新了最佳成绩，一时风头无两。自然语言处理领域近两年最受关注，并且进展迅速的当属机器阅读理解，其中斯坦福大学于2016年提出的SQuAD数据集对于推动Machine Comprehension的发展起到了巨大的作用。SQuAD 1.0发布时，Google一直没有出手，微软曾长期占据榜首位置，阿里巴巴也曾短暂登顶。2018年1月3日微软亚洲研究院提交的R-NET模型在EM值（Exact Match表示预测答案和真实答案完全匹配）上以82.650的最高分领先，并率先超越人类分数82.304。而当谷歌一出手，便知有没有，目前SQuAD排行榜上已经被BERT霸屏，排行前列的模型几乎全部基于BERT。关于通用语言模型的介绍，可以参考另一篇翻译的博客，以及张俊林老师的介绍，参考链接附在本文末尾。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>谷歌已开放源码：</p>
<blockquote>
<p><a href="https://github.com/google-research/bert" target="_blank" rel="external">https://github.com/google-research/bert</a></p>
</blockquote>
<p>其中create_pretraining_data.py用于创建训练数据，run_pretraining.py用于进行预训练。此外，谷歌还提供了二阶段fine tunning的训练代码，run_classifier.py用于句子分类任务，run_squad.py用于机器阅读理解任务，可直接使用。而基于BERT的语言模型可直接对预训练模型进行改造后获得，参考链接：</p>
<blockquote>
<p><a href="https://github.com/xu-song/bert-as-language-model" target="_blank" rel="external">https://github.com/xu-song/bert-as-language-model</a></p>
</blockquote>
<p>作者主要对get_masked_lm_output函数进行了改造，具体地，计算masked lm loss时不使用masked_lm_weights，参考代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">#原代码</div><div class="line">def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,</div><div class="line">                         label_ids, label_weights):</div><div class="line">  &quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</div><div class="line">  input_tensor = gather_indexes(input_tensor, positions)</div><div class="line"></div><div class="line">  with tf.variable_scope(&quot;cls/predictions&quot;):</div><div class="line">    # We apply one more non-linear transformation before the output layer.</div><div class="line">    # This matrix is not used after pre-training.</div><div class="line">    with tf.variable_scope(&quot;transform&quot;):</div><div class="line">      input_tensor = tf.layers.dense(</div><div class="line">          input_tensor,</div><div class="line">          units=bert_config.hidden_size,</div><div class="line">          activation=modeling.get_activation(bert_config.hidden_act),</div><div class="line">          kernel_initializer=modeling.create_initializer(</div><div class="line">              bert_config.initializer_range))</div><div class="line">      input_tensor = modeling.layer_norm(input_tensor)</div><div class="line"></div><div class="line">    # The output weights are the same as the input embeddings, but there is</div><div class="line">    # an output-only bias for each token.</div><div class="line">    output_bias = tf.get_variable(</div><div class="line">        &quot;output_bias&quot;,</div><div class="line">        shape=[bert_config.vocab_size],</div><div class="line">        initializer=tf.zeros_initializer())</div><div class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</div><div class="line">    logits = tf.nn.bias_add(logits, output_bias)</div><div class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</div><div class="line"></div><div class="line">    label_ids = tf.reshape(label_ids, [-1])</div><div class="line">    label_weights = tf.reshape(label_weights, [-1])</div><div class="line"></div><div class="line">    one_hot_labels = tf.one_hot(</div><div class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</div><div class="line"></div><div class="line">    # The `positions` tensor might be zero-padded (if the sequence is too</div><div class="line">    # short to have the maximum number of predictions). The `label_weights`</div><div class="line">    # tensor has a value of 1.0 for every real prediction and 0.0 for the</div><div class="line">    # padding predictions.</div><div class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])</div><div class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</div><div class="line">    denominator = tf.reduce_sum(label_weights) + 1e-5</div><div class="line">    loss = numerator / denominator</div><div class="line"></div><div class="line">  return (loss, per_example_loss, log_probs)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">#改造后代码</div><div class="line">def get_masked_lm_output(bert_config, input_tensor, output_weights, positions,</div><div class="line">                         label_ids):</div><div class="line">  &quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</div><div class="line">  input_tensor = gather_indexes(input_tensor, positions)</div><div class="line"></div><div class="line">  with tf.variable_scope(&quot;cls/predictions&quot;):</div><div class="line">    # We apply one more non-linear transformation before the output layer.</div><div class="line">    # This matrix is not used after pre-training.</div><div class="line">    with tf.variable_scope(&quot;transform&quot;):</div><div class="line">      input_tensor = tf.layers.dense(</div><div class="line">          input_tensor,</div><div class="line">          units=bert_config.hidden_size,</div><div class="line">          activation=modeling.get_activation(bert_config.hidden_act),</div><div class="line">          kernel_initializer=modeling.create_initializer(</div><div class="line">              bert_config.initializer_range))</div><div class="line">      input_tensor = modeling.layer_norm(input_tensor)</div><div class="line"></div><div class="line">    # The output weights are the same as the input embeddings, but there is</div><div class="line">    # an output-only bias for each token.</div><div class="line">    output_bias = tf.get_variable(</div><div class="line">        &quot;output_bias&quot;,</div><div class="line">        shape=[bert_config.vocab_size],</div><div class="line">        initializer=tf.zeros_initializer())</div><div class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</div><div class="line">    logits = tf.nn.bias_add(logits, output_bias)</div><div class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</div><div class="line"></div><div class="line">    label_ids = tf.reshape(label_ids, [-1])</div><div class="line"></div><div class="line">    one_hot_labels = tf.one_hot(</div><div class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</div><div class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])</div><div class="line">    loss = tf.reshape(per_example_loss, [-1, tf.shape(positions)[1]])</div><div class="line">    # TODO: dynamic gather from per_example_loss</div><div class="line">  return loss</div></pre></td></tr></table></figure>
<p>Python中可直接构造输入，然后利用Tensorflow高级API来获得结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">result = estimator.predict(input_fn=predict_input_fn)</div></pre></td></tr></table></figure></p>
<p>estimator.predict的预测结果在model_fn_builder中指定：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">if mode == tf.estimator.ModeKeys.PREDICT:</div><div class="line">    output_spec = tf.contrib.tpu.TPUEstimatorSpec(</div><div class="line">    mode=mode, predictions=masked_lm_example_loss, scaffold_fn=scaffold_fn)  # 输出mask_word的score</div></pre></td></tr></table></figure></p>
<p>BERT作为语言模型时，一个不便之处是需要逐个计算每个token的prob，然后计算句子的ppl。</p>
<blockquote>
<p>ppl: 自然语言处理领域（NLP）中，衡量语言模型好坏的指标。根据每个词来估计一句话出现的概率，并用句子长度作normalize，ppl值越小，表示该句子越合理。</p>
</blockquote>
<p>结果解析，ppl计算代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">def parse_result(result, all_tokens, output_file=None):</div><div class="line">  with tf.gfile.GFile(output_file, &quot;w&quot;) as writer:</div><div class="line">    tf.logging.info(&quot;***** Predict results *****&quot;)</div><div class="line">    i = 0</div><div class="line">    sentences = []</div><div class="line">    for word_loss in result:</div><div class="line">      # start of a sentence</div><div class="line">      if all_tokens[i] == &quot;[CLS]&quot;:</div><div class="line">        sentence = &#123;&#125;</div><div class="line">        tokens = []</div><div class="line">        sentence_loss = 0.0</div><div class="line">        word_count_per_sent = 0</div><div class="line">        i += 1</div><div class="line"></div><div class="line">      # add token</div><div class="line">      tokens.append(&#123;&quot;token&quot;: tokenization.printable_text(all_tokens[i]),</div><div class="line">                     &quot;prob&quot;: float(np.exp(-word_loss[0])) &#125;)</div><div class="line">      sentence_loss += word_loss[0]</div><div class="line">      word_count_per_sent += 1</div><div class="line">      i += 1</div><div class="line"></div><div class="line">      token_count_per_word = 0</div><div class="line">      while is_subtoken(all_tokens[i]):</div><div class="line">        token_count_per_word += 1</div><div class="line">        tokens.append(&#123;&quot;token&quot;: tokenization.printable_text(all_tokens[i]),</div><div class="line">                       &quot;prob&quot;: float(np.exp(-word_loss[token_count_per_word]))&#125;)</div><div class="line">        sentence_loss += word_loss[token_count_per_word]</div><div class="line">        i += 1</div><div class="line"></div><div class="line">      # end of a sentence</div><div class="line">      if all_tokens[i] == &quot;[SEP]&quot;:</div><div class="line">        sentence[&quot;tokens&quot;] = tokens</div><div class="line">        sentence[&quot;ppl&quot;] = float(np.exp(sentence_loss / word_count_per_sent))</div><div class="line">        sentences.append(sentence)</div><div class="line">        i += 1</div><div class="line"></div><div class="line">    if output_file is not None:</div><div class="line">      tf.logging.info(&quot;Saving results to %s&quot; % output_file)</div><div class="line">      writer.write(json.dumps(sentences, indent=2, ensure_ascii=False))</div></pre></td></tr></table></figure></p>
<h2 id="模型训练、导出和部署"><a href="#模型训练、导出和部署" class="headerlink" title="模型训练、导出和部署"></a>模型训练、导出和部署</h2><p>由于预训练模型中masked lm loss节点并未命名，所以添加name后需要启动很短暂的预训练，同时将模型导出。get_masked_lm_output函数参考bert-as-language-model中的代码进行相应改造。Tensorflow版本升级后，使用estimator接受输入，原来我们最爱的placeholder找不到了，而在部署模型时，仍需要使用placeholder接受输入，可在run_pretraining.py导出模型时添加如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">if FLAGS.do_export:</div><div class="line">    estimator._export_to_tpu = False</div><div class="line">    name_to_features = [(&quot;input_ids&quot;, tf.int32), (&quot;input_mask&quot;, tf.int32),</div><div class="line">                            (&quot;segment_ids&quot;, tf.int32), (&quot;masked_lm_positions&quot;, tf.int32), (&quot;masked_lm_ids&quot;, tf.int32),</div><div class="line">                            (&quot;masked_lm_weights&quot;, tf.float32), (&quot;next_sentence_labels&quot;, tf.int32)]</div><div class="line">    feature_placeholders = &#123;name: tf.placeholder(dtype, [1, FLAGS.max_seq_length],</div><div class="line">                                                     name=&apos;bert/&apos; + name + &quot;_placeholder&quot;) for name, dtype in</div><div class="line">                                name_to_features&#125;</div><div class="line">    serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)</div><div class="line">    path = estimator.export_savedmodel(&quot;./export/&quot;, serving_input_fn)</div></pre></td></tr></table></figure></p>
<p>本文部署模型使用golang语言，基于tfgo实现模型的加载和tensorflow对应节点的计算。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">//模型加载</div><div class="line">model := tg.LoadModel(*modleDir, []string&#123;&quot;serve&quot;&#125;, nil)</div></pre></td></tr></table></figure></p>
<p>参考run_pretraining.py导出模型时的代码，golang程序中需要构造7个输入，而masked_lm_weights和next_sentence_labels对于语言模型没有影响，可按自己喜爱构造。以下面的例子说明一下输入的构造标准：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">//输入句子</div><div class="line">何异浮云过太空</div><div class="line">//处理后的句子</div><div class="line">[[CLS] 何 异 浮 云 过 太 空 [SEP]]</div><div class="line">------------------------token 1-----------------------------</div><div class="line">//input_ids</div><div class="line">[101 103 2460 3859 756 6814 1922 4958 102 0 0 0 ...]</div><div class="line">//input_mask</div><div class="line">[1 1 1 1 1 1 1 1 1 0 0 0 ...]</div><div class="line">//segment_ids</div><div class="line">[0 0 0 ...]</div><div class="line">//masked_lm_positions</div><div class="line">[1 0 0 0 ...]</div><div class="line">//masked_lm_ids</div><div class="line">[862 0 0 0 0 0 ...]</div><div class="line">------------------------token 2----------------------------</div><div class="line">[101 862 103 3859 756 6814 1922 4958 102 0 0 0 ...]</div><div class="line">[1 1 1 1 1 1 1 1 1 0 0 0 ...]</div><div class="line">[0 0 0 ...]</div><div class="line">[2 0 0 0 ...]</div><div class="line">[2460 0 0 0 ...]</div></pre></td></tr></table></figure></p>
<p>Golang程序计算句子ppl：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">for i := 1; i &lt; ids_len - 1; i++&#123;</div><div class="line">	...</div><div class="line">	result := model.Exec([]tf.Output&#123;</div><div class="line">				model.Op(&quot;cls/predictions/lm_loss&quot;, 0),</div><div class="line">			&#125;, map[tf.Output]*tf.Tensor&#123;</div><div class="line">				model.Op(&quot;bert/input_ids_placeholder&quot;, 0):           inputX1,</div><div class="line">				model.Op(&quot;bert/input_mask_placeholder&quot;, 0):           inputX2,</div><div class="line">				model.Op(&quot;bert/segment_ids_placeholder&quot;, 0):           inputX3,</div><div class="line">				model.Op(&quot;bert/masked_lm_positions_placeholder&quot;, 0):           inputX4,</div><div class="line">				model.Op(&quot;bert/masked_lm_ids_placeholder&quot;, 0):           inputX5,</div><div class="line">				model.Op(&quot;bert/masked_lm_weights_placeholder&quot;, 0):           inputX6,</div><div class="line">				model.Op(&quot;bert/next_sentence_labels_placeholder&quot;, 0):           inputX7,</div><div class="line">			&#125;)</div><div class="line">	</div><div class="line">	val := result[0].Value().([][]float32)[0][0]</div><div class="line">	sentence_loss += float64(val)</div><div class="line">	...</div><div class="line">&#125;</div><div class="line"></div><div class="line">ppl := math.Pow(math.E, sentence_loss / float64(ids_len))</div></pre></td></tr></table></figure></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="external">从Word Embedding到Bert模型</a><br><a href="https://zhuanlan.zhihu.com/p/56865533" target="_blank" rel="external">效果惊人的GPT 2.0模型</a><br><a href="http://octopuscoder.github.io/2019/03/11/通用语言模型/" target="_blank" rel="external">通用语言模型</a></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/avatar.gif"
              alt="Jeb" />
          
            <p class="site-author-name" itemprop="name">Jeb</p>
            <p class="site-description motion-element" itemprop="description">我菜故我在</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">24</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeb</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
