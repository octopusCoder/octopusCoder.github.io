<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="我菜故我在">
<meta property="og:type" content="website">
<meta property="og:title" content="小菜鸡">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="小菜鸡">
<meta property="og:description" content="我菜故我在">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小菜鸡">
<meta name="twitter:description" content="我菜故我在">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>小菜鸡</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-106410509-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小菜鸡</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/04/问答系统模型总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/04/问答系统模型总结/" itemprop="url">问答系统模型总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-04T15:52:55+08:00">
                2017-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>机器问答是自然语言处理领域的核心任务，一个典型的开放式问答系统分为三个部分：(1) 问题分析和候选篇章检索；(2) 候选篇章排序；（3）答案选择。本文主要关注答案选择部分，传统的模型大多基于特征工程实现，使用词法、句法和语法等特征，需要额外的资源，一旦外部工具出错，模型性能会受到影响，并且外部资源的获得也需要付出一些成本。深度学习模型可以自动学习这些特征，通过将句子映射到一个向量空间，然后在隐藏空间对问题和候选答案匹配完成答案选择，在众多数据集的测试结果均优于传统模型。本文从基本模型Siamense Network开始、逐个介绍了模型的改进版本Attention Network和Compare-Aggregate Network，随后对匹配函数选择和如何利用句子不相似部分两个点进行了说明。此外为方便对各模型进行比较，文章最后收集了各模型在WikiQA数据集上的实验结果并进行了展示。 </p>
<blockquote>
<p>注：本文所使用的图片均来自对应论文。</p>
</blockquote>
<h2 id="基本模型Siamense-Network"><a href="#基本模型Siamense-Network" class="headerlink" title="基本模型Siamense Network"></a>基本模型Siamense Network</h2><p>Siamense网络[1]的特点是包含两路结构非常相似的网络，网络之间的参数共享，在最后进行连接，适用于计算两路输入信息的相似性。</p>
<h3 id="Attentive-pooling-networks"><a href="#Attentive-pooling-networks" class="headerlink" title="Attentive pooling networks"></a>Attentive pooling networks</h3><p>dos Santos在根据Query选择Candidate answer一文[2]中使用了Siamense网络作为Baseline。模型如下图所示：<br><img src="/images/apnmodel.png" alt="图1"></p>
<p>Question和Candidate answer中的每个单词首先经过word embedding处理后获得Question和Candidate answer对应的表示矩阵，然后分别利用CNN或者BiLSTM处理后获得对应的特征矩阵，在特征矩阵上应用Column-wise max pooling后获得Question和Candidate answer对应的向量$r_q$和$r_a$，最后计算两个向量之间的余弦相似度。模型的损失函数采用Hinge loss函数，即:<span>$L=max\{0,m-s_\theta(q,a^+)+s_\theta(q,a^-)\}$</span><!-- Has MathJax --></p>
<h3 id="Multi-Perspective-Sentence-Similarity-Modeling-with-Convolutional-Neural-Networks"><a href="#Multi-Perspective-Sentence-Similarity-Modeling-with-Convolutional-Neural-Networks" class="headerlink" title="Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks"></a>Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks</h3><p>Hua He[3]于2015年提出的用于句子相似度计算的MPCNN模型仍然遵循Siamense network，模型使用CNN网络提取多粒度特征，配合多种pooling策略，在相似性计算层采用了多种计算方法，在以CNN构建的Siamense network模型上进行了创新。模型结构如下图所示：<br><img src="/images/MPSSM1.png" alt="图3"></p>
<p>模型使用了两种卷积核，如下图所示：<br><img src="/images/MMSSMfliter.png" alt="图4"><br>上图左边的卷积核即常用的卷积核，文中称为holistic filters。右边的卷积核限制了卷积的维度，对输入word embeddings各维度进行处理，文中称为per-dimension filters。<br>模型使用了三种pooling类型，分别为max-pooling, min-pooling和mean-pooling，并与上述两种filters进行组合，如下图所示：<br><img src="/images/MPSMblock.png" alt="图5"></p>
<p>模型使用了大小不同的卷积窗口，并与上述两种filters和pooling类型进行组合，如下图所示：<br><img src="/images/MSBMMwin.png" alt="图6"></p>
<p>相似性计算层根据四个条件选择局部比较区域，分别是： 1) 是否来自同一个building block; 2）是否来自相同窗口大小的卷积层；3) 是否来自同一pooling层； 4) 是否来自卷积层中同一filter。当至少满足两个条件时进行相似性比较，相似性计算采用余弦相似度和欧式距离。该模型是基于CNN构建的Siamense network，采用了不同的卷积核，多种pooling策略等，模型比较复杂。</p>
<h3 id="LSTM-BASED-DEEP-LEARNING-MODELS-FOR-NON-FACTOID-ANSWER-SELECTION"><a href="#LSTM-BASED-DEEP-LEARNING-MODELS-FOR-NON-FACTOID-ANSWER-SELECTION" class="headerlink" title="LSTM-BASED DEEP LEARNING MODELS FOR NON FACTOID ANSWER SELECTION"></a>LSTM-BASED DEEP LEARNING MODELS FOR NON FACTOID ANSWER SELECTION</h3><p>Ming Tan[4]于2016年发表的论文主要关注答案选择任务，文中提出了四个模型，其中前两个模型QA-LSTM，QA-LSTM/CNN属于Siamense network。QA-LSTM模型较为简单，利用双向LSTM分别处理Question和Answer，然后利用mean/max pooling获得Question和Answer的向量表示，利用两者的余弦相似度预测结果。QA-LSTM模型如下图所示：<br><img src="/images/qalstm.png" alt="图7"></p>
<p>QA-LSTM/CNN模型为获得Question和Answer更多的组合表示信息，利用CNN对双向LSTM的输出进行处理，同样利用max-1 pooling产生Question和Answer对应的表示，然后利用余弦相似度预测结果，如下图所示：<br><img src="/images/qalstmcnn.png" alt="图7"></p>
<h3 id="Learning-to-rank-short-text-pairs-with-convolutional-deep-neural-networks"><a href="#Learning-to-rank-short-text-pairs-with-convolutional-deep-neural-networks" class="headerlink" title="Learning to rank short text pairs with convolutional deep neural networks"></a>Learning to rank short text pairs with convolutional deep neural networks</h3><p>Severyn A在对短文本对相似性排序一文[5]中使用了类似网络，与基本Siamense network相比略微有些变化。query和document中的单词首先通过word embedding处理后获得对应的表示矩阵，分别利用CNN网络进行处理获得各自的feature map，pooling后获得query对应的向量表示Xq和document的向量Xd。不同于传统的Siamense网络在这一步利用欧式距离或余弦距离直接对Xq和Xd进行相似性计算后预测结果，作者首先采用了一个相似矩阵来计算Xq和Xd的相似度，然后将Xd，Xq和sim(Xq,Xd)进行连接，并添加了word overlap和IDF word overlap的特征后作为特征向量输入一个神经网络层，神经网络层的输出经过一个全连接层，利用softmax函数得出预测结果。模型结构如下图所示：<br><img src="/images/QAsum1.png" alt="图8"></p>
<h2 id="改进一-Attention-Network"><a href="#改进一-Attention-Network" class="headerlink" title="改进一 Attention Network"></a>改进一 Attention Network</h2><p>Siamense network单独处理输入的句子对，忽略了句子间的语义信息，通过引入注意力机制捕获句子间关联信息，用于相似性计算。注意力机制最早应用于视觉领域，2014年Bahdanau[4]在机器翻译任务中将翻译和对齐同时进行，在自然语言处理领域引入了注意力机制。</p>
<h3 id="双向Attention-Network"><a href="#双向Attention-Network" class="headerlink" title="双向Attention Network"></a>双向Attention Network</h3><p>上文中已经提到，dos Santos[2]使用了Siamense网络作为Baseline，随后在Siamense网络的基础上引入了注意力机制，模型如下图所示：<br><img src="/images/apnmodel2.png" alt="图9"><br>模型前期的处理与基本模型-Siamense network部分相同，在获得Question和Candidate answer的特征矩阵Q和A后，计算矩阵G：<span>$G=tanh(Q^TUA)$</span><!-- Has MathJax -->。在矩阵G上应用Column-wise max pooling和Row-wise max pooling，并使用softmax函数处理后可分别获得Query和Candidate answer对应的注意力向量$\sigma ^{q}$和$\sigma ^{a}$，与Query矩阵和Answer矩阵对应相乘后获得Query向量$r_q$和Answer向量$r_a$，后续处理与基本模型Siamense network部分一致。</p>
<h4 id="Attention-Based-Convolutional-Neural-Network-for-Modeling-Sentence-Pairs"><a href="#Attention-Based-Convolutional-Neural-Network-for-Modeling-Sentence-Pairs" class="headerlink" title="Attention-Based Convolutional Neural Network for Modeling Sentence Pairs"></a>Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</h4><p>Yin W在其提出的ABCNN模型[7]中对Attention机制在CNN模型上的应用进行了更深入的探索，ABCNN模型主要解决句子对匹配问题。文中首先提出了BCNN基本模型，采用Siamense network结构，如下图所示：<br><img src="/images/BCNN.png" alt="图11"></p>
<p>随后作者提出了ABCNN-1模型，如下图所示：<br><img src="/images/ABCNN1.png" alt="图12"></p>
<p>首先计算句子$S_0$和$S_1$的attention matrix A：<span>$A_{i,j}=matchScore(F_{0,r}[:,i],F_{1,r}[:,j])$</span><!-- Has MathJax -->。$S_0$和$S_1$对应的representation feature map为$W_0$和$W_1$，将$W_0$和$W_1$与矩阵A相乘后可获得$S_0$和$S_1$对应的attention feature map。representation feature map和attention feature map输入CNN。<br>ABCNN-2模型则先利用CNN对representation feature map进行处理，如下图所示：<br><img src="/images/ABCNN2.png" alt="图14"><br>计算$S_0$和$S_1$经Convolution后的所得feature map的attention matrix A，在对Convolution feature map进行pooling处理时结合矩阵A，图中对应Attention-based average pooling部分。<br>ABCNN-1和ABCNN-2模型在不同粒度的语言单元上应用了Attention机制，ABCNN-3模型则将ABCNN-1和ABCNN-2模型进行了结合，旨在获得多粒度的特征，ABCNN-3模型如下图所示：<br><img src="/images/ABCNN3.png" alt="图15"></p>
<h4 id="Attention-Based-Multi-Perspective-Convolutional-Neural-Networks-for-Textual-Similarity-Measurement"><a href="#Attention-Based-Multi-Perspective-Convolutional-Neural-Networks-for-Textual-Similarity-Measurement" class="headerlink" title="Attention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement"></a>Attention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement</h4><p>本篇论文[8]是基本模型部分介绍的MPCNN模型的改进，作者同样是Hua He。在MPCNN模型的基础上添加了Attention层，模型如下图所示：<br><img src="/images/MPCNNattmodel.png" alt="图16"></p>
<p>设$S^{0}$和$S^{1}$分别表示两个句子，$S^{0}\left[ a\right]$表示$S^{0}$中第a个单词，$S^{1}\left[ b\right]$表示$S^{1}$中第b个单词，首先计算attention matrix D：<span>$D[a][b]=cosine(S^0[a],S^1[b])$</span><!-- Has MathJax -->。获得矩阵D后，对矩阵按行/列求和，然后利用softmax获得句子对应的注意力权重向量，计算公式如下：<br><span>$$\begin{gather*}
E^0[a]=\sum _{b}D[a][b] \\
E^1[b]=\sum _{a}D[a][b] \\
A^i=softmax(E^i) \\
\end{gather*}$$</span><!-- Has MathJax --></p>
<p>将单词原始embeddings和attention-reweighted embeddings进行连接后作为单词的表示输入Multi-perspective sentence model。文中并未使用常用的word2vec或GloVe模型，而是使用了PARAGRAM-PHRASE[9]对单词进行向量表示。</p>
<h3 id="单向Attention-Network"><a href="#单向Attention-Network" class="headerlink" title="单向Attention Network"></a>单向Attention Network</h3><h4 id="LSTM-BASED-DEEP-LEARNING-MODELS-FOR-NON-FACTOID-ANSWER-SELECTION-1"><a href="#LSTM-BASED-DEEP-LEARNING-MODELS-FOR-NON-FACTOID-ANSWER-SELECTION-1" class="headerlink" title="LSTM-BASED DEEP LEARNING MODELS FOR NON- FACTOID ANSWER SELECTION"></a>LSTM-BASED DEEP LEARNING MODELS FOR NON- FACTOID ANSWER SELECTION</h4><p>Ming Tan[4]在基本模型部分介绍的QA-LSTM和QA-LSTM/CNN的基础上引入了Attention机制。ATTENTION-BASED QA-LSTM在QA-LSTM模型基础上进行了改进，如下图所示：<br><img src="/images/qalstmatt.png" alt="图19"></p>
<p>模型根据以下公式计算Answer的Attention表示：<br><span>$$\begin{gather*}
m_{a,q}(t) = tanh(W_{am}h_a(t)+W_{qm}o_q) \\
s_{a,q}(t) \propto exp(w_{ms}^Tm_{a,q}(t)) \\
\tilde{h_{a}}\left(t\right) = h_a(t)s_{a,q}(t) \\
\end{gather*}$$</span><!-- Has MathJax --></p>
<p>其中$\tilde{h_{a}}\left(t\right)$表示经Attention处理后Answer中每个单词的表示。<br>经Attention处理后的Answer通过mean/max pooing获得Answer向量$o_a$，与Question向量$o_q$计算余弦相似度，预测结果。<br>QA-LSTM/CNN WITH ATTENTION对QA-LSTM/CNN的改进类似。</p>
<h4 id="Inner-Attention-based-Recurrent-Neural-Networks-for-Answer-Selection"><a href="#Inner-Attention-based-Recurrent-Neural-Networks-for-Answer-Selection" class="headerlink" title="Inner Attention based Recurrent Neural Networks for Answer Selection"></a>Inner Attention based Recurrent Neural Networks for Answer Selection</h4><p>Bingning Wang[10]在发表于ACL 2016一文中改进了Attention的计算方式，用于解决QA中的答案选择问题。传统的Attention based RNN模型一般在RNN处理后再添加Attention信息，该文则在计算句子的表示前添加Attention信息，作者称为“Attention before representation”。文中首先给出了传统的attention based RNN模型，与3.3节介绍的ATTENTION-BASED QA-LSTM结构非常类似，可以参考3.3节内容。随后作者提出了四个“Inner Attention based Recurrent Neural Networks”模型，简称为IARNN。<br><strong>IARNN-WORD</strong><br>模型首先计算Question向量<span>$r_q$</span><!-- Has MathJax -->与Candidate answer中各单词的注意力权重，并利用Attention结果更新单词向量，计算公式如下：<br><span>$$\begin{gather*}
\alpha_t = \sigma(r_q^TM_{qi}x_t) \\
\tilde {x}_{t} = \alpha_t * x_t \\
\end{gather*}$$</span><!-- Has MathJax --></p>
<p>$\tilde {x}_{t}$表示更新后的单词向量，<span>$r_q$</span><!-- Has MathJax -->代表Question向量。利用GRU处理<span>$\tilde {x}_{t}$</span><!-- Has MathJax -->后进行average pooling作为Answer的表示，模型如下图所以：<br><img src="/images/iarnnmodel.png" alt="图22"></p>
<p><strong>IARNN-CONTEXT</strong><br>IARNN-WORD模型只是单独的处理每个单词，无法获得多个词间的联系，即一些上下文信息。IARNN-CONTEXT模型为了解决这一问题，在计算Attention时将$h_{t-1}$作为上下文信息加入，如下图所示：<br><span>$$\begin{gather*}
w_C(t) = M_{hc}h_{t-1} + M_{qc}r_q \\
\alpha_C^t = \sigma(w_C^T(t)x_t) \\
\tilde {x}_{t} = \alpha_C^t * x_t \\
\end{gather*}$$</span><!-- Has MathJax --></p>
<p><strong>IARNN-GATE</strong><br>受到LSTM[]和利用主题信息建立单词表示[]的启发，作者将Attention应用到GRU内部的激活单元，上述IARNN-WORD和IARNN-CONTEXT是在单词原始表示上添加了Attention信息。因为GRU内部的激活单元控制了隐藏状态信息流，通过添加Attention信息可以影响隐藏表示，对原始GRU的改造如下图所示：<br><span>$$\begin{gather*}
z_t = \sigma(W_{xz}x_t + W_{hz}h_{t-1} + {\color{red} {M_{qz}r_q}}) \\
f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + {\color{red} {M_{qf}r_q}}) \\
\tilde {h}_{t} = tanh(W_{xh}x_t + W_{hh}(f_t \odot h_{t-1})) \\
h_t = (1-z_t) \odot h_{t-1} + z_t \odot \tilde {h}_{t} \\
\end{gather*}$$</span><!-- Has MathJax --></p>
<span>$M_{qz}$</span><!-- Has MathJax -->和<span>$M_{hz}$</span><!-- Has MathJax -->表示Attention权重矩阵，IARNN-GATE模型如下图所以：<br><img src="/images/iarnngatepic.png" alt="图25"><br><br><strong>IARNN-OCCAM</strong><br>该模型的思想来源于Occam’s Razor，即：“Among the whole words set, we choose those with fewest number that can represent the sentence”。因为不同的问题对应的答案长度不同，例如When或者Who类型的问题的答案就相对较短，对应的正则数值应该大一点，而Why或How类型的问题答案较长，正则数字应该较小。文中利用下面的公式获得问题对应的正则系数，并加入目标函数$J_i$：<br><span>$$\begin{gather*}
n_p^i = max\{w_{qp}^Tr_q^i,\lambda_q\} \\
J_i^* = J_i + n_p^i\sum _{t=1}^{mc}\alpha_t^i \\
\end{gather*}$$</span><!-- Has MathJax -->
<p>其中$\gamma _{q}^{i}$代表问题$Q_i$对应的表示，<span>$W_{ap}$</span><!-- Has MathJax -->用于将<span>$\gamma _{q}^{i}$</span><!-- Has MathJax -->映射到一个标量，<span>$\lambda _q$</span><!-- Has MathJax -->表示一个正超参数，<span>$\alpha _{t}^{i}$</span><!-- Has MathJax -->表示注意力权重。</p>
<h2 id="改进二-Compare-Aggregate-Network"><a href="#改进二-Compare-Aggregate-Network" class="headerlink" title="改进二 Compare-Aggregate Network"></a>改进二 Compare-Aggregate Network</h2><p>引入Attention机制后可以捕获句子间的语义信息用于相似性计算，改进一中的模型在获得句子经Attention处理后的特征后，一般会利用pooling对特征进行降维，这不可避免的会丢失一些信息，COMPARE-AGGREGATE Network为了解决这一问题，最后用于预测的特征不再是句子的表示特征，而是直接利用句子间的匹配特征用于预测，尽可能保留了句子间的相似性信息。</p>
<h3 id="Pairwise-Word-Interaction-Modeling-with-Deep-Neural-Networks-for-Semantic-Similarity-Measurement"><a href="#Pairwise-Word-Interaction-Modeling-with-Deep-Neural-Networks-for-Semantic-Similarity-Measurement" class="headerlink" title="Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement"></a>Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement</h3><p>Hua He在ACL 2016一文[11]中提出了一种“Pairwise Word Interaction”模型，旨在解决句子相似性计算问题，模型充分利用了单词级的细粒度信息。模型分为四个部分，分别为：Context Modeling、Pairwise Word Interaction Modeling simCube、Similarity Focus Layer focusCube和19-Layer Deep ConvNet，如下图所示：<br><img src="/images/pwimodel.png" alt="图31"><br><strong>Context Modeling</strong><br>首先对输入的句子单词进行Word embedding处理，然后利用BiLSTM获得句子中各单词包含上下文信息的表示。<br><strong>Pairwise Word Interaction Modeling simCube</strong><br>对句子$S_1$中BiLSTM的隐层状态$\overrightarrow {h_1}$与句子$S_2$的$\overrightarrow {h_2}$，模型使用下面的公式获得两者的匹配结果：<br><span>$$\begin{gather*}
coU(\overrightarrow {h}_{1},\overrightarrow {h}_{2}) = \{cos(\overrightarrow {h}_{1},\overrightarrow {h}_{2}),L_2Euclid(\overrightarrow {h}_{1},\overrightarrow {h}_{2}),DotProduct(\overrightarrow {h}_{1},\overrightarrow {h}_{2})\}
\end{gather*}$$</span><!-- Has MathJax --></p>
<p>其中cos表示余弦距离，Euclid表示欧式距离。对句子$S_1$和句子$S_2$中每个单词进行匹配，获得句子间单词匹配结果simCube。在求取simCude时，作者使用了BiLSTM隐层状态的前向、后向、前后向拼接和前后向相加四种信息。<br><strong>Similarity Focus Layer</strong><br>在衡量句子间相似性时，不同的单词对相似性的影响不一样，本层识别对相对重要的单词对。首先按照simCube中相似性数值由大到小排序，然后使用一种标记算法（参考论文[]中的Algorithm 2）识别重要单词，并记录到mask矩阵。其中重要单词对的权重设为1，不重要单词对的权重设为0.1。最终的“focus-weighted similarity cube”由mask矩阵与simCube进行element-wise相乘后获得，记为focusCube。<br><strong>19-Layer Deep ConvNet</strong><br>focusCube可以看作是一个有13个通道的“图片”，因此相似性计算问题可以转换为识别图片中的“strong pairwise word”，即图片中“pairwise word interactions”越强，对应句子的相似度就越高。模型使用了CNN，配合Max pooling策略，最终由一个全连接层和lonSoftMax函数输出结果，模型比较复杂。</p>
<h3 id="Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences"><a href="#Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences" class="headerlink" title="Bilateral Multi-Perspective Matching for Natural Language Sentences"></a>Bilateral Multi-Perspective Matching for Natural Language Sentences</h3><p>Zhiguo Wang[12]针对自然语言句子匹配任务提出了bilateral multi-perspective matching (BiMPM) model，关于multi-perspective参见改进四部分，本节主要关注matching部分。设需要匹配的句子为P和Q，作者在P-&gt;Q，Q-&gt;P两个方向上进行了匹配，使用了四种匹配策略，这些策略可以理解为不同的Attention，这里也是Attention机制方面创新一个很好的参考。下面以P-&gt;Q方向为例进行说明，Q-&gt;P方向同理。<br>BiMPM模型如下图所示：<br><img src="/images/bimpm-model.png" alt="图27"></p>
<p>模型自下而上分为五层，分别为单词表示层、文法表示层、匹配层、聚合层和预测层，其中匹配层为模型的核心。单词表示层对单词进行Word embedding处理。文法表示层与聚合层类似，都是利用BiLSTM对输入序列进行处理。匹配层包含的四种匹配策略示意图：<br><img src="/images/bimpm-match.png" alt="图28"><br><strong>Full-Matching</strong><br>P中每一个前向(反向)文法向量与Q前向(反向)的最后一个时间步的输出进行匹配。<br><strong>Maxpooling-Matching</strong><br>P中每一个前向(反向)文法向量与Q前向(反向)每一个时间步的输出进行匹配，最后仅保留匹配最大的结果向量。<br><strong>Attentive-Matching</strong><br>先计算P中每一个前向(反向)文法向量与Q中每一个前向(反向)文法向量的余弦相似度，然后利用余弦相似度作为权重对Q各个文法向量进行加权求平均作为Q的整体表示，最后P中每一个前向(后向)文法向量与Q对应的整体表示进行匹配。<br><strong>Max-Attentive-Matching</strong><br>与Attentive-Matching类似，不同的是不进行加权求和，而是直接取Q中余弦相似度最高的单词文法向量作为Q整体向量表示，与P中每一个前向(反向)文法向量进行匹配。</p>
<p>Matching Layer输出的匹配向量经Aggregation Layer双向LSTM处理后作为最后预测层的输入，预测层利用softmax函数输出预测结果。</p>
<h3 id="A-COMPARE-AGGREGATE-MODEL-FOR-MATCHING-TEXT-SEQUENCES"><a href="#A-COMPARE-AGGREGATE-MODEL-FOR-MATCHING-TEXT-SEQUENCES" class="headerlink" title="A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES"></a>A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES</h3><p>本篇论文[13]依然关注自然语言句子匹配任务，模型充分体现了Compare-Aggregate网络的优点，主要在匹配方面进行了改进，使用了六种匹配函数，并对比了效果。上文介绍的BiMPM模型在Aggregation层利用双向LSTM对匹配向量进行处理，只取前（后）向最后的输出作为特征输入预测层。本篇论文没有使用RNN对匹配结果进行序列处理，而是采用CNN进行处理后用于最终的预测，模型如下图所示：<br><img src="/images/camteleft.png" alt="图29"><br>模型分为预处理层、Attention层、匹配层和聚合层，预处理层并未在图中进行说明，模型预处理与通常的模型一致，对Question和Answer进行Word Embedding处理，然后利用LSTM/GRU获得Question和Answer序列表示信息，这里作者进行了一些改变，仅保留了input gates。Attention层应用了单向Attention，对Question进行了Attention表示，具体计算公式如下：<br><span>$$G = softmax((W^g\overline {Q}) + b^g \otimes e_Q)^T \overline {A}) \\
H = \overline {Q}G$$</span><!-- Has MathJax --><br>G表示注意力权重矩阵，H表示经Attention处理后的Question矩阵。匹配层在最后的“One more thing”部分会单独进行介绍，本节主要关注Compare-Aggregate模型。</p>
<h2 id="One-more-thing"><a href="#One-more-thing" class="headerlink" title="One more thing"></a>One more thing</h2><p>最后介绍两个在计算自然语言句子相似性时可能遇到的问题：1. 匹配时使用什么函数；2. 当多个句子相似程度较高时，如何利用句子中不相似的部分。 </p>
<h3 id="选择什么匹配函数"><a href="#选择什么匹配函数" class="headerlink" title="选择什么匹配函数"></a>选择什么匹配函数</h3><p>Wang S[13]在构建自然语言句子匹配模型时使用了六种匹配函数，如下图所示：<br><img src="/images/camttcmp.png" alt="图33"><br>图中左半部分已在4.2节进行了介绍，右半部分属于模型的匹配层，包含了六种匹配函数，公式如下：<br><span>$t_j$</span><!-- Has MathJax -->表示匹配向量，f表示匹配函数，<span>$\overline {a}_{j}$</span><!-- Has MathJax -->表示Answer中一个单词向量，<span>$h_j$</span><!-- Has MathJax -->表示Question中一个单词向量。<br><strong>NEURALNET (NN):</strong><br><span>$$\begin{align}
t_j = f(\overline {a_j},h_j) = ReLU(W \left[ \begin{matrix} \overline {a}_{i}\\ h_{j}\end{matrix} \right] + b)
\end{align}$$</span><!-- Has MathJax --><br><strong>NEURALTENSORNET (NTN): </strong><br><span>$$\begin{align}
t_j = f(\overline {a_j},h_j) = ReLU(\overline {a_{j}}^{T}T^{\left[ 1\ldots l\right]}h_j + b) 
\end{align}$$</span><!-- Has MathJax --></p>
<p><strong>Euclidean distance ans Cosine similarity(EUCCOS): </strong><br><span>$$t_j = f(\overline {a_j},h_j) = \begin{bmatrix}
\left | \left | \bar{a}_j - h_j  \right | \right | _2 \\ 
cos(\bar{a}_j,h_j) \\
\end{bmatrix}$$</span><!-- Has MathJax --></p>
<p><strong>SUBTRACTION (SUB):</strong><br><span>$$\begin{align}
t_j = f(\overline {a_j},h_j) = (\overline {a}_j - h_j) \odot (\overline {a}_j - h_j)
\end{align}$$</span><!-- Has MathJax --><br><strong>MULTIPLICATION (MULT):</strong><br><span>$$\begin{align}
t_j = f(\overline {a_j},h_j) = \overline {a}_j \odot h_j
\end{align}$$</span><!-- Has MathJax --><br><strong>SUBMULT+NN</strong><br><span>$$\begin{align}
t_j = f(\overline {a_j},h_j) = ReLU(W \left[ \begin{matrix} \left( \overline {a}_{j}-h_{j}\right) \odot \left( \overline {a}_{j}-h_{j}\right) \\ \overline {a}_{j}\odot h_{j}\end{matrix} \right] + b)
\end{align}$$</span><!-- Has MathJax --><br>模型在四个数据集上进行了测试，结果如下图所示：<br><img src="/images/camtttest.png" alt="图40"><br>测试结果表明，一般情况下SUBMULT+NN性能较好。另一个值得注意的是一些简单的匹配函数在部分数据集上性能优于NN或NTN这些较为复杂的函数，例如MULT函数在WikiQA数据集上表现最好。</p>
<p>4.1节中介绍的BiMPM模型[12]中Multi-perspective也是对匹配函数进行了改进，使用了作者定义为“Multi-perspective cosine matching”的函数，公式如下：<span>$m = f_m(v_1,v_2;W)$</span><!-- Has MathJax -->。其中$v_1$和$v_2$表示两个d维向量，W表示一个可训练参数，维度为[l,d]，l表示perspectives数，函数$f_m$返回结果m为一个l维的向量：m=[$m_1$,…$m_k$,…,$m_l$]，其中$m_k$代表第k个perspective的匹配结果：<span>$m_k = cosine(W_k \circ v_1, W_k \circ v_2)$</span><!-- Has MathJax -->。o表示element-wise乘法，$W_k$表示W的第k行。</p>
<h3 id="如何利用句子不相似的部分"><a href="#如何利用句子不相似的部分" class="headerlink" title="如何利用句子不相似的部分"></a>如何利用句子不相似的部分</h3><p>Zhiguo Wang提出了一种“Lexical Decomposition and Composition”方法[14]用于解决句子相似性计算问题，本文简称为LDC，不同于一般模型主要关注相似的部分，作者通过对句子词法和语义信息进行分解和组合综合考虑了句子间相似和不相似的部分。模型在WikiQA数据集的测试结果达到了state-of-the-art水平，值得说明的一点是主要关注相似性部分的模型[BiMPM]在WikiQA数据集的测试结果要优于LDC模型，个人猜想与具体数据集特点有关，LDC模型更适合句子间相似度较高，需要利用不相似部分进行区分的数据集，例如文章中给出的例子，如下所示：</p>
<blockquote>
<p>E1 The research is [irrelevant] to sockeye.<br>E2 The study is [not related] to salmon.<br>E3 The research is relevant to salmon.<br>E4 The study is relevant to sockeye, hinstead of cohoi.<br>E5 The study is relevant to sockeye, hrather than flounderi.</p>
</blockquote>
<p>给定一个句子对S和T，模型计算相似度sim(S,T)。模型自下而上分为：Word Representation、Semantic Matching、Decomposition和Composition四个部分。Word Representation即使用预训练的word embeddings表示句子S和T。模型结构如下图所示：<br><img src="/images/LDCMODEL.png" alt="图44"></p>
<h4 id="Semantic-Matching"><a href="#Semantic-Matching" class="headerlink" title="Semantic Matching"></a>Semantic Matching</h4><p>计算句子S中每个单词<span>$s_i$</span><!-- Has MathJax -->与句子T的匹配向量<span>$\widehat {s}_{i}$</span><!-- Has MathJax -->，同样也需要计算句子T每个单词的匹配向量<span>$\widehat {t}_{i}$</span><!-- Has MathJax -->，公式如下：<br><span>$$\widehat {s}_{i} = f_{match}(s_i,T) \qquad \forall s_i\in S \\
\widehat {t}_{j} = f_{match}(t_j,S)  \qquad \forall t_j\in T \\$$</span><!-- Has MathJax --><br>对于一个句子对S和T，首先计算相似矩阵A，对于矩阵中的一个元素<span>$a_{i,j}$</span><!-- Has MathJax -->，计算公式如下：<br><span>$a_{i,j}=\dfrac {S_{i}^{T} t_j} {\left| \left| s_i\right| \right| \cdot \left\| t_j\right\| } \qquad \forall s_i\in S,\forall t_j\in T$</span><!-- Has MathJax --><br>获得相似矩阵A后，作者使用了三种匹配函数计算S和T的匹配向量，其中S匹配向量的计算公式如下（T计算公式同理）：<br><span>$$\begin{align}
f_{match}(s_i,T) = \begin{cases} \dfrac {\Sigma _{j=0}^{n}a_{i,j}t_{j}} {\Sigma _{j=0}^{n}a_{i,j}} \qquad global \\  \dfrac {\Sigma _{j=k-w}^{k+w}a_{i,j}t_{j}} {\Sigma _{j=k-w}^{k+w}a_{i,j}} \qquad local-w\\ t_{k} \qquad max \end{cases}
\end{align}$$</span><!-- Has MathJax --></p>
<p>其中<span>$k=\arg \max _{j}a_{i,j}$</span><!-- Has MathJax -->，w表示local-w函数中以k为中心的窗口大小。</p>
<h4 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h4><p>分解部分将句子S的匹配向量<span>$s_i$</span><!-- Has MathJax -->分解为<span>$s_{i}^{+}$</span><!-- Has MathJax -->和<span>$s_{i}^{-}$</span><!-- Has MathJax -->两个部分，其中<span>$s_{i}^{+}$</span><!-- Has MathJax -->表示相似部分，<span>$s_{i}^{-}$</span><!-- Has MathJax -->表示不相似部分，句子T的分解方式同理，分解公式如下所示：<br><span>$$[s_i^+;s_i^-] = f_{decomp}(s_i,\widehat {s}_{i}) \qquad \forall s_{i}\in S \\
[t_j^+;t_j^-] = f_{decomp}(t_j,\widehat {t}_{j}) \qquad \forall t_{j}\in T$$</span><!-- Has MathJax --></p>
<p>作者使用了三种分解函数，分别是：<br><strong>rigid decomposition</strong><br><span>$$[s_i^+ = s_i; s_i^- = 0]  \qquad if s_i = \widehat {s}_{i} \\
[s_i^+ = 0; s_i^- = s_i]  \qquad otherwise$$</span><!-- Has MathJax --><br><strong>linear decomposition</strong><br><span>$$\alpha =\dfrac {s_{i}^{T}\widehat {s}_{i}} {\left| \left| s_{i}\right| \left| \cdot \right| \left| \widehat {s}_{i}\right| \right| } \\
s_i^+ = \alpha s_i \\
s_i^- = (1- \alpha)s_i\\$$</span><!-- Has MathJax --><br><strong>orthogonal decomposition</strong><br><span>$$s_i^+ = \dfrac {s_{i} \cdot \widehat {s}_{i}} {\widehat {s}_{i} \cdot \widehat {s}_{i}} \widehat {s}_{i} \qquad parallel\\
s_i^- = s_i - s_i^+  \qquad perpendicular \\$$</span><!-- Has MathJax --></p>
<h4 id="Composition"><a href="#Composition" class="headerlink" title="Composition"></a>Composition</h4><p>组合部分利用CNN将相似部分<span>$s_{i}^{+}$</span><!-- Has MathJax -->和不相似部<span>$s_{i}^{-}$</span><!-- Has MathJax -->组合为最终S的特征向量<span>$\overrightarrow {s}$</span><!-- Has MathJax -->，公式为：<span>$c_{o,i} = f(w_o * S_{[i:i+h]}^+ + w_o * S_{[i:i+h]}^- + b_o)$</span><!-- Has MathJax -->。其中<span>$w_o$</span><!-- Has MathJax -->表示CNN中的一系列filters，<span>$S_{\left[ i:i+h\right] }^{+}$</span><!-- Has MathJax -->和<span>$S_{\left[ i:i+h\right] }^{-}$</span><!-- Has MathJax -->表示<span>$S^+$</span><!-- Has MathJax -->和<span>$S^-$</span><!-- Has MathJax -->中的一部分。一个filter对<span>$S^+$</span><!-- Has MathJax -->和<span>$S^-$</span><!-- Has MathJax -->处理后会产生一系列特征<span>$\overrightarrow {c_{o}}=[c_{o,1},c_{o,2},...,c_{o,O}$</span><!-- Has MathJax -->，利用max-pooling选择最大值，则全部的filters最终输出S和T对应的特征向量<span>$\overrightarrow {S}$</span><!-- Has MathJax -->和<span>$\overrightarrow {T}$</span><!-- Has MathJax -->。</p>
<h4 id="Similarity-assessing"><a href="#Similarity-assessing" class="headerlink" title="Similarity assessing"></a>Similarity assessing</h4><p>计算S和T特征向量$\overrightarrow {S}$和$\overrightarrow {T}$的相似度：<span>$sim(S,T) = f_{sim}(\overrightarrow {S},\overrightarrow {T})$</span><!-- Has MathJax -->，$f_{sim}$使用sigmod函数。</p>
<h2 id="WikiQA数据集测试结果"><a href="#WikiQA数据集测试结果" class="headerlink" title="WikiQA数据集测试结果"></a>WikiQA数据集测试结果</h2><p>为方便的比较上述各模型，这里对各模型在WikiQA数据集上的实验结果（取最好）进行收集和整理，见下表：</p>
<table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">MAP</th>
<th style="text-align:right">MRR</th>
</tr>
</thead>
<tbody>
<tr>
<td>CAM[13]</td>
<td style="text-align:center">0.743</td>
<td style="text-align:right">0.754</td>
</tr>
<tr>
<td>IARNN[10]</td>
<td style="text-align:center">0.734</td>
<td style="text-align:right">0.741</td>
</tr>
<tr>
<td>BiMPM[12]</td>
<td style="text-align:center">0.718</td>
<td style="text-align:right">0.731</td>
</tr>
<tr>
<td>PWIM[11]</td>
<td style="text-align:center">0.709</td>
<td style="text-align:right">0.723</td>
</tr>
<tr>
<td>LDC[14]</td>
<td style="text-align:center">0.705</td>
<td style="text-align:right">0.722</td>
</tr>
<tr>
<td>ABCNN[7]</td>
<td style="text-align:center">0.692</td>
<td style="text-align:right">0.712</td>
</tr>
<tr>
<td>APN[2]</td>
<td style="text-align:center">0.688</td>
<td style="text-align:right">0.695</td>
</tr>
</tbody>
</table>
<p>表中ABCNN、APN和LDC模型属于Attention Network，CAM、IARNN、BiMPM和PWIM模型属于Compare-Aggregate Network。WikiQA数据集上的实验结果表明Compare-Aggregate Network优于Attention Network。<br><strong>结果分析</strong><br>基本模型Siamense Network单独的对两个输入句子进行特征提取，Attention Network除了获得两个句子各自特征外，还使用句子间的相关特征。Attention Network依然遵循Siamense Network思想，一般将句子表示为一个特征向量用于预测，最后的降维过程不可避免的损失一些相关性信息，Compare-Aggregate Network则直接利用句子间相关特征进行预测。模型的测试结果表明，在计算句子相似度时，如何有效的利用句子间的相关特征是模型的关键。</p>
<h2 id="各模型主要针对的问题及解决方法"><a href="#各模型主要针对的问题及解决方法" class="headerlink" title="各模型主要针对的问题及解决方法"></a>各模型主要针对的问题及解决方法</h2><table>
<thead>
<tr>
<th>Model</th>
<th style="text-align:center">问题</th>
<th style="text-align:right">解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>CAM[13]</td>
<td style="text-align:center">选择什么匹配函数</td>
<td style="text-align:right">对比六种匹配函数在四个数据集的结果，选择SUBMULT+NN或MULT</td>
</tr>
<tr>
<td>IARNN[10]</td>
<td style="text-align:center">传统的Attention-based RNN模型使用“Attention after represention”，在生成单词表示时存在不足。</td>
<td style="text-align:right">“Attention before represention”，在使用LSTM处理之前进行Attention。</td>
</tr>
<tr>
<td>BiMPM[12]</td>
<td style="text-align:center">传统的模型只在一个方向上应用Attention，匹配时只在词级别或句子级别进行匹配。</td>
<td style="text-align:right">双向Attention，multi-perspective匹配函数，四种匹配策略。</td>
</tr>
<tr>
<td>PWIM[11]</td>
<td style="text-align:center">一般的神经网络模型主要利用句子粗粒度信息，无法捕获细粒度词级别的信息。</td>
<td style="text-align:right">pairwise word interaction modeling，捕获跨句子的词关联信息。</td>
</tr>
<tr>
<td>LDC[14]</td>
<td style="text-align:center">当要比较的句子结构非常相似时，如何利用不相似的部分进行区分。</td>
<td style="text-align:right">Lexical Decomposition and Composition</td>
</tr>
<tr>
<td>ABCNN[7]</td>
<td style="text-align:center">基本模型单独处理两个句子，没有充分考虑句子间的联系。</td>
<td style="text-align:right">双向Attention。</td>
</tr>
<tr>
<td>APN[2]</td>
<td style="text-align:center">基本S模型无法获得两路输入的联合表示。</td>
<td style="text-align:right">双向Attention。</td>
</tr>
</tbody>
</table>
<h2 id="各模型损失函数及额外特征选取"><a href="#各模型损失函数及额外特征选取" class="headerlink" title="各模型损失函数及额外特征选取"></a>各模型损失函数及额外特征选取</h2><p>下表中<em>，表示不明确，论文中未明确说明。此外部分论文针对不同的数据集使用了相应的损失函数。<br>| Model  | 损失函数 | 额外特征|<br>| - | :-: | -: |<br>| CAM[13] | </em> | 未使用 |<br>| IARNN[10] | max-margin hinge loss| 未使用 |<br>| BiMPM[12] | Cross-Entropy| 未使用，但可扩展使用POS,NER特征。|<br>| PWIM[11]| hinge loss, KL-divergence | 未使用 |<br>| LDC[14] | <em> | 未使用 |<br>| ABCNN[7] | </em> | sentence lengths、WordCnt and WgtWordCnt |<br>| APN[2] | hinge loss| 未使用 |</p>
<blockquote>
<p>本文中错误和不明确之处欢迎指出，一定积极改正，谢谢！</p>
</blockquote>
<p>引用：<br>[1] Bromley J, Guyon I, LeCun Y, et al. Signature verification using a” siamese” time delay neural network[C]//Advances in Neural Information Processing Systems. 1994: 737-744.<br>[2] dos Santos C N, Tan M, Xiang B, et al. Attentive pooling networks[J]. CoRR, abs/1602.03609, 2016.<br>[3] He H, Gimpel K, Lin J J. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks[C]//EMNLP. 2015: 1576-1586.<br>[4] Tan M, Santos C, Xiang B, et al. Lstm-based deep learning models for non-factoid answer selection[J]. arXiv preprint arXiv:1511.04108, 2015.<br>[5] Severyn A, Moschitti A. Learning to rank short text pairs with convolutional deep neural networks[C]//Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015: 373-382.<br>[6] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.<br>[7] Yin W, Schütze H, Xiang B, et al. Abcnn: Attention-based convolutional neural network for modeling sentence pairs[J]. arXiv preprint arXiv:1512.05193, 2015.<br>[8] He H, Wieting J, Gimpel K, et al. UMD-TTIC-UW at SemEval-2016 Task 1: Attention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement[C]//SemEval@ NAACL-HLT. 2016: 1103-1108.<br>[9] Wieting J, Bansal M, Gimpel K, et al. Towards universal paraphrastic sentence embeddings[J]. arXiv preprint arXiv:1511.08198, 2015.<br>[10] Wang B, Liu K, Zhao J. Inner Attention based Recurrent Neural Networks for Answer Selection[C]//ACL (1). 2016.<br>[11] He H, Lin J J. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement[C]//HLT-NAACL. 2016: 937-948.<br>[12] Wang Z, Hamza W, Florian R. Bilateral multi-perspective matching for natural language sentences[J]. arXiv preprint arXiv:1702.03814, 2017.<br>[13] Wang S, Jiang J. A Compare-Aggregate Model for Matching Text Sequences[J]. arXiv preprint arXiv:1611.01747, 2016.<br>[14] Wang Z, Mi H, Ittycheriah A. Sentence similarity learning by lexical decomposition and composition[J]. arXiv preprint arXiv:1602.07019, 2016.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/23/tensorflow-broadcast-details/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/23/tensorflow-broadcast-details/" itemprop="url">TensorFlow-Broadcast details</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-23T17:36:00+08:00">
                2017-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>TensorFlow官方网站对Broadcast的解释不够详细，具体要参考Scipy网站的解析。</p>
<blockquote>
<p><a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="external">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a></p>
</blockquote>
<p>Broadcasting用于处理不同形状的numpy arrays的算术操作，形状较小的array通过复制自身，”broadcast”到与较大的array匹配的形状。例如：</p>
<blockquote>
<p>a = np.array([1.0, 2.0, 3.0])<br>b = 2.0<br>a * b<br>array([ 2.,  4.,  6.])</p>
</blockquote>
<p>处理两个array时，NumPy element-wise地比较它们的形状，两个array的dimensions必须满足以下两个条件之一：</p>
<blockquote>
<ol>
<li>they are equal</li>
<li>one of them is 1</li>
</ol>
</blockquote>
<p>例如：</p>
<blockquote>
<p>A      (4d array):  8 x 1 x 6 x 1<br>B      (3d array):      7 x 1 x 5<br>Result (4d array):  8 x 7 x 6 x 5</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/18/论文简读-Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/18/论文简读-Sentence-Similarity-Learning-by-Lexical-Decomposition-and-Composition/" itemprop="url">论文简读-Sentence Similarity Learning by Lexical Decomposition and Composition</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-18T14:23:11+08:00">
                2017-08-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者为Zhiguo Wang等人，属于IBM T.J. Watson Research Center，发表于2016年。大多数的传统的句子相似性计算方法只关注两个句子相似的部分，而忽略不相似的部分，但在某些情况下，不相似的部分对于决定两个句子是否相似也很重要。本文通过对句子lexical semantics进行分解和合成同时考虑相关性和不相关性，利用两通道的CNN将相关部分和不相关部分进行合成得到特征。模型主要涉及矩阵数学运算，CNN参数调整，没有使用RNN，训练速度较快。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h1 id="Model-Overview"><a href="#Model-Overview" class="headerlink" title="Model Overview"></a>Model Overview</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/08/论文简读-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/08/论文简读-Attention-over-Attention-Neural-Networks-for-Reading-Comprehension/" itemprop="url">论文简读-Attention-over-Attention Neural Networks for Reading Comprehension</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-08T10:29:28+08:00">
                2017-08-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>科大讯飞与哈工大联合实验室最近(截止2017.8.9)在SQuAD数据集上的准确率夺得全球第一名，力压微软亚洲研究院。本篇论文是该实验室在2016年发布的，是模型的早期版本。印象中当时在SQuAD榜单上single model准确率第二名，整体排名大概是第7名，F1 score约为79+，此处信息有待核实。准确率第一名的模型为“Interactive AoA Reader”，基于交互式层叠注意力。主要思想是根据给定的问题对篇章进行多次过滤，同时根据已经被过滤的文章进一步筛选出问题中的关键提问点，交互式地精确划定答案的范围。个人猜想其思想与斯坦福大学CS224n公开课Lecture 16中提出的Dynamic Memory Network基本一致。由于新模型的论文可能暂时没有发表，这里先对早期模型进行一下总结。</p>
<h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>面向完形填空型阅读理解任务，可利用一个三元组表示：<d,q,a>，D表示文档，Q表示查询，A表示答案。测试数据集使用了CNN News和CBTest。</d,q,a></p>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p><img src="/images/aoamodel.png" alt="aoamodel"><br>模型主要思想是直接从文档级别的注意力选择答案，而不是计算文档的混合特征。</p>
<h2 id="Contextual-Embedding"><a href="#Contextual-Embedding" class="headerlink" title="Contextual Embedding"></a>Contextual Embedding</h2><p>首先将单词转换为one-hot表示，然后利用一个共享embedding matrix We将单词转换为向量表示（与使用word2vec类似）。利用双向GRUs分别获得文档和查询单词的文法表示，每个单词的表示由GRU前向和后向表示连接而成。<br><img src="/images/aoace.png" alt="aoace"></p>
<h2 id="Pair-wise-Matching-Score"><a href="#Pair-wise-Matching-Score" class="headerlink" title="Pair-wise Matching Score"></a>Pair-wise Matching Score</h2><p>计算每个文档单词与查询单词的匹配值，直接求dot product。<br><img src="/images/aoapms.png" alt="aoapms"></p>
<h2 id="Individual-Attentions"><a href="#Individual-Attentions" class="headerlink" title="Individual Attentions"></a>Individual Attentions</h2><p>通过Pair-wise Matching获得了匹配矩阵M，在矩阵列上使用softmax函数获得每一列的注意力权重，则每一列的结果表示给定一个query单词，仅document-level的注意力。α(t)表示给定t时刻的query单词时document-level的注意力分布，可以认为是query-to-document注意力。<br><img src="/images/aoaia.png" alt="aoaia"></p>
<h2 id="Attention-over-Attention"><a href="#Attention-over-Attention" class="headerlink" title="Attention-over-Attention"></a>Attention-over-Attention</h2><p>首先，计算给定一个document单词时query单词的注意力分布，β(t)表示给定t时刻document单词时query-level注意力分布。<br><img src="/images/aoad2q.png" alt="aoad2q"><br>通过上面的计算获得了query-to-document注意力α和document-to-query注意力β。下面对query-level的注意力求均值：<br><img src="/images/aoaavg.png" alt="aoaavg"><br>最后计算α与β dot product的结果，作为“attended document-level注意力”。这里的计算表示在t时刻给定query word时对每个仅document-level注意力进行加权求和。每个query word的贡献值可以被显式的学习，最后document-level的注意力经过每个query word的重要性加权获得。<br><img src="/images/aoaab.png" alt="aoaab"></p>
<h2 id="Final-Predictions"><a href="#Final-Predictions" class="headerlink" title="Final Predictions"></a>Final Predictions</h2><p>与《Text understanding with the attention sum reader network》一文类似，本文也采用了sum attention机制获得答案。最终的输出应该被映射到词汇空间V，而非document-level attention |D|，这个对结果影响很大。<br><img src="/images/aoafor11.png" alt="aoafor11"><br>I(w, D)表示单词w在document D中出现的位置。训练时目标函数最大化正确答案的log-likelihood。<br><img src="/images/aoafor12.png" alt="aoafor12"></p>
<h1 id="N-best-Re-ranking-Strategy"><a href="#N-best-Re-ranking-Strategy" class="headerlink" title="N-best Re-ranking Strategy"></a>N-best Re-ranking Strategy</h1><p>一般来说，做完形填空时，都会将候选答案填到句子中，二次检查其是否适合、流畅和符合语法。本文提出了N-best re-ranking策略来进行二次检查。模型输出N个可能性最大的候选答案，然后将这些答案一一填充到句子中，然后对句子进行打分。此种Re-ranking的方法适用于完形填空型阅读理解任务。</p>
<h2 id="Feature-Scoring"><a href="#Feature-Scoring" class="headerlink" title="Feature Scoring"></a>Feature Scoring</h2><p>使用了三种特征：</p>
<ol>
<li>Global N-gram LM. 利用训练集中的document数据训练语言模型，计算句子的流畅度。</li>
<li>Local N-gram LM. 利用测试集中的document数据训练语言模型，计算句子的流畅度。sample-by-sample训练方式，不是在整个测试集上训练，该模型在测试sample中有许多未知词时比较有用。</li>
<li>Word-class LM. 与global相似，该模型在整个训练集的document数据上进行训练，词转换为对应的词类编号，编号通过聚类方法（mkcls tool）获得。</li>
</ol>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="/images/aoaexp.png" alt="aoaexp"><br>在CNNnews,CBTest NE 和 CN数据集上进行了测试，达到了state-of-art水平。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/08/06/论文简读-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/06/论文简读-Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/" itemprop="url">论文简读-Bilateral Multi-Perspective Matching for Natural Language Sentences</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-06T11:30:16+08:00">
                2017-08-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文发表于2017.7.14，作者为Zhiguo Wang等人，属于IBM Watson实验中心，截止2017.8.6，在TREC QA数据集上测试准确率排名第一。论文的主要目标是解决自然语言句子匹配的问题，在释义识别、自然语言推断和答案选择三个方面均取得了state-of-the-art水平。</p>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p><img src="/images/bimpm-model.png" alt="模型结构图"></p>
<p>模型自下而上分为五层，分别为单词表示层、文法表示层、匹配层、聚合层和预测层，其中匹配层为模型的核心，文中共提出了四种匹配策略，这里的匹配也就是Attention机制。<br>假设要匹配的句子分别为P和Q。下面以P-&gt;Q方向为例进行说明，Q-&gt;P方向同理。</p>
<h2 id="单词表示层"><a href="#单词表示层" class="headerlink" title="单词表示层"></a>单词表示层</h2><p>单词级别使用了GloVe模型，字符级别对字符embedding进行随机初始化，逐个输入LSTM，在训练中进行学习，由单词中的字符组成单词的向量表示。</p>
<h2 id="文法表示层"><a href="#文法表示层" class="headerlink" title="文法表示层"></a>文法表示层</h2><p>使用双向LSTM对P和Q进行编码。</p>
<h2 id="匹配层"><a href="#匹配层" class="headerlink" title="匹配层"></a>匹配层</h2><p>本层是模型的核心层，包含四种匹配策略，分别为：Full-Matching、Maxpooling-Matching、Attentive-Matching和 Max-Attentive-Matching。在介绍四种匹配策略之前，先对论文的关键点Multi-Perspective进行说明。</p>
<h3 id="Multi-Perspective"><a href="#Multi-Perspective" class="headerlink" title="Multi-Perspective"></a>Multi-Perspective</h3><p>l表示perspectives数，W是一个可训练的参数矩阵，维度为l * d。<br><img src="/images/bimpm-eql3.png" alt="eql-3"><br><img src="/images/bimpm-eql4.png" alt="eql-4"><br>根据上面的公式，每两个向量的匹配结果为一个l维的向量m，m = [m1,…,mk,…,ml]。</p>
<h3 id="four-matching-strategies"><a href="#four-matching-strategies" class="headerlink" title="four matching strategies"></a>four matching strategies</h3><p><img src="/images/bimpm-match.png" alt="four"></p>
<h4 id="Full-Matching"><a href="#Full-Matching" class="headerlink" title="Full-Matching"></a>Full-Matching</h4><p><img src="/images/fm.png" alt="fm"><br>P中每一个前向(反向)文法向量与Q前向(反向)的最后一个时间步的输出进行匹配。</p>
<h4 id="Maxpooling-Matching"><a href="#Maxpooling-Matching" class="headerlink" title="Maxpooling-Matching"></a>Maxpooling-Matching</h4><p><img src="/images/maxpool.png" alt="maxpool"><br>P中每一个前向(反向)文法向量与Q前向(反向)每一个时间步的输出进行匹配，最后仅保留匹配最大的结果向量。</p>
<h4 id="Attentive-Matching"><a href="#Attentive-Matching" class="headerlink" title="Attentive-Matching"></a>Attentive-Matching</h4><p>先计算P中每一个前向(反向)文法向量与Q中每一个前向(反向)文法向量的余弦相似度，然后利用余弦相似度作为权重对Q各个文法向量进行加权求平均作为Q的整体表示，最后P中每一个前向(后向)文法向量与Q对应的整体表示进行匹配。<br><img src="/images/calcos.png" alt="calcos"><br><img src="/images/mean.png" alt="mean"><br><img src="/images/ammatch.png" alt="ammatch"></p>
<h4 id="Max-Attentive-Matching"><a href="#Max-Attentive-Matching" class="headerlink" title="Max-Attentive-Matching"></a>Max-Attentive-Matching</h4><p>与Attentive-Matching类似，不同的是不进行加权求和，而是直接取Q中余弦相似度最高的单词文法向量作为Q整体向量表示，与P中每一个前向(反向)文法向量进行匹配。</p>
<h2 id="聚合层"><a href="#聚合层" class="headerlink" title="聚合层"></a>聚合层</h2><p>利用双向LSTM对匹配层输出的匹配向量进行处理，取得P、Q前向和后向最后一个时间步的输出向量，连接后输入预测层。</p>
<h2 id="预测层"><a href="#预测层" class="headerlink" title="预测层"></a>预测层</h2><p>利用一个两层的前向神经网络处理固定长度的匹配向量，在输出层应用softmax函数获得Pr(y|P,Q)。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="the-influence-of-multi-perspective-cosine-matching-function"><a href="#the-influence-of-multi-perspective-cosine-matching-function" class="headerlink" title="the influence of multi-perspective cosine matching function"></a>the influence of multi-perspective cosine matching function</h2><p><img src="/images/lres.png" alt="lres"><br>l=0时相当于直接计算余弦相似度。</p>
<h2 id="Experiments-on-Answer-Sentence-Selection"><a href="#Experiments-on-Answer-Sentence-Selection" class="headerlink" title="Experiments on Answer Sentence Selection"></a>Experiments on Answer Sentence Selection</h2><p><img src="/images/trecqa.png" alt="trecqa"><br>截止2017.8.7，在TREC-QA数据集上的表现算是并列第一吧。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/27/论文简读-Reading-Wikipedia-to-Answer-Open-Domain-Questions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/27/论文简读-Reading-Wikipedia-to-Answer-Open-Domain-Questions/" itemprop="url">论文简读-Reading Wikipedia to Answer Open-Domain Questions</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-27T16:30:19+08:00">
                2017-07-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>ACL 2017入选论文，作者为来自斯坦福的Danqi Chen(cs224n助教)。Facebook于2017.7.27日开放源码，项目名为DrQA。</p>
<blockquote>
<p>GitHub地址：<a href="https://github.com/facebookresearch/DrQA" target="_blank" rel="external">https://github.com/facebookresearch/DrQA</a></p>
</blockquote>
<p>DrQA的主要任务是大规模机器阅读（MRS）。DrQA会在一个非常庞大的非结构化文档语料库中寻找问题的答案。这个系统最大的挑战是文档检索与文本的机器理解如何更好的结合。</p>
<p><img src="/images/rwaodq-model.png" alt="模型结构图"></p>
<h2 id="文档检索"><a href="#文档检索" class="headerlink" title="文档检索"></a>文档检索</h2><p>采用基于二元语法哈希（bigram hashing）和TF-IDF匹配的搜索方法。</p>
<h2 id="文档阅读理解"><a href="#文档阅读理解" class="headerlink" title="文档阅读理解"></a>文档阅读理解</h2><p>篇章和问题均采用三层双向LSTM，128个hidden units进行编码。</p>
<h3 id="篇章编码"><a href="#篇章编码" class="headerlink" title="篇章编码"></a>篇章编码</h3><h4 id="Word-embedding"><a href="#Word-embedding" class="headerlink" title="Word embedding"></a>Word embedding</h4><p>GloVe, 300-dimensional</p>
<h4 id="Exact-match"><a href="#Exact-match" class="headerlink" title="Exact match."></a>Exact match.</h4><p>篇章中的某个词的原始、全部转换为小写和形态上是否匹配。</p>
<h4 id="Token-features"><a href="#Token-features" class="headerlink" title="Token features."></a>Token features.</h4><p><img src="/images/tf.png" alt="tf"></p>
<h4 id="Aligned-question-embedding"><a href="#Aligned-question-embedding" class="headerlink" title="Aligned question embedding."></a>Aligned question embedding.</h4><p>与问题计算注意力权重后获得的特征。</p>
<p><img src="/images/aij.png" alt="aij"></p>
<h3 id="问题编码"><a href="#问题编码" class="headerlink" title="问题编码"></a>问题编码</h3><p>使用一个需要学习的权值向量计算问题各个单词的注意力权重，然后加权求和作为问题的整体表示。<br><img src="/images/bj.png" alt="bj"></p>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p><img src="/images/pred.png" alt="pred"><br>获得篇章单词与问题的相似度结果后，选择Pstart(i)×Pend(i′)最大的作为答案开始坐标和结束坐标，坐标之间即为答案，同时可利用i ≤ i′ ≤ i+15限制答案长度。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/09/03/LeetCode-279-Perfect-Squares/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/09/03/LeetCode-279-Perfect-Squares/" itemprop="url">LeetCode-279.Perfect Squares</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-09-03T10:34:15+08:00">
                2016-09-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>Given a positive integer n, find the least number of perfect square numbers<br>(for example, 1, 4, 9, 16, …) which sum to n.<br>For example, given n = 12, return 3 because 12 = 4 + 4 + 4; given n = 13, return 2 because 13 = 4 + 9.</p>
</blockquote>
<h1 id="算法思想"><a href="#算法思想" class="headerlink" title="算法思想"></a>算法思想</h1><ol>
<li>动态规划。当然我们会首先尝试贪心，例如12，贪心的结果是12=9+1+1+1，可见贪心不能得到最优解，所以只能上动规了。关于动规，思想和编码都不难，不过我想结合编码讨论一下设计模式的东西，也就是在面向对象编程中如何设计一个函数，使函数调用更加高效，其中也涉及一些内存使用优化的问题，开另一篇文章讨论。</li>
<li>数学方法。一个会数学的程序猿可以变身金刚…数学对于效率的提升不能更酷，运行时间先卖个关子，最后再给出。</li>
<li>广度优先搜索。最少个数，对应空间搜索树的最小深度，很多类似的最小问题都可以用广度优先搜索解决，不过这题BFS并不适合，效率不高，代码较长，且有很多细节容易出错，不建议使用。</li>
</ol>
<h1 id="编码与解释"><a href="#编码与解释" class="headerlink" title="编码与解释"></a>编码与解释</h1><h5 id="1-动态规划：见另一篇文章。"><a href="#1-动态规划：见另一篇文章。" class="headerlink" title="1. 动态规划：见另一篇文章。"></a>1. 动态规划：见另一篇文章。</h5><h5 id="2-数学方法。"><a href="#2-数学方法。" class="headerlink" title="2. 数学方法。"></a>2. 数学方法。</h5><blockquote>
<p>四平方定理：每个正整数均可表示为4个整数的平方和。<br>更多：<a href="https://zh.wikipedia.org/wiki/四平方和定理" target="_blank" rel="external">https://zh.wikipedia.org/wiki/四平方和定理</a><br>三平方定理：每个自然数均可表示为3个整数的平方和，当且仅当该数不能表示为n=pow(4,a)*(8b+7),a,b是整数。<br>更多<a href="https://en.wikipedia.org/wiki/Legendre%27s_three-square_theorem" target="_blank" rel="external">https://en.wikipedia.org/wiki/Legendre%27s_three-square_theorem</a></p>
</blockquote>
<p>有了两个定理后，作为一个菜鸡程序员，应该也能搞定代码。注意两个小细节：<br><strong>1.使用&amp;,==等不明确优先级的运算符时，最好加上括号！</strong><br><strong>2.不要忘记单独对结果为2时的判断，见代码注释处//check</strong></p>
<pre><code>//cpp
class Solution {
public:
    int numSquares(int n) {
        if(n&lt;=0) return 0;
        if(isSqure(n)) return 1;
        while((n&amp;3)==0) n&gt;&gt;=2;
        if((n&amp;7)==7) return 4;
        int sqrt_n=(int)sqrt(n);
        for(int i=1;i&lt;=sqrt_n;++i){//check
            if(isSqure(n-i*i)) return 2;
        } 
        return 3;
    }
    bool isSqure(int n)
    {
        int sqrt_n=(int)sqrt(n);
        return sqrt_n*sqrt_n==n?1:0;
    }
};
</code></pre><h5 id="3-广度优先搜索"><a href="#3-广度优先搜索" class="headerlink" title="3.广度优先搜索"></a>3.广度优先搜索</h5><p>搜索树大致形状：</p>
<blockquote>
<pre><code>             0 
  1       4      9     16
2 5...   5...  10..  17...
</code></pre></blockquote>
<p>无论是广度优先还是深度优先(回溯常用)，关键都是要画出空间搜索树。三个细节要注意：<br><strong>注释1：提高效率，跳过同层已计算过的节点。</strong><br><strong>注释2：为什么不能使用else？结合空间搜索树考虑，else的情况包括cur+i&lt;n&amp;&amp;num[cur+i-1]!=0，什么意思呢，表示该节点对应的该值在之前已经考虑过，但是能直接从这里break吗？不能！因为该值只是被计算过，不代表该节点右边的兄弟节点应该被跳过。这些兄弟节点仍然应该被计算。</strong><br><strong>注释3：队列q.pop()的在循环中的正确位置</strong></p>
<pre><code>//cpp
class Solution {
public:
    int numSquares(int n) {
        if(n&lt;=0) return 0;
        vector&lt;int&gt; num(n,0);
        vector&lt;int&gt; ps;
        for(int i=1;i&lt;=n/i;++i)
        {
            ps.push_back(i*i);
            num[i*i-1]=1;
        }
        if(ps.back()==n) return 1;
        queue&lt;int&gt; q;
        for(int &amp;i:ps) q.push(i);
        int numSq=1;
    while(!q.empty())
    {
        ++numSq;
        int size=q.size();
            while(size--)
            {
                int cur=q.front();
                for(int &amp;i:ps)
                {
                    if(cur+i==n) return numSq;
                    else if(cur+i&lt;n&amp;&amp;num[cur+i-1]==0)//1
                    {
                        q.push(cur+i);
                        num[cur+i-1]=numSq;
                    }
                    else if(cur+i&gt;n) break;//2
                }
                q.pop();//3
            }
        }
        return numSq;
    }
};
</code></pre><p>运行时间对比(600 test cases)：</p>
<table>
<thead>
<tr>
<th>动态规划(优化版)</th>
<th style="text-align:center">数学</th>
<th style="text-align:right">广度优先</th>
</tr>
</thead>
<tbody>
<tr>
<td>16ms</td>
<td style="text-align:center">3ms</td>
<td style="text-align:right">68ms</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/23/LeetCode-134-Gas-Station/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/08/23/LeetCode-134-Gas-Station/" itemprop="url">LeetCode-Gas Station</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-08-23T10:19:49+08:00">
                2016-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>There are N gas stations along a circular route, where the amount of gas at station i is gas[i].<br>You have a car with an unlimited gas tank and it costs cost[i] of gas to travel from station i to its next station (i+1). You begin the journey with an empty tank at one of the gas stations.<br>Return the starting gas station’s index if you can travel around the circuit once, otherwise return -1.<br>Note:<br>The solution is guaranteed to be unique.</p>
</blockquote>
<h1 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h1><p>如果gas总和大于cost总和，那么一定有一种解法。<br>在知道这个假设的前提下解题并不难，一次遍历，当油量无法达到时更新可能的起始点，最后比较有油和缺油的量即可，关键是如何证明？<br>当然，可以用严谨的数学公式去推导，不过理解起来总是需要点时间。不妨做个类比，假设你去买东西，手里有3，5，7等不同面值的钞票，商品的价格有4，4，1等等，我们都知道，如果手里钱的总数大于商品价格总数，那自然可以将所有商品都买到(走完)，这与本题是不是一个道理？只是提供个想法，正确性可以讨论和验证。</p>
<h1 id="AC代码"><a href="#AC代码" class="headerlink" title="AC代码"></a>AC代码</h1><pre><code>//c++
class Solution {
public:
    int canCompleteCircuit(vector&lt;int&gt;&amp; gas, vector&lt;int&gt;&amp; cost) {
        int start=0,tank=0,lack=0;
        for(int i=0;i&lt;gas.size();++i)
        {
            if((tank=tank+gas[i]-cost[i])&lt;0){
               start=i+1;lack+=tank;tank=0;  
            }
        }
        return tank+lack&lt;0?-1:start;
    }
};
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/22/数据传输中/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/08/22/数据传输中/" itemprop="url">数据传输中...</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-08-22T16:27:51+08:00">
                2016-08-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>博客重新整理了一番，换了主题，内容清空，增加新的内容的同时，原来那些破铜烂铁，有空也会慢慢搬过来…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/20/思想经济与知乎/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jeb">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小菜鸡">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/08/20/思想经济与知乎/" itemprop="url">思想经济与知乎</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-08-20T15:52:55+08:00">
                2016-08-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ww1.sinaimg.cn/crop.0.0.920.300/718878b5jw1f5lj8g0ugwj20pk08cwgd.jpg" alt="知乎"> </p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近整理U盘以前的东西时发现了一篇两年前写的文章，大多是在地铁13号线上用Evernote码的，感谢印象笔记！当时还年轻，对喜欢和经常用的产品的进行了一些简单的思考，加上最近搞出了个“值乎”，决定趁机把这些文字发出来，再放着就发霉了。作为一名Too young, Too naive的学生，文中许多观点可能都有讨论的空间，欢迎各位不吝赐教。另外，文章中图片和部分名词引用自网络。 </p>
<h3 id="关于知乎"><a href="#关于知乎" class="headerlink" title="关于知乎"></a>关于知乎</h3><p>在最近提了2个问题，获得0个回答。回答一个问题，获得0个赞后来思考知乎。 </p>
<blockquote>
<p>知乎是神马?从宣传片中得到这样的信息:<br>世界的细节，认知的精度，博儒邂逅专家，隐士挑战权威，认真收获认同，专业赢得掌声。崇尚真诚，回避浮夸，精于专业，友善有趣，好好的说话，认真的沟通。<br>在知乎，总有一个领域，你比别人更专业。分享知识，经验和见解。让好奇不再孤单。知乎，认真你就赢了！ </p>
</blockquote>
<h1 id="思想经济"><a href="#思想经济" class="headerlink" title="思想经济"></a>思想经济</h1><p>周源说做知乎的初衷来自于这样的需求:一个人脑子里的想法，见识很有可能是另一个人需要的。 理想状态就是超级连接不同人，思想的交流碰撞变得有趣。 从原始社会到现在 ，物质盈余导致了交换，进而形成商业。随着社会的发展，思想开始盈余，交换的需求自然产生。思想盈余早已开始，但当时没有足够的产品承载。当时新浪新闻下的评论可以反映思想盈余的盛况了，百度贴吧也于此时横空出世，飞速发展。还有一件不得不提的产品就是博客，它极大的满足了用户产出内容的需求，但最终没落了。为什么?不能很好的承载思想交换;社会节奏趋于快速化，移动互联网使内容和时间碎片化。现在突然想提出一个问题。神马东东来承载思想交换?先看一下商品交换的发展史，从原始的物物直接交换，到货币中介的产生，发展到如今的商品经济。一种直觉，思想经济必然与商品经济有诸多相似之处。需要一个”市场”，也就是周源所说的连接，传统的市场正是连接了消费者与生产者。 </p>
<h1 id="思想经济交换市场"><a href="#思想经济交换市场" class="headerlink" title="思想经济交换市场"></a>思想经济交换市场</h1><p>我们来思考这种思想上的连接，可以理解为思想经济的交换市场。结合一些产品：博客是一对多的形式，博主在输出思想，用户进行评论，这很难称为思想交换。微博更符合互联网的快节奏，碎片化的特点，降低了输出思想的门槛，用户交流也更加频繁。输出思想门槛的降低使更多人成为思想的生产者，这意味着思想市场”商品”的极大丰富，但是由于微博整体偏娱乐化，就思想经济市场价值标准来衡量，这里的商品价值偏低，更像是小商品市场，当然微博里面不乏有思想沉淀的文章，不过不占据主题。社交网站的思想商品并不开放，而且价值标准是封闭的，与开放思想市场的价值标准并不一致，这一点很好理解。比如女神发的内容关注人就更多一点，相对价值就更高。而广大屌丝费力写了篇精美的文章也很难引起大家注意。这个价值标准其实对应这样一句话，成也微博，败也微博。微博越来越中心化，思想的输出集中到部分人。越来越像媒体。思想经济，大交换，超连接的宏大蓝图中，微博难以承载交换市场这个任务，这是由微博自身结构，特点决定的。</p>
<h1 id="思想经济与知乎"><a href="#思想经济与知乎" class="headerlink" title="思想经济与知乎"></a>思想经济与知乎</h1><p>这里的经济不是狭义的概念，泛指整个社会思想的生产，流通，交换等活动（参考百度百科经济定义）。知乎能承担起交换市场这个任务吗?相比于微博，知乎商品价值更高，而且去中心化，明星的概念和影响力被削弱。微博的核心点是人，知乎的核心点是问题。底层的结构决定了知乎更加高明，距离大思想交换市场更近。为什么?商品市场显性主角是物品，而人在背后。同样，在思想市场。思想应成为主体，人也应该在后面。这里提出一个问题：知乎需要实名认证吗?商品市场流通的物品不会印刷生产者的名字，但会标注生产商的名称。即思想商品需要一个靠谱的代号，这种代号一旦实名，可能会影响到思想的生产，流通等。 </p>
<h1 id="知乎发展历程思考"><a href="#知乎发展历程思考" class="headerlink" title="知乎发展历程思考"></a>知乎发展历程思考</h1><p>知乎前期得到了美好的发展，引起了众多人的关注，用户质量高，单个用户产出思想商品数量，质量都很优质。整个市场呈现一种”共产主义”。外面的人看到知乎上的回答后非常震惊。网上竟然有人无私的写了那么长的内容来回答问题。甚至有人就这个问题上在知乎提问，有很多回答，例如为了炫耀自己等等，那这在百度知道上不能完成吗?其实更多是因为知乎的氛围，这里的人决定了产品的气质。而后知乎对外开放，这种开放带来的影响是巨大的。这时有人说知乎已经不是以前的知乎了。至少，很多人变得不是那么无私了。我们来打个比方，以前的知乎是这样一个小村子，村民素质较高，且生产能力很强，大家乐于把自己的东西拿出来互相交换。而后村子里面来了一群人，他们不生产产品，而只拿走产品，甚至这群人里有人嘲笑那些默默生产产品，无私拿出来交换的人是很傻。以前人们觉得分享自己的生产品是很自然的事，现在他们开始考虑为什么要分享，分享的欲望降低，甚至有人选择了离开。 单从上面的内容看，这种开放带来了诸多不利影响。知乎团队为什么选择完全开放？以我校的BT站点为例，本校学生可以注册，同时采取部分开放政策，定期派发邀请码。并记录用户上传下载行为，一旦用户分享率低到一个阈值，则封禁其账户。大多数BT站点采用这种半开放的方式，并给予上传资源多的用户奖励，分享率低的用户封禁处理，保证了资源的正常流转。BT站点是资源的交换。周团长曾以p2p对知乎知识分享模型进行解释，的确从技术结构看知乎，BT站点均为p2p形式。虽然知乎采用的是p2p的结构，但表层的呈现并不像电影，音乐那样出色。以电影为例，虽然用户从其他用户获得是影片的各个碎片，但最终呈现给用户的是一部完整的影片，影片的碎片聚集对用户是不可见的。而知乎上一个问题答案虽然来自不同用户的碎片，但最终呈现的仍是零乱的，无论是电影还是一个问题的答案，用户不关心获取自己想要东西的过程，他只想要他需要的东西，知乎的答案组织形式是需要进行思考和改进的。 </p>
<h1 id="知乎应该完全开放吗"><a href="#知乎应该完全开放吗" class="headerlink" title="知乎应该完全开放吗"></a>知乎应该完全开放吗</h1><p>继续知乎的开放问题，在看到很多问题答案是2011年的时候，真的很有必要思考目前完全开放的战略是否正确。有人说，封闭没有未来，但为什么苹果一样取得了成功。因为乔帮主够nb，他能做出你喜欢的东西，他能引领大众。知乎有这个能力吗？肯定是有的，早期知乎聚集了一大群各个领域的大神，这样的社区绝对可以形成引领作用。选择完全开放，意味着一件珍贵的东西人人都可得到，自然不觉得珍贵了。如果思想有价格的话，这意味着完全打破了供求关系决定价值理论。用户端有需求，开始生产者在供给，但不能获得“利益”，这种利益不是那种飘渺的炫耀自己，而是更为实在的利益。没有了足够的生产者，这个系统很难运转。后面会试着从供求-价格理论讨论半开放状态下的知乎市场。再提出一个问题，为什么搜索能完全开放？之所以提出这个问题，是因为知乎有人提到“社交搜索”的概念，并把它作为知乎未来的蓝图。搜索免费，利用竞价排名或者Google AdWords这种商业模式能让你想到什么？是，传统电视传媒基本上采取的就是这种方式，免费观看节目，但会植入广告。值得注意的是，这种模式知乎并不适合。首先，从信息提供层面来讲，搜索引擎的内容基本上是静态的。从信息流动来讲，搜索引擎的流向基本是单向的。完全开放，大量用户涌入，他们基本不会对搜索引擎提供的内容及算法造成影响，知乎显然与其不同。然后我们再看一款产品，豆瓣。知乎之于百度知道相当于豆瓣之于百度贴吧，那么既然豆瓣可以完全开放，而且还发展的不错，那么知乎为什么不能？这个问题曾困扰我了一段时间，在足球场的看台上突然想明白了。豆瓣的核心是兴趣，它与知乎的连接方式不同。兴趣连接方式会自然将用户聚类，小团体中的用户无论是提出话题或者参与话题讨论，其背后有着兴趣的驱动，而且用户本身也会为提出话题，参与讨论自主的吸收内容，整个系统得以流转。知乎的连接方式是问题，相对于兴趣的连接方式，这种方式太碎片了，不够逻辑，不够强烈。想象这样一个系统，我们将用户比作一个个点，用蓝色标示。问题也是一个个点，用红色标示，红色的点可以连接蓝色的点。从零开始，我们看到一条条线经过红色的点进行连接，蓝色点之间也出现了连线（关注），通过红色连接较多的蓝色点之间经过复杂的网络作用，慢慢形成了一种类似群组的形态，群组形态最稳定，最活跃的形态可能是全连接，即用户之间互相关注，知乎早期是可能出现这种形态的。完全开放以后，大量用户节点的涌入造成了一种稀释作用，因为红色点的连接功能本身就不够逻辑性，不够强大。连接慢慢变得混乱，甚至出现了很多孤点，这些孤点中包含了许多红色点与蓝色点。这些点对于整个系统并无益处，甚至是有害的，因为他们会影响到其他连接。微博中的用户关系也可以利用这个模型进行数字化，我们只关注其中连接的情况。微博中的连接关系是具有“繁殖”作用的，即连接会产生新的连接，而且因为微博内容的碎片化，互动的及时行，这种连接具有了社交性，知乎与其相比，连接的“繁殖”能力很弱，这意味着知乎中社交性的产生要更多的依靠问题节点的连接。从这个模型中可以推测，知乎是不适合完全开放的，这是由知乎的连接方式决定的。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <img class="site-author-image" itemprop="image"
              src="/images/avatar.gif"
              alt="Jeb" />
          
            <p class="site-author-name" itemprop="name">Jeb</p>
            <p class="site-description motion-element" itemprop="description">我菜故我在</p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jeb</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
